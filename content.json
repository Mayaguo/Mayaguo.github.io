{"meta":{"title":"GMaya","subtitle":"风萧萧兮易水寒","description":"一分耕耘，一分收获。","author":"GMaya","url":"https://gmaya.top","root":"/"},"pages":[{"title":"about","date":"2019-12-26T07:35:07.000Z","updated":"2020-05-15T07:06:32.780Z","comments":true,"path":"about/index.html","permalink":"https://gmaya.top/about/index.html","excerpt":"","text":"个人介绍 杭漂在外的Java后端攻城狮。爱听歌，爱上网，爱新技术，爱折腾，爱钻研，爱简约。 解决不了BUG ，就解决发现BUG的人。 – 鲁迅 联系方式： 邮箱：gmaya@qq.com 博客介绍 不要求博客多么炫酷，不要求多么优秀，不要求很多访问。 但，访问速度必须快，页面必须整洁（洁癖），代码必须对齐。 如果不把经历过的错误以及解决办法记录下来，时隔一年半载，再次遇到，大几率还是面向百度编程。。 好记性不如烂笔头，自己亲手搭建个博客，亲手写一遍，亲手再整理一遍，我觉得经历的越多，时间越久，收获就更大！ 期待五年后的我再次看自己的个人博客会有很多感触。"},{"title":"tags","date":"2020-01-01T04:39:04.000Z","updated":"2020-05-15T07:06:16.390Z","comments":false,"path":"tags/index.html","permalink":"https://gmaya.top/tags/index.html","excerpt":"","text":""},{"title":"friends","date":"2020-01-01T04:39:04.000Z","updated":"2020-05-19T03:13:48.008Z","comments":true,"path":"links/index.html","permalink":"https://gmaya.top/links/index.html","excerpt":"","text":"友链须知 原创博客、技术博客 请确定链接有效，无不良信息 保持更新哦 想要交换友链的小伙伴，欢迎在下方留言，留言格式： 博客名称：GMaya 头像链接：https://gmaya.top/images/headpic.jpg 博客地址：https://gmaya.top 个人说明：一分耕耘，一分收获。"},{"title":"categories","date":"2020-01-01T04:39:04.000Z","updated":"2020-05-15T07:06:10.265Z","comments":false,"path":"categories/index.html","permalink":"https://gmaya.top/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"AOP实现日志入参出参打印","slug":"20200723","date":"2020-07-23T07:03:11.000Z","updated":"2020-07-23T07:07:18.014Z","comments":true,"path":"2020/20200723/","link":"","permalink":"https://gmaya.top/2020/20200723/","excerpt":"","text":"描述减少代码中接口的入参出参打印，减少工作量，保持打印风格一致性。 效果展示[2020-07-23 14:50:09 INFO http-nio-9001-exec-2] t.g.gmayaserviceadminimpl.system.aspect.LogAspect - top.gmaya.gmayaserviceadminimpl.system.controller.UserController.add()【新增登录信息】：=================== [2020-07-23 14:50:09 INFO http-nio-9001-exec-2] t.g.gmayaserviceadminimpl.system.aspect.LogAspect - top.gmaya.gmayaserviceadminimpl.system.controller.UserController.add()【方法请求参数为】：{\"data\":{\"id\":1,\"name\":\"1212\"},\"token\":\"11111111\"} [2020-07-23 14:50:09 INFO http-nio-9001-exec-2] t.g.gmayaserviceadminimpl.system.aspect.LogAspect - top.gmaya.gmayaserviceadminimpl.system.controller.UserController.add()【方法返回结果为】：{\"msg\":\"success\",\"code\":0,\"data\":1} [2020-07-23 14:50:09 INFO http-nio-9001-exec-2] t.g.gmayaserviceadminimpl.system.aspect.LogAspect - top.gmaya.gmayaserviceadminimpl.system.controller.UserController.add()【方法执行时长为】：208 ms [2020-07-23 14:50:09 INFO http-nio-9001-exec-2] t.g.gmayaserviceadminimpl.system.aspect.LogAspect - {\"method\":\"POST\",\"createTime\":1595487009526,\"ip\":\"192.168.21.1\",\"methodName\":\"add\",\"className\":\"top.gmaya.gmayaserviceadminimpl.system.controller.UserController\",\"createUser\":\"GMaya\",\"time\":208,\"operation\":\"新增登录信息\",\"url\":\"http://localhost:9001/user/add\"} [2020-07-23 14:50:09 INFO http-nio-9001-exec-2] t.g.gmayaserviceadminimpl.system.aspect.LogAspect - top.gmaya.gmayaserviceadminimpl.system.controller.UserController.add()【保存数据库成功！】：=================== 实现思路：一个注解类， 一个切面类。使用环绕通知，将入参出参打印出来，可以根据实际情况，有些接口只需要打印即可，有些需要打印并保存到数据库。 注解类GmLog package top.gmaya.gmayaserviceadminimpl.system.annotation; import java.lang.annotation.*; /** * 自定义日志注解 * @author GMaya * @dateTime 2020/7/23 10:29 * 1.运行时 使用使用注解 * 2.注解作用于方法上 * 3.注解是否将包含在 JavaDoc 中 */ @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.METHOD }) @Documented public @interface GmLog { // 打印日志描述信息 String value() default \"\"; // TODO 是否保存到数据库 boolean isSave() default false; } 切面类LogAspect里面有两种方式，一个是拦截添加注解的方法（适合新建项目，写接口的时候加上注解），一个是指定的包名下面所有的接口（适合现有项目，不必要改变其余代码） package top.gmaya.gmayaserviceadminimpl.system.aspect; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import lombok.extern.slf4j.Slf4j; import org.aspectj.lang.JoinPoint; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.Signature; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Pointcut; import org.aspectj.lang.reflect.MethodSignature; import org.springframework.stereotype.Component; import org.springframework.web.context.request.RequestContextHolder; import org.springframework.web.context.request.ServletRequestAttributes; import top.gmaya.gmayaserviceadminimpl.system.annotation.GmLog; import javax.servlet.http.HttpServletRequest; import java.lang.reflect.Method; import java.net.InetAddress; import java.util.Date; /** * 用于记录注解上接口的入参出参，统一规范。 */ @Aspect @Component @Slf4j public class LogAspect { /** * 方法一： 不需要自定义注解， 直接拦截所有controller的请求。全部打印 * 定义切入点表达式 * 第一个*号：表示返回类型， *号表示所有的类型。 * 包名：表示需要拦截的包名，后面的两个句点表示当前包和当前包的所有子包. * 第二个*号：表示类名，*号表示所有的类。 * *(..):最后这个星号表示方法名，*号表示所有的方法，后面括弧里面表示方法的参数，两个句点表示任何参数 */ @Pointcut(\"execution(public * top.gmaya.gmayaserviceadminimpl.system.controller..*.*(..))\") public void privilege() { } /** * 方法二：拦截该注解标识的方法 */ @Pointcut(\"@annotation(top.gmaya.gmayaserviceadminimpl.system.annotation.GmLog)\") public void logPointCut() { } /** * 环绕通知 * @param pjd * @return * @throws Throwable */ // @Around(\"privilege()\") // 第一种方式 @Around(\"logPointCut()\") // 第二种方式 public Object arount(ProceedingJoinPoint pjd) throws Throwable { long startTime = System.currentTimeMillis(); // 类名 String className = pjd.getTarget().getClass().getName(); // 获取执行的方法名称 String methodName = pjd.getSignature().getName(); // 1. 如果是使用的第二种方式，则判断该方法是否使用了改注解 // 2. 如果是使用的第一种方式，直接注释即可。 GmLog gmLog = this.getAnnotationLog(pjd); if (gmLog != null) { String value = gmLog.value(); log.info(\"{}.{}()【{}】：===================\", className, methodName, value); } Object[] args = pjd.getArgs(); try { String params = JSON.toJSONString(args[0]); //打印请求参数参数 log.info(\"{}.{}()【方法请求参数为】：{}\", className, methodName, params); } catch (Exception e) { log.info(\"{}.{}()【方法请求参数打印失败】：{}\", className, methodName, e); } // 执行目标方法 Object result = pjd.proceed(); // 打印返回结果 try { String s = JSON.toJSONString(result); log.info(\"{}.{}()【方法返回结果为】：{}\", className, methodName, s); } catch (Exception e) { log.info(\"{}.{}()【方法返回结果打印失败】：{}\", className, methodName, e); } // 获取执行完的时间 long time = System.currentTimeMillis() - startTime; log.info(\"{}.{}()【方法执行时长为】：{}{}\", className, methodName, time, \" ms\"); // 如果使用第一种方式，把这里注释掉 // TODO 这里可以考虑新加一个异步方法，保存信息到数据库，入参，出参，请求人，请求时间，ip信息等，如果有异常，还有异常信息。 if (gmLog != null) { boolean save = gmLog.isSave(); if (save) { String val = gmLog.value(); // 调用异步保存数据库方法 int i = this.saveLog(pjd, time, val); if (i > 0) { // 判断插入条数，大于0，保存成功。 log.info(\"{}.{}()【{}】：===================\", className, methodName, \"保存数据库成功！\"); } } } return result; } /** * 是否存在注解，如果存在就获取 * @param joinPoint * @return */ private GmLog getAnnotationLog(JoinPoint joinPoint) { Signature signature = joinPoint.getSignature(); MethodSignature methodSignature = (MethodSignature) signature; Method method = methodSignature.getMethod(); if (method != null) { return method.getAnnotation(GmLog.class); } return null; } /** * 保存到数据库 * @param joinPoint * @param time 方法执行时间 单位ms * @param val 方法请求描述 * @return */ private int saveLog(JoinPoint joinPoint, long time, String val) { ServletRequestAttributes requestAttributes = (ServletRequestAttributes) RequestContextHolder .getRequestAttributes(); HttpServletRequest request = requestAttributes.getRequest(); JSONObject jsonObject = new JSONObject(); // ip地址 String hostAddress = \"\"; try { hostAddress = InetAddress.getLocalHost().getHostAddress(); } catch (Exception e) { log.error(\"获取ip失败\"); } // redis.getUserId(); 结合实际情况 获取当前登录人信息 // 类名 String className = joinPoint.getTarget().getClass().getName(); // 获取执行的方法名称 String methodName = joinPoint.getSignature().getName(); String url = request.getRequestURL().toString(); String method = request.getMethod(); jsonObject.put(\"ip\", hostAddress); jsonObject.put(\"className\", className); jsonObject.put(\"methodName\", methodName); jsonObject.put(\"url\", url); // 执行时间 jsonObject.put(\"time\", time); jsonObject.put(\"createTime\", new Date()); jsonObject.put(\"createUser\", \"GMaya\"); // 操作描述 jsonObject.put(\"operation\", val); jsonObject.put(\"method\", method); String s = jsonObject.toJSONString(); // 调用日志service的add方法即可！ log.info(s); return 1; } } controller层接口展示： @RequestMapping(\"add\") @GmLog(value = \"新增登录信息\" , isSave = true) public R add(@RequestBody F&lt;UserEntity> f) { // 登录用户信息 UserEntity user = this.getUser(f.getToken()); return R.data(userService.add(f,user)); } 只需要添加注解即可，以及是否保存到数据库，默认不保存。 @GmLog(value = \"新增登录信息\" , isSave = true) 写完，测试一下，完美！","categories":[{"name":"AOP","slug":"AOP","permalink":"https://gmaya.top/categories/AOP/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://gmaya.top/tags/Spring-Boot/"},{"name":"AOP","slug":"AOP","permalink":"https://gmaya.top/tags/AOP/"}]},{"title":"Feign注解的简单使用","slug":"20200628","date":"2020-06-28T07:54:21.000Z","updated":"2020-06-28T07:58:43.903Z","comments":true,"path":"2020/20200628/","link":"","permalink":"https://gmaya.top/2020/20200628/","excerpt":"","text":"调用微服务如果在同一个注册中心上的微服务。name:就是在注册中心的名称 @FeignClient(name = \"gmaya-service-admin\") public interface UserFacade { @RequestMapping(\"user/add\") R add(@RequestBody F&lt;UserDTO> f); @RequestMapping(\"user/test\") R test(); } 调用单体服务如果调用一个独立的服务。一般可以直接使用httpUtil直接调用。使用feign调用： @FeignClient(url = \"${userUrl}\") public interface UserFacade { @RequestMapping(\"user/add\") R add(@RequestBody F&lt;UserDTO> f); @RequestMapping(\"user/test\") R test(); } 同时在application.yml配置url userUrl: http://10.10.10.10:1010","categories":[{"name":"Feign","slug":"Feign","permalink":"https://gmaya.top/categories/Feign/"}],"tags":[{"name":"Feign","slug":"Feign","permalink":"https://gmaya.top/tags/Feign/"}]},{"title":"手撕Redis6.0","slug":"20200522","date":"2020-05-22T03:24:21.000Z","updated":"2020-08-12T08:03:44.774Z","comments":true,"path":"2020/20200522/","link":"","permalink":"https://gmaya.top/2020/20200522/","excerpt":"","text":"简介 Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。性能极高： Redis能读的速度是110000次/s,写的速度是81000次/s 。 安装Linux下安装Redis。去年好像写过一次。。。传送门 下载本次下载的是Redis官网最新稳定版本 # wget http://download.redis.io/releases/redis-6.0.3.tar.gz 解压# tar -zxvf redis-6.0.3.tar.gz 注意：解压完成，尽量检查一下Redis所依赖的gcc版本，版本过低会编译失败。需要gcc 4.9以上。 升级gcc版本： // 检查gcc版本 # gcc -v // 升级gcc到9 # yum -y install centos-release-scl # yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils # scl enable devtoolset-9 bash // 以上为临时使用，如果长期使用执行下方： # echo \"source /opt/rh/devtoolset-9/enable\" >>/etc/profile 编译进入redis-6.0.3文件夹，进行编译。 # make PREFIX=/usr/local/redis install 安装成功会出现： Hint: It's a good idea to run 'make test' 然后执行make test测试一下。最终出现： All tests passed without errors! 注：如果出现tcl之类的错误，解决办法： # yum install tcl 启动和关闭直接启动：注意自己的.conf文件和/bin/的路径 # ./bin/redis-server ./redis-6.0.3/redis.conf 后台启动：编辑redis.conf，将daemonize on修改为 daemonize yes # vi redis.conf 直接输入/daemonize 快速搜索，n是寻找下一个。找到位置之后，输入a开启编辑，然后Esc,输入:wq保存退出。再次使用上面的启动命令即可。 查看是否启动成功 # ps -ef | grep redis 简单使用 # ./bin/redis-cli 关闭方式一 # ./bin/redis-cli shutdown 关闭方式二先查找出Redis的端口号，然后直接kill掉 # ps -ef | grep redis -9 代表强制 # kill -9 39491 配置用户名和密码版本6.0之前编辑redis.conf，将requirepass 注释打开，后面跟上自己想要设置的密码。重启Redis即可客户端连接输入密码的两种方式：方式一：先进入客户端，然后输入密码。 [root@localhost redis]# ./bin/redis-cli 127.0.0.1:6379> auth admin 方式二：直接进入并输入密码 # ./bin/redis-cli -a admin 版本6.0之后提供了ACL,可以设置用户名和密码。官方文档https://redis.io/topics/acl如果只是设置密码，那么用户名就是“default”创建一个适用于生产环境的用户： 假如我这个用户redis环境主要用于gmaya-shop项目那么我规定，这个用户使用的时候，所有存放的key必须以gmaya-shop:开头，不然不允许访问。只允许此用户使用get，和set命令。 127.0.0.1:6379> acl setuser gmayashopredis on >gmayapassword +get +set ~gmaya-shop:* 用户名：gmayashopredis 密码：gmayapassword 可用命令：get，set 可操作的key： gmaya-shop:作为前缀的key 此时设置两个key 127.0.0.1:6379> set aaa 111 OK 127.0.0.1:6379> set gmaya-shop:userid 1001 OK 切换gmayashopredis 用户 127.0.0.1:6379> auth gmayashopredis gmayapassword 测试 127.0.0.1:6379> get aaa (error) NOPERM this user has no permissions to access one of the keys used as arguments 127.0.0.1:6379> get gmaya-shop:userid \"1001\" ACL常用命令查看当前用户: 127.0.0.1:6379> acl getuser gmayashopredis 1) \"flags\" 2) 1) \"on\" 3) \"passwords\" 4) 1) \"74a3fbd0037d8c4c44137eb226451df5e5458446a768df424da0fd2a9938f7ba\" 5) \"commands\" 6) \"-@all +set +get\" 7) \"keys\" 8) 1) \"gmaya-shop:*\" 查看所有命令： 127.0.0.1:6379> acl cat 增加命令减少命令： 127.0.0.1:6379> acl setuser gmayashopredis -get OK 127.0.0.1:6379> acl setuser gmayashopredis +hget OK 127.0.0.1:6379> acl getuser gmayashopredis 1) \"flags\" 2) 1) \"on\" 3) \"passwords\" 4) 1) \"74a3fbd0037d8c4c44137eb226451df5e5458446a768df424da0fd2a9938f7ba\" 5) \"commands\" 6) \"-@all +set +hget\" 7) \"keys\" 8) 1) \"gmaya-shop:*\" 刷新key的范围： 127.0.0.1:6379> acl setuser gmayashopredis resetkeys ~gmaya:* OK 127.0.0.1:6379> acl getuser gmayashopredis 1) \"flags\" 2) 1) \"on\" 3) \"passwords\" 4) 1) \"74a3fbd0037d8c4c44137eb226451df5e5458446a768df424da0fd2a9938f7ba\" 5) \"commands\" 6) \"-@all +set +hget\" 7) \"keys\" 8) 1) \"gmaya:*\" 直接添加key： 127.0.0.1:6379> acl setuser gmayashopredis ~shop:* OK 127.0.0.1:6379> acl getuser gmayashopredis 1) \"flags\" 2) 1) \"on\" 3) \"passwords\" 4) 1) \"74a3fbd0037d8c4c44137eb226451df5e5458446a768df424da0fd2a9938f7ba\" 5) \"commands\" 6) \"-@all +set +hget\" 7) \"keys\" 8) 1) \"gmaya:*\" 2) \"shop:*\" 添加密码减少密码 添加密码123456，减少密码gmayapassword，一个用户允许多个密码同时存在 127.0.0.1:6379> acl setuser gmayashopredis >123456 OK 127.0.0.1:6379> acl setuser gmayashopredis &lt;gmayapassword 多线程这次6.0版本加入了多线程模块。只有两个配置参数：默认都是注释掉的。也就是不开启多线程 # io-threads 4 # 开启线程数 # io-threads-do-reads no # 是否开启多线程 通过下面的官方配置里面的介绍分析： Redis主要是单线程的，但是也有一些特定的线程操作，比如断开链接、缓慢的I/O访问和其他在侧线程上执行的操作。 默认情况下，线程是禁用的，我们建议只在拥有至少4个或更多内核的机器上启用线程，而保留至少一个备用内核。使用8个以上的线程不太可能有太大的帮助。我们还建议仅当您确实存在性能问题时才使用线程I/O，因为Redis实例能够使用相当大的CPU时间百分比，否则使用此特性是没有意义的。 例如，如果你有4个内核，尝试使用2或3个I/O线程，如果你有8个内核，尝试使用6个线程。 通常多线程读取不会有太大帮助。 Redis6.0配置文件解读自己把redis配置文件拷贝出来一份，一行一行翻译一下。如果有不对的，希望指出来，虚心学习。 redis.config文件：版本号：6.0.3注意：你可能要花很长时间来阅读这个配置。 TODO:有几个重点的地方，我还没来得及深入研究，大致已经清楚，后期肯定会写的。最近辞职回老家河南找工作了。 主从复制 哨兵（Sentinel） Redis Cluster # Redis configuration file example. # # Note that in order to read the configuration file, Redis must be # started with the file path as first argument: # GMaya总结: redis在启动的时候,指定固定的配置文件启动。 # ./redis-server /path/to/redis.conf # Note on units: when memory size is needed, it is possible to specify # it in the usual form of 1k 5GB 4M and so forth: # # 1k => 1000 bytes # 1kb => 1024 bytes # 1m => 1000000 bytes # 1mb => 1024*1024 bytes # 1g => 1000000000 bytes # 1gb => 1024*1024*1024 bytes # # units are case insensitive so 1GB 1Gb 1gB are all the same. ################################## INCLUDES ################################### # Include one or more other config files here. This is useful if you # have a standard template that goes to all Redis servers but also need # to customize a few per-server settings. Include files can include # other files, so use this wisely. # # Notice option \"include\" won't be rewritten by command \"CONFIG REWRITE\" # from admin or Redis Sentinel. Since Redis always uses the last processed # line as value of a configuration directive, you'd better put includes # at the beginning of this file to avoid overwriting config change at runtime. # # If instead you are interested in using includes to override configuration # options, it is better to use include as the last line. # 引入其它配置文件 # include /path/to/local.conf # include /path/to/other.conf ################################## MODULES ##################################### # Load modules at startup. If the server is not able to load modules # it will abort. It is possible to use multiple loadmodule directives. # # loadmodule /path/to/my_module.so # loadmodule /path/to/other_module.so ################################## NETWORK ##################################### # By default, if no \"bind\" configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \"bind\" configuration directive, followed by one or more IP addresses. # 默认情况下，如果没有指定“bind”配置指令，则Redis侦听 # 对于来自服务器上可用的所有网络接口的连接。 # 可以只监听一个或多个选定的接口 # “bind”配置指令，后面跟着一个或多个IP地址。 # Examples: # # bind 192.168.1.100 10.0.0.1 # bind 127.0.0.1 ::1 # # ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the # internet, binding to all the interfaces is dangerous and will expose the # instance to everybody on the internet. So by default we uncomment the # following bind directive, that will force Redis to listen only into # the IPv4 loopback interface address (this means Redis will be able to # accept connections only from clients running into the same computer it # is running). # 如果运行Redis的计算机直接暴露于 # 绑定到所有接口是危险的，将会暴露 # 互联网上的每个人。默认情况下，我们取消注释 # 遵循绑定指令，这将迫使Redis只监听 # IPv4环回接口地址(这意味着Redis将能够 # 只接受来自与it运行在同一台计算机上的客户机的连接 # 正在运行)。 # IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES # JUST COMMENT THE FOLLOWING LINE. # 如果您确定希望实例侦听所有接口，注释下面的行。 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ bind 127.0.0.1 # Protected mode is a layer of security protection, in order to avoid that # Redis instances left open on the internet are accessed and exploited. # GMaya总结： 保护模式开启时，需配置`bind ip`或者设置访问密码。 # 保护模式是一种安全防护层，以避免这种情况的发生 # 在internet上打开的Redis实例被访问和利用。 # When protected mode is on and if: # 当保护模式开启时，如果: # 1) The server is not binding explicitly to a set of addresses using the # \"bind\" directive. # 服务器没有使用“bind”指令显式地绑定到一组地址。，也就是说bind没有指定远程ip # 2) No password is configured. # 没有配置密码。 # The server only accepts connections from clients connecting from the # IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain # sockets. # 服务器只接受从IPv4和IPv6环回地址127.0.0.1和::1，来自Unix域套接字。 # By default protected mode is enabled. You should disable it only if # you are sure you want clients from other hosts to connect to Redis # even if no authentication is configured, nor a specific set of interfaces # are explicitly listed using the \"bind\" directive. # 默认情况下，受保护模式是启用的。您应该仅在以下情况下禁用它 # 您确定希望来自其他主机的客户端连接到Redis # 即使没有配置任何身份验证，也没有一组特定的接口 # 使用“bind”指令显式列出。 protected-mode yes # Accept connections on the specified port, default is 6379 (IANA #815344). # If port 0 is specified Redis will not listen on a TCP socket. # 接受指定端口上的连接，默认为6379 (IANA #815344)。 # 如果端口0被指定，Redis将不会监听TCP套接字。 port 6379 # TCP listen() backlog. # 此参数和`somaxconn`确定了`TCP`连接中已完成队列(完成三次握手之后)的长度。 # 取两者最小值。当高并发的时候，可以考虑增加`somaxconn`的值，然后增加`tcp-backlog`。 # In high requests-per-second environments you need an high backlog in order # to avoid slow clients connections issues. Note that the Linux kernel # will silently truncate it to the value of /proc/sys/net/core/somaxconn so # make sure to raise both the value of somaxconn and tcp_max_syn_backlog # in order to get the desired effect. # 在每秒请求数很高的环境中，您需要按顺序进行高backlog # 以避免慢速客户端连接问题。注意Linux内核 # 是否会静默地将其截断为/proc/sys/net/core/somaxconn的值 # 确保同时提高somaxconn和tcp_max_syn_backlog的值 # 以达到预期的效果。 tcp-backlog 511 # Unix socket. # # Specify the path for the Unix socket that will be used to listen for # incoming connections. There is no default, so Redis will not listen # on a unix socket when not specified. # # unixsocket /tmp/redis.sock # unixsocketperm 700 # Close the connection after a client is idle for N seconds (0 to disable) # 在客户端空闲N秒后关闭连接(0表示禁用) timeout 0 # TCP keepalive. # 表示将周期性的使用SO_KEEPALIVE检测客户端是否还处于健康状态，避免服务器一直阻塞。 # If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence # of communication. This is useful for two reasons: # 如果非零，使用SO_KEEPALIVE在客户端不存在时发送TCP ack # 的沟通。这样做有两个原因: # 1) Detect dead peers. # 发现死去的客户端 # 2) Take the connection alive from the point of view of network # equipment in the middle. # 从网络设备的角度来看，连接在中间是活的 # On Linux, the specified value (in seconds) is the period used to send ACKs. # Note that to close the connection the double of the time is needed. # On other kernels the period depends on the kernel configuration. # 在Linux上，指定的值(以秒为单位)是用于发送ack的周期。 # 注意，关闭连接需要双倍的时间。 # 在其他内核上，周期取决于内核配置。 # A reasonable value for this option is 300 seconds, which is the new # Redis default starting with Redis 3.2.1. # 这个选项的合理值是300秒，这是新的值 # Redis默认从Redis 3.2.1开始。 tcp-keepalive 300 ################################# TLS/SSL ##################################### # By default, TLS/SSL is disabled. To enable it, the \"tls-port\" configuration # directive can be used to define TLS-listening ports. To enable TLS on the # default port, use: # # port 0 # tls-port 6379 # Configure a X.509 certificate and private key to use for authenticating the # server to connected clients, masters or cluster peers. These files should be # PEM formatted. # # tls-cert-file redis.crt # tls-key-file redis.key # Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange: # # tls-dh-params-file redis.dh # Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL # clients and peers. Redis requires an explicit configuration of at least one # of these, and will not implicitly use the system wide configuration. # # tls-ca-cert-file ca.crt # tls-ca-cert-dir /etc/ssl/certs # By default, clients (including replica servers) on a TLS port are required # to authenticate using valid client side certificates. # # It is possible to disable authentication using this directive. # # tls-auth-clients no # By default, a Redis replica does not attempt to establish a TLS connection # with its master. # # Use the following directive to enable TLS on replication links. # # tls-replication yes # By default, the Redis Cluster bus uses a plain TCP connection. To enable # TLS for the bus protocol, use the following directive: # # tls-cluster yes # and include \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\", \"TLSv1.3\" (OpenSSL >= 1.1.1) # # tls-protocols TLSv1.2 # Configure allowed ciphers. See the ciphers(1ssl) manpage for more information # about the syntax of this string. # # Note: this configuration applies only to &lt;= TLSv1.2. # # tls-ciphers DEFAULT:!MEDIUM # Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more # information about the syntax of this string, and specifically for TLSv1.3 # ciphersuites. # # tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256 # When choosing a cipher, use the server's preference instead of the client # preference. By default, the server follows the client's preference. # # tls-prefer-server-ciphers yes ################################# GENERAL ##################################### # By default Redis does not run as a daemon. Use 'yes' if you need it. # Note that Redis will write a pid file in /var/run/redis.pid when daemonized. # 默认情况下，Redis不作为守护进程运行。如果你需要，使用“yes”。 # 请注意，Redis将在/var/run/ Redis中写入一个pid文件。监控pid。 daemonize yes # If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # 如果你从upstart或systemd运行Redis, Redis可以与你的管理树交互。选项: # supervised no - no supervision interaction 没有监督互动 # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # 通过将Redis置于SIGSTOP模式来启动信号 # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # signal systemd将READY = 1写入$ NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # 检测upstart或systemd方法基于 UPSTART_JOB或NOTIFY_SOCKET环境变量 # Note: these supervision methods only signal \"process is ready.\" # 注意:这些监督方法只表明“过程准备好了”。 # They do not enable continuous liveness pings back to your supervisor. supervised no # If a pid file is specified, Redis writes it where specified at startup # and removes it at exit. # 如果指定了pid文件，Redis会在启动时将其写入指定的位置 # 并在退出时删除它。 # When the server runs non daemonized, no pid file is created if none is # specified in the configuration. When the server is daemonized, the pid file # is used even if not specified, defaulting to \"/var/run/redis.pid\". # 当服务器运行非守护进程时，如果没有pid文件，则不创建pid文件 # 在配置中指定。当服务器以守护进程运行时，pid文件 # 即使未指定，默认也使用“/var/run/redis.pid”。 # Creating a pid file is best effort: if Redis is not able to create it # nothing bad happens, the server will start and run normally. # 创建一个pid文件是最好的努力，如果Redis不能创建它 # 没有什么不好的事情发生，服务器将正常启动和运行。（最好创建它，但是不创建也没事。） pidfile /var/run/redis_6379.pid # Specify the server verbosity level. # 指定服务器详细日志级别。 # This can be one of: # debug (a lot of information, useful for development/testing) # 大量的信息，对开发/测试非常有用 # verbose (many rarely useful info, but not a mess like the debug level) # 很多很少有用的信息，但不像调试级别那么混乱 # notice (moderately verbose, what you want in production probably) # 有些冗长，这可能是您在生产环境中需要的 # warning (only very important / critical messages are logged) # 只记录非常重要/关键的消息 loglevel notice # Specify the log file name. Also the empty string can be used to force # Redis to log on the standard output. Note that if you use standard # output for logging but daemonize, logs will be sent to /dev/null # 指定日志文件名。此外，还可以使用空字符串强制Redis登录标准输出。 # 注意，如果使用标准输出进行日志记录，但是使用守护进程运行，那么日志将被发送到/dev/null logfile \"\" # To enable logging to the system logger, just set 'syslog-enabled' to yes, # and optionally update the other syslog parameters to suit your needs. # 要启用系统日志记录器的日志功能，只需将'syslog-enabled'设置为yes， # 并可选地更新其他syslog参数以满足您的需要。 # syslog-enabled no # Specify the syslog identity. # 指定syslog标识。 # syslog-ident redis # Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7. # 指定syslog功能。必须是USER或介于LOCAL0-LOCAL7之间。 # syslog-facility local0 # Set the number of databases. The default database is DB 0, you can select # a different one on a per-connection basis using SELECT &lt;dbid> where # dbid is a number between 0 and 'databases'-1 # 设置数据库的数量。默认数据库是DB 0，您可以选择 # 在每个连接的基础上使用SELECT &lt;dbid>，其中 # dbid是一个介于0和‘databases’-1之间的数字。（也就是0到15） databases 16 # By default Redis shows an ASCII art logo only when started to log to the # standard output and if the standard output is a TTY. Basically this means # that normally a logo is displayed only in interactive sessions. # 默认情况下，Redis只在开始登录时显示一个ASCII art徽标 # 标准输出，如果标准输出是TTY。基本上这意味着 # 标志通常只在交互式会话中显示。 # However it is possible to force the pre-4.0 behavior and always show a # ASCII art logo in startup logs by setting the following option to yes. # 然而，强制执行4.0之前的行为并总是显示a是可能的 # 通过将以下选项设置为yes，可以在启动日志中显示ASCII art徽标。 always-show-logo yes ################################ SNAPSHOTTING ################################ # 持久化：RDB快照！！！！重点！ # Save the DB on disk: # 将数据库保存在磁盘上: # save &lt;seconds> &lt;changes> # # Will save the DB if both the given number of seconds and the given # number of write operations against the DB occurred. # 如果给定的秒数和给定的对该DB的写操作数同时发生，则将保存该DB。 # In the example below the behaviour will be to save: # 在下面的例子中，行为将被保存: # after 900 sec (15 min) if at least 1 key changed # 在900秒(15分钟)后，如果至少改变了一个键 # after 300 sec (5 min) if at least 10 keys changed # 300秒后(5分钟)如果至少10个键改变 # after 60 sec if at least 10000 keys changed # 60秒后如果至少10000个键改变 # Note: you can disable saving completely by commenting out all \"save\" lines. # 注意:您可以通过注释掉所有“保存”行来完全禁用保存。 # It is also possible to remove all the previously configured save # points by adding a save directive with a single empty string argument # like in the following example: # 也可以删除以前配置的所有save # 通过添加一个带有单个空字符串参数的save指令来获得 # 就像下面的例子: # save \"\" save 900 1 save 300 10 save 60 10000 # By default Redis will stop accepting writes if RDB snapshots are enabled # (at least one save point) and the latest background save failed. # This will make the user aware (in a hard way) that data is not persisting # on disk properly, otherwise chances are that no one will notice and some # disaster will happen. # 默认情况下，如果启用了RDB快照, # (至少一个保存点)和最新的后台保存失败,Redis将停止接受写操作. # 这将使用户(以一种困难的方式)意识到数据不是持久的 # 正确地存储在磁盘上，否则没有人会注意到灾难将会发生。 # If the background saving process will start working again Redis will # automatically allow writes again. # 如果后台保存过程将再次开始工作，Redis将自动允许再次写入。 # However if you have setup your proper monitoring of the Redis server # and persistence, you may want to disable this feature so that Redis will # continue to work as usual even if there are problems with disk, # permissions, and so forth. # 但是，如果您已经设置了对Redis服务器的适当监视 # 和持久性，你可能想要禁用这个功能，这样Redis会 # 继续正常工作，即使有问题的磁盘， # 权限等等。 # GMaya总结：如果存储数据（持久化）失败，则停止写入。 stop-writes-on-bgsave-error yes # Compress string objects using LZF when dump .rdb databases? # 在转储.rdb数据库时使用LZF压缩字符串对象? # For default that's set to 'yes' as it's almost always a win. # 默认情况下，它被设置为“yes”。 # If you want to save some CPU in the saving child set it to 'no' but # the dataset will likely be bigger if you have compressible values or keys. # GMaya总结：是否开启对rdb持久化文件的压缩 rdbcompression yes # Since version 5 of RDB a CRC64 checksum is placed at the end of the file. # This makes the format more resistant to corruption but there is a performance # hit to pay (around 10%) when saving and loading RDB files, so you can disable it # for maximum performances. # 是否CRC64校验rdb文件，会有一定的性能损失（大概10%）。 # RDB files created with checksum disabled have a checksum of zero that will # tell the loading code to skip the check. # 禁用校验和创建的RDB文件的校验和为零 # 告诉加载代码跳过检查。 rdbchecksum yes # The filename where to dump the DB # rdb文件的名字。 dbfilename dump.rdb # Remove RDB files used by replication in instances without persistence # enabled. By default this option is disabled, however there are environments # where for regulations or other security concerns, RDB files persisted on # disk by masters in order to feed replicas, or stored on disk by replicas # in order to load them for the initial synchronization, should be deleted # ASAP. Note that this option ONLY WORKS in instances that have both AOF # and RDB persistence disabled, otherwise is completely ignored. # 删除实例中复制使用的RDB文件，但不删除持久性 # 启用。默认情况下，这个选项是禁用的，但是有一些环境 # 对于法规或其他安全问题，RDB文件在哪里持久存在 # 主服务器通过磁盘来提供副本，或通过副本存储在磁盘上 # 为了加载它们进行初始同步，应该删除它们 # 尽快。注意，这个选项只在同时具有AOF的实例中起作用 # 并且禁用了RDB持久性，否则将完全忽略。 # An alternative (and sometimes better) way to obtain the same effect is # to use diskless replication on both master and replicas instances. However # in the case of replicas, diskless is not always an option. # 另一种(有时是更好的)达到同样效果的方法是 #在主实例和副本实例上使用无磁盘复制。然而 #在副本的情况下，无磁盘并不总是一个选择。 rdb-del-sync-files no # The working directory. # 数据库存放目录。 # The DB will be written inside this directory, with the filename specified # above using the 'dbfilename' configuration directive. # DB将在此目录中写入，并指定文件名 # 上面使用'dbfilename'配置指令。 # The Append Only File will also be created inside this directory. # 仅追加文件也将在此目录中创建（AOF文件也在这个目录创建） # Note that you must specify a directory here, not a file name. # 注意，这里必须指定一个目录，而不是文件名。 dir ./ ################################# REPLICATION ################################# # Master-Replica replication. Use replicaof to make a Redis instance a copy of # another Redis server. A few things to understand ASAP about Redis replication. # 主从复制 # +------------------+ +---------------+ # | Master | ---> | Replica | # | (receive writes) | | (exact copy) | # +------------------+ +---------------+ # # 1) Redis replication is asynchronous, but you can configure a master to # stop accepting writes if it appears to be not connected with at least # a given number of replicas. # Redis复制是异步的，但是您可以配置一个主服务器来停止接受写操作，如果它看起来至少没有连接到给定数量的副本。 # 2) Redis replicas are able to perform a partial resynchronization with the # master if the replication link is lost for a relatively small amount of # time. You may want to configure the replication backlog size (see the next # sections of this file) with a sensible value depending on your needs. # 3) Replication is automatic and does not need user intervention. After a # network partition replicas automatically try to reconnect to masters # and resynchronize with them. # GMaya总结：5.0及之后版本replicaof代替了slaveof. # 在这里配置属于永久配置，replicaof 主ip 主端口。只有从的需要配置，主的不需要 # replicaof &lt;masterip> &lt;masterport> # If the master is password protected (using the \"requirepass\" configuration # directive below) it is possible to tell the replica to authenticate before # starting the replication synchronization process, otherwise the master will # refuse the replica request. # 如果主服务器是密码保护的(使用下面的“requirepass”配置指令)，可以在启动复制同步进程之前告诉副本进行身份验证，否则主服务器将拒绝副本请求。 # 也就是如果你的主设置了密码，这个地方需要写上主的密码，才能连接上 # masterauth &lt;master-password> # # However this is not enough if you are using Redis ACLs (for Redis version # 6 or greater), and the default user is not capable of running the PSYNC # command and/or other commands needed for replication. In this case it's # better to configure a special user to use with replication, and specify the # masteruser configuration as such: # 但是，如果您使用的是Redis acl(适用于Redis版本6或更高版本)，并且默认用户不能运行PSYNC命令和/或复制所需的其他命令，那么这还不够。 # 在这种情况下，最好配置一个特殊用户与复制一起使用，并指定masteruser配置如下: # masteruser &lt;username> # # When masteruser is specified, the replica will authenticate against its # master using the new AUTH form: AUTH &lt;username> &lt;password>. # When a replica loses its connection with the master, or when the replication # is still in progress, the replica can act in two different ways: # 当副本失去与主副本的连接时，或者复制仍在进行时，副本可以以两种不同的方式执行操作: # 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will # still reply to client requests, possibly with out of date data, or the # data set may just be empty if this is the first synchronization. # 如果replica-serve-stale-data被设置为“yes”(默认值)，则副本将执行此操作 # 仍然响应客户端请求，可能是数据过期，或者如果这是第一次同步，那么数据集可能是空的。 # 2) if replica-serve-stale-data is set to 'no' the replica will reply with # an error \"SYNC with master in progress\" to all the kind of commands # but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, # SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, # COMMAND, POST, HOST: and LATENCY. # 如果replica-serve-stale-data被设置为“no” # 除了INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG，订阅，取消订阅，PSUBSCRIBE, PUNSUBSCRIBE，发布，PUBSUB，命令，发布，主机:和延迟。 # 其他的都会返回“与主进程同步”错误 replica-serve-stale-data yes # You can configure a replica instance to accept writes or not. Writing against # a replica instance may be useful to store some ephemeral data (because data # written on a replica will be easily deleted after resync with the master) but # may also cause problems if clients are writing to it because of a # misconfiguration. # # Since Redis 2.6 by default replicas are read-only. # 默认情况下Redis 2.6的副本是只读的。 # Note: read only replicas are not designed to be exposed to untrusted clients # on the internet. It's just a protection layer against misuse of the instance. # Still a read only replica exports by default all the administrative commands # such as CONFIG, DEBUG, and so forth. To a limited extent you can improve # security of read only replicas using 'rename-command' to shadow all the # administrative / dangerous commands. # 也就是从库只允许读，不允许写，读写分离啊 replica-read-only yes # Replication SYNC strategy: disk or socket. # 复制同步策略:磁盘或套接字。 # New replicas and reconnecting replicas that are not able to continue the # replication process just receiving differences, need to do what is called a # \"full synchronization\". An RDB file is transmitted from the master to the # replicas. # # The transmission can happen in two different ways: # # 1) Disk-backed: The Redis master creates a new process that writes the RDB # file on disk. Later the file is transferred by the parent # process to the replicas incrementally. # 2) Diskless: The Redis master creates a new process that directly writes the # RDB file to replica sockets, without touching the disk at all. # # With disk-backed replication, while the RDB file is generated, more replicas # can be queued and served with the RDB file as soon as the current child # producing the RDB file finishes its work. With diskless replication instead # once the transfer starts, new replicas arriving will be queued and a new # transfer will start when the current one terminates. # # When diskless replication is used, the master waits a configurable amount of # time (in seconds) before starting the transfer in the hope that multiple # replicas will arrive and the transfer can be parallelized. # # With slow disks and fast (large bandwidth) networks, diskless replication # works better. # 对于慢速磁盘和快速(大带宽)网络，无磁盘复制工作得更好。 # 主从数据复制是否使用无硬盘复制功能。 repl-diskless-sync no # When diskless replication is enabled, it is possible to configure the delay # the server waits in order to spawn the child that transfers the RDB via socket # to the replicas. # 当启用无磁盘复制时，可以配置服务器等待的延迟，以生成通过套接字传输RDB的子节点的副本。 # # This is important since once the transfer starts, it is not possible to serve # new replicas arriving, that will be queued for the next RDB transfer, so the # server waits a delay in order to let more replicas arrive. # # The delay is specified in seconds, and by default is 5 seconds. To disable # it entirely just set it to 0 seconds and the transfer will start ASAP. # diskless复制的延迟时间 repl-diskless-sync-delay 5 # ----------------------------------------------------------------------------- # WARNING: RDB diskless load is experimental. Since in this setup the replica # does not immediately store an RDB on disk, it may cause data loss during # failovers. RDB diskless load + Redis modules not handling I/O reads may also # cause Redis to abort in case of I/O errors during the initial synchronization # stage with the master. Use only if your do what you are doing. # ----------------------------------------------------------------------------- # # Replica can load the RDB it reads from the replication link directly from the # socket, or store the RDB to a file and read that file after it was completely # recived from the master. # # In many cases the disk is slower than the network, and storing and loading # the RDB file may increase replication time (and even increase the master's # Copy on Write memory and salve buffers). # However, parsing the RDB file directly from the socket may mean that we have # to flush the contents of the current database before the full rdb was # received. For this reason we have the following options: # # \"disabled\" - Don't use diskless load (store the rdb file to the disk first) # 不要使用无磁盘加载(首先将rdb文件存储到磁盘) # \"on-empty-db\" - Use diskless load only when it is completely safe. # 只有在完全安全的情况下才使用无磁盘加载。 # \"swapdb\" - Keep a copy of the current db contents in RAM while parsing # the data directly from the socket. note that this requires # sufficient memory, if you don't have it, you risk an OOM kill. repl-diskless-load disabled # Replicas send PINGs to server in a predefined interval. It's possible to # change this interval with the repl_ping_replica_period option. The default # value is 10 seconds. # # The following option sets the replication timeout for: # # 1) Bulk transfer I/O during SYNC, from the point of view of replica. # 2) Master timeout from the point of view of replicas (data, pings). # 3) Replica timeout from the point of view of masters (REPLCONF ACK pings). # # It is important to make sure that this value is greater than the value # specified for repl-ping-replica-period otherwise a timeout will be detected # every time there is low traffic between the master and the replica. # 复制连接超时时间。 # repl-timeout 60 # Disable TCP_NODELAY on the replica socket after SYNC? # # If you select \"yes\" Redis will use a smaller number of TCP packets and # less bandwidth to send data to replicas. But this can add a delay for # the data to appear on the replica side, up to 40 milliseconds with # Linux kernels using a default configuration. # # If you select \"no\" the delay for data to appear on the replica side will # be reduced but more bandwidth will be used for replication. # # By default we optimize for low latency, but in very high traffic conditions # or when the master and replicas are many hops away, turning this to \"yes\" may # be a good idea. # 是否禁止复制tcp链接的tcp nodelay参数 repl-disable-tcp-nodelay no # Set the replication backlog size. The backlog is a buffer that accumulates # replica data when replicas are disconnected for some time, so that when a # replica wants to reconnect again, often a full resync is not needed, but a # partial resync is enough, just passing the portion of data the replica # missed while disconnected. # # The bigger the replication backlog, the longer the time the replica can be # disconnected and later be able to perform a partial resynchronization. # 复制待办事项列表越大，副本断开连接的时间就越长，并且以后能够执行部分重新同步。 # The backlog is only allocated once there is at least a replica connected. # 积压只在至少连接了一个副本时才分配。 # 复制缓冲区大小 # repl-backlog-size 1mb # After a master has no longer connected replicas for some time, the backlog # will be freed. The following option configures the amount of seconds that # need to elapse, starting from the time the last replica disconnected, for # the backlog buffer to be freed. # # Note that replicas never free the backlog for timeout, since they may be # promoted to masters later, and should be able to correctly \"partially # resynchronize\" with the replicas: hence they should always accumulate backlog. # # A value of 0 means to never release the backlog. # 0的值意味着永远不释放积压。 # master没有slave一段时间会释放复制缓冲区的内存，repl-backlog-ttl用来设置该时间长度。单位为秒。 # repl-backlog-ttl 3600 # The replica priority is an integer number published by Redis in the INFO # output. It is used by Redis Sentinel in order to select a replica to promote # into a master if the master is no longer working correctly. # 副本优先级是一个由Redis在信息中发布的整数输出。Redis Sentinel使用它来选择一个副本，以便在主服务器不再正常工作时将副本提升到主服务器。 # A replica with a low priority number is considered better for promotion, so # for instance if there are three replicas with priority 10, 100, 25 Sentinel # will pick the one with priority 10, that is the lowest. # 低优先级的副本被认为更适合升级，例如，如果有3个优先级为10,100，25的副本，Sentinel将选择优先级为10的副本，这是最低的。 # However a special priority of 0 marks the replica as not able to perform the # role of master, so a replica with priority of 0 will never be selected by # Redis Sentinel for promotion. # 但是一个特殊的优先级为0的副本将不能执行master的角色，所以一个优先级为0的副本将永远不会被Redis Sentinel选中进行升级。 # # By default the priority is 100. # 默认情况下，优先级是100。 # 也就是当主机挂了，从机那个优先级低，那个就会被推选为主机。 replica-priority 100 # It is possible for a master to stop accepting writes if there are less than # N replicas connected, having a lag less or equal than M seconds. # 如果连接的副本少于N个，延迟小于或等于M秒，那么主机可能会停止接受写操作。 # The N replicas need to be in \"online\" state. # N个副本需要处于“在线”状态。 # The lag in seconds, that must be &lt;= the specified value, is calculated from # the last ping received from the replica, that is usually sent every second. # N个副本以秒为单位的延迟(必须&lt;=指定的值)是从副本接收的最后一个ping，通常每秒发送一次。需要处于“在线”状态。 # This option does not GUARANTEE that N replicas will accept the write, but # will limit the window of exposure for lost writes in case not enough replicas # are available, to the specified number of seconds. # 此选项不保证N个副本将接受写操作，但在没有足够的副本可用的情况下，将把丢失的写操作的曝光窗口限制在指定的秒数内。 # For example to require at least 3 replicas with a lag &lt;= 10 seconds use: # 例如，需要至少3个 从机 延迟小于等于 10秒 才会写入 # min-replicas-to-write 3 # min-replicas-max-lag 10 # # Setting one or the other to 0 disables the feature. # 将其中一个设置为0将禁用该特性。 # By default min-replicas-to-write is set to 0 (feature disabled) and # min-replicas-max-lag is set to 10. # 默认情况下，最小复制写入设置为0(禁用特性) # 最小复制最大延迟设置为10。 # A Redis master is able to list the address and port of the attached # replicas in different ways. For example the \"INFO replication\" section # offers this information, which is used, among other tools, by # Redis Sentinel in order to discover replica instances. # Redis主服务器能够以不同的方式列出所附副本的地址和端口。例如，“信息复制”部分提供了这些信息，Redis Sentinel使用这些信息和其他工具来发现副本实例。 # Another place where this info is available is in the output of the # \"ROLE\" command of a master. # # The listed IP and address normally reported by a replica is obtained # in the following way: # # IP: The address is auto detected by checking the peer address # of the socket used by the replica to connect with the master. # # Port: The port is communicated by the replica during the replication # handshake, and is normally the port that the replica is using to # listen for connections. # # However when port forwarding or Network Address Translation (NAT) is # used, the replica may be actually reachable via different IP and port # pairs. The following two options can be used by a replica in order to # report to its master a specific set of IP and port, so that both INFO # and ROLE will report those values. # # There is no need to use both the options if you need to override just # the port or the IP address. # # replica-announce-ip 5.5.5.5 # replica-announce-port 1234 ############################### KEYS TRACKING ################################# # Redis implements server assisted support for client side caching of values. # This is implemented using an invalidation table that remembers, using # 16 millions of slots, what clients may have certain subsets of keys. In turn # this is used in order to send invalidation messages to clients. Please # to understand more about the feature check this page: # # https://redis.io/topics/client-side-caching # # When tracking is enabled for a client, all the read only queries are assumed # to be cached: this will force Redis to store information in the invalidation # table. When keys are modified, such information is flushed away, and # invalidation messages are sent to the clients. However if the workload is # heavily dominated by reads, Redis could use more and more memory in order # to track the keys fetched by many clients. # # For this reason it is possible to configure a maximum fill value for the # invalidation table. By default it is set to 1M of keys, and once this limit # is reached, Redis will start to evict keys in the invalidation table # even if they were not modified, just to reclaim memory: this will in turn # force the clients to invalidate the cached values. Basically the table # maximum size is a trade off between the memory you want to spend server # side to track information about who cached what, and the ability of clients # to retain cached objects in memory. # # If you set the value to 0, it means there are no limits, and Redis will # retain as many keys as needed in the invalidation table. # In the \"stats\" INFO section, you can find information about the number of # keys in the invalidation table at every given moment. # # Note: when key tracking is used in broadcasting mode, no memory is used # in the server side so this setting is useless. # 这个是有关客户端缓存的 # tracking-table-max-keys 1000000 ################################## SECURITY ################################### # 安全，在6.0版本之后，加入了用户名+密码的设置，如果设置密码，用户名为默认。 # 新格式：AUTH &lt;username> &lt;password> 或者 旧格式：AUTH &lt;password> # Warning: since Redis is pretty fast an outside user can try up to # 1 million passwords per second against a modern box. This means that you # should use very strong passwords, otherwise they will be very easy to break. # Note that because the password is really a shared secret between the client # and the server, and should not be memorized by any human, the password # can be easily a long string from /dev/urandom or whatever, so by using a # long and unguessable password no brute force attack will be possible. # Redis ACL users are defined in the following format: # # user &lt;username> ... acl rules ... # # For example: # # user worker +@list +@connection ~jobs:* on >ffa9203c493aa99 # # The special username \"default\" is used for new connections. If this user # has the \"nopass\" rule, then new connections will be immediately authenticated # as the \"default\" user without the need of any password provided via the # AUTH command. Otherwise if the \"default\" user is not flagged with \"nopass\" # the connections will start in not authenticated state, and will require # AUTH (or the HELLO command AUTH option) in order to be authenticated and # start to work. # # The ACL rules that describe what an user can do are the following: # 描述用户可以做什么的ACL规则如下所示 # on Enable the user: it is possible to authenticate as this user.启用用户:可以作为此用户进行身份验证。 # off Disable the user: it's no longer possible to authenticate # with this user, however the already authenticated connections # will still work. # 禁用该用户:不再能够使用该用户进行身份验证，但是已验证的连接仍然可以工作。 # +&lt;command> Allow the execution of that command 允许执行该命令，也就是添加用户命令权限，例：+get +set # -&lt;command> Disallow the execution of that command 不允许执行该命令 # +@&lt;category> Allow the execution of all the commands in such category # with valid categories are like @admin, @set, @sortedset, ... # and so forth, see the full list in the server.c file where # the Redis command table is described and defined. # The special category @all means all the commands, but currently # present in the server, and that will be loaded in the future # via modules. # +&lt;command>|subcommand Allow a specific subcommand of an otherwise # disabled command. Note that this form is not # allowed as negative like -DEBUG|SEGFAULT, but # only additive starting with \"+\". # allcommands Alias for +@all. Note that it implies the ability to execute # all the future commands loaded via the modules system. # nocommands Alias for -@all. # ~&lt;pattern> Add a pattern of keys that can be mentioned as part of # commands. For instance ~* allows all the keys. The pattern # is a glob-style pattern like the one of KEYS. # It is possible to specify multiple patterns. # 添加可作为命令的一部分提到的键的模式。例如~*允许所有的键。模式是一个全局样式的模式，类似于键的模式。可以指定多个模式。 # GMaya总结：也就是允许访问那些key，例子：只允许访问~gmaya:*开头的key。如：gmaya:userid， gmaya:roleid # allkeys Alias for ~* allkeys别名为~* # resetkeys Flush the list of allowed keys patterns.刷新允许的键模式列表。 # >&lt;password> Add this passowrd to the list of valid password for the user. # For example >mypass will add \"mypass\" to the list. # This directive clears the \"nopass\" flag (see later). # 将这个passowrd添加到用户的有效密码列表中。例如，>mypass会将“mypass”添加到列表中。这个指令清除“nopass”标志(参见后面)。 # &lt;&lt;password> Remove this password from the list of valid passwords. 从有效密码列表中删除此密码。 # nopass All the set passwords of the user are removed, and the user # is flagged as requiring no password: it means that every # password will work against this user. If this directive is # used for the default user, every new connection will be # immediately authenticated with the default user without # any explicit AUTH command required. Note that the \"resetpass\" # directive will clear this condition. # 删除用户的所有设置密码，并将用户标记为不需要密码:这意味着每个密码都将对该用户起作用。如果此指令用于默认用户， # 则每个新连接都将立即与默认用户进行身份验证，而不需要任何显式的AUTH命令。注意“resetpass”指令将清除这个条件。 # resetpass Flush the list of allowed passwords. Moreover removes the # \"nopass\" status. After \"resetpass\" the user has no associated # passwords and there is no way to authenticate without adding # some password (or setting it as \"nopass\" later).\\ # 刷新允许的密码列表。此外，删除“nopass”状态。在“resetpass”之后，用户没有相关联的密码，并且没有办法在不添加一些密码(或稍后将其设置为“nopass”)的情况下进行身份验证。 # reset Performs the following actions: resetpass, resetkeys, off, # -@all. The user returns to the same state it has immediately # after its creation. # 执行以下操作:resetpass、resetkeys、off、-@all。用户在创建后立即返回到相同的状态。 # ACL rules can be specified in any order: for instance you can start with # passwords, then flags, or key patterns. However note that the additive # and subtractive rules will CHANGE MEANING depending on the ordering. # For instance see the following example: # # user alice on +@all -DEBUG ~* >somepassword # # This will allow \"alice\" to use all the commands with the exception of the # DEBUG command, since +@all added all the commands to the set of the commands # alice can use, and later DEBUG was removed. However if we invert the order # of two ACL rules the result will be different: # # user alice on -DEBUG +@all ~* >somepassword # # Now DEBUG was removed when alice had yet no commands in the set of allowed # commands, later all the commands are added, so the user will be able to # execute everything. # # Basically ACL rules are processed left-to-right. # # For more information about ACL configuration please refer to # the Redis web site at https://redis.io/topics/acl # ACL LOG # # The ACL Log tracks failed commands and authentication events associated # with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in and consumes memory. There is no limit # to its length.You can reclaim memory with ACL LOG RESET or set a maximum # length below. acllog-max-len 128 # Using an external ACL file # 使用外部ACL文件 # Instead of configuring users here in this file, it is possible to use # a stand-alone file just listing users. The two methods cannot be mixed: # if you configure users here and at the same time you activate the exteranl # ACL file, the server will refuse to start. # 不需要在这个文件中配置用户，可以使用一个单独的文件来列出用户。这两种方法不能混合使用:如果在这里配置用户，同时激活exteranl ACL文件，服务器将拒绝启动。 # The format of the external ACL user file is exactly the same as the # format that is used inside redis.conf to describe users. # 外部ACL用户文件的格式与在redis.conf中用于描述用户的格式完全相同。 # aclfile /etc/redis/users.acl # IMPORTANT NOTE: starting with Redis 6 \"requirepass\" is just a compatiblity # layer on top of the new ACL system. The option effect will be just setting # the password for the default user. Clients will still authenticate using # AUTH &lt;password> as usually, or more explicitly with AUTH default &lt;password> # if they follow the new protocol: both will work. # 密码，如果使用redis需要密码，在这里设置 # requirepass foobared # Command renaming (DEPRECATED). # 命令重命名(弃用)。 # ------------------------------------------------------------------------ # WARNING: avoid using this option if possible. Instead use ACLs to remove # commands from the default user, and put them only in some admin user you # create for administrative purposes. # ------------------------------------------------------------------------ # # It is possible to change the name of dangerous commands in a shared # environment. For instance the CONFIG command may be renamed into something # hard to guess so that it will still be available for internal-use tools # but not available for general clients. # # Example: # # rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52 # # It is also possible to completely kill a command by renaming it into # an empty string: # # rename-command CONFIG \"\" # # Please note that changing the name of commands that are logged into the # AOF file or transmitted to replicas may cause problems. ################################### CLIENTS #################################### # Set the max number of connected clients at the same time. By default # this limit is set to 10000 clients, however if the Redis server is not # able to configure the process file limit to allow for the specified limit # the max number of allowed clients is set to the current file limit # minus 32 (as Redis reserves a few file descriptors for internal uses). # 同时设置最大连接客户端数量。默认情况下这个限制设置为10000个客户, # 但是如果复述,服务器不能配置过程文件限制允许指定限制允许的最大数量的客户设置为（当前文件限制- 32）(redis本身会用到32左右的连接,储备一些为内部使用文件描述符)。 # Once the limit is reached Redis will close all the new connections sending # an error 'max number of clients reached'. # 一旦达到限制，Redis将关闭所有新连接，发送一个错误“达到的客户端最大数量”。 # maxclients 10000 ############################## MEMORY MANAGEMENT ################################ # 内存管理 # Set a memory usage limit to the specified amount of bytes. # When the memory limit is reached Redis will try to remove keys # according to the eviction policy selected (see maxmemory-policy). # # If Redis can't remove keys according to the policy, or if the policy is # set to 'noeviction', Redis will start to reply with errors to commands # that would use more memory, like SET, LPUSH, and so on, and will continue # to reply to read-only commands like GET. # # This option is usually useful when using Redis as an LRU or LFU cache, or to # set a hard memory limit for an instance (using the 'noeviction' policy). # # WARNING: If you have replicas attached to an instance with maxmemory on, # the size of the output buffers needed to feed the replicas are subtracted # from the used memory count, so that network problems / resyncs will # not trigger a loop where keys are evicted, and in turn the output # buffer of replicas is full with DELs of keys evicted triggering the deletion # of more keys, and so forth until the database is completely emptied. # # In short... if you have replicas attached it is suggested that you set a lower # limit for maxmemory so that there is some free RAM on the system for replica # output buffers (but this is not needed if the policy is 'noeviction'). # 指定Redis最大内存限制 # maxmemory &lt;bytes> # MAXMEMORY POLICY: how Redis will select what to remove when maxmemory # is reached. You can select one from the following behaviors: # 当内存使用达到最大值时，redis使用的清除策略。 # volatile-lru -> Evict using approximated LRU, only keys with an expire set. 使用近似LRU，仅使用具有过期设置的键。 # allkeys-lru -> Evict any key using approximated LRU. 使用近似LRU退出任何key。 # volatile-lfu -> Evict using approximated LFU, only keys with an expire set. 使用近似的LFU，仅使用具有过期集的键。 # allkeys-lfu -> Evict any key using approximated LFU. 使用近似的LFU退出任何键。 # volatile-random -> Remove a random key having an expire set. 删除具有过期集的随机key # allkeys-random -> Remove a random key, any key. 删除一个随机key，任何key。 # volatile-ttl -> Remove the key with the nearest expire time (minor TTL) 删除具有最近过期时间的密钥(次要TTL) # noeviction -> Don't evict anything, just return an error on write operations. 不删除任何内容，只在写操作时返回一个错误。 # # LRU means Least Recently Used # LRU的意思是最近最少使用的 # LFU means Least Frequently Used # LFU的意思是最不常用的 # # Both LRU, LFU and volatile-ttl are implemented using approximated # randomized algorithms. # LRU、LFU和volatile-ttl都是使用近似随机算法实现的。 # # Note: with any of the above policies, Redis will return an error on write # operations, when there are no suitable keys for eviction. # # At the date of writing these commands are: set setnx setex append # incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd # sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby # zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby # getset mset msetnx exec sort # # The default is: # # maxmemory-policy noeviction # LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated # algorithms (in order to save memory), so you can tune it for speed or # accuracy. For default Redis will check five keys and pick the one that was # used less recently, you can change the sample size using the following # configuration directive. # LRU、LFU和最小TTL算法并不是精确的算法，而是近似的算法(为了节省内存)，因此您可以对其进行调优以获得速度或精度。 # 对于默认情况，Redis将检查五个键并选择最近较少使用的键，您可以使用以下配置指令更改样本大小。 # # The default of 5 produces good enough results. 10 Approximates very closely # true LRU but costs more CPU. 3 is faster but not very accurate. # 默认的5产生足够好的结果。10非常接近真实的LRU，但是消耗更多的CPU。3更快，但不是很准确。 # maxmemory-samples 5 # Starting from Redis 5, by default a replica will ignore its maxmemory setting # (unless it is promoted to master after a failover or manually). It means # that the eviction of keys will be just handled by the master, sending the # DEL commands to the replica as keys evict in the master side. # 从Redis 5开始，在默认情况下，副本将忽略它的maxmemory设置(除非在故障转移后或手动将其提升为master)。 # 这意味着键的回收将只由主进程处理，当主进程中的键被回收时，将DEL命令发送到副本。 # This behavior ensures that masters and replicas stay consistent, and is usually # what you want, however if your replica is writable, or you want the replica # to have a different memory setting, and you are sure all the writes performed # to the replica are idempotent, then you may change this default (but be sure # to understand what you are doing). # # Note that since the replica by default does not evict, it may end using more # memory than the one set via maxmemory (there are certain buffers that may # be larger on the replica, or data structures may sometimes take more memory # and so forth). So make sure you monitor your replicas and make sure they # have enough memory to never hit a real out-of-memory condition before the # master hits the configured maxmemory setting. # 注意，由于副本在默认情况下不会被逐出，因此它最终使用的内存可能比通过maxmemory设置的内存多(在副本上有一些缓冲区可能更大，或者数据结构有时会占用更多内存，等等)。 # 因此，请确保您监控您的副本，并确保他们有足够的内存，从来没有遇到真正的内存不足的情况之前master点击已配置的maxmemory设置。 # replica-ignore-maxmemory yes # Redis reclaims expired keys in two ways: upon access when those keys are # found to be expired, and also in background, in what is called the # \"active expire key\". The key space is slowly and interactively scanned # looking for expired keys to reclaim, so that it is possible to free memory # of keys that are expired and will never be accessed again in a short time. # Redis以两种方式回收过期的密钥:在访问时发现这些密钥已经过期，在后台，也称为“活动过期密钥”。 # 对密钥空间进行缓慢而交互式的扫描，寻找过期的密钥进行回收，以便释放过期的密钥的内存，这些密钥在短时间内永远不会被再次访问。 # The default effort of the expire cycle will try to avoid having more than # ten percent of expired keys still in memory, and will try to avoid consuming # more than 25% of total memory and to add latency to the system. However # it is possible to increase the expire \"effort\" that is normally set to # \"1\", to a greater value, up to the value \"10\". At its maximum value the # system will use more CPU, longer cycles (and technically may introduce # more latency), and will tollerate less already expired keys still present # in the system. It's a tradeoff betweeen memory, CPU and latecy. # 过期周期的默认工作将尝试避免在内存中保留超过10%的过期密钥，并尝试避免消耗超过25%的总内存并增加系统延迟。但是，可以将过期的“工作”(通常设置为“1”)增加到更大的值，直到值“10”。 # 在其最大值时，系统将使用更多的CPU，更长的周期(技术上可能引入更多的延迟)，并且将减少系统中仍然存在的过期密钥。这是内存、CPU和延迟之间的权衡。 # active-expire-effort 1 ############################# LAZY FREEING #################################### # 延迟加载 # Redis has two primitives to delete keys. One is called DEL and is a blocking # deletion of the object. It means that the server stops processing new commands # in order to reclaim all the memory associated with an object in a synchronous # way. If the key deleted is associated with a small object, the time needed # in order to execute the DEL command is very small and comparable to most other # O(1) or O(log_N) commands in Redis. However if the key is associated with an # aggregated value containing millions of elements, the server can block for # a long time (even seconds) in order to complete the operation. # Redis有两个原语来删除键。一个是DEL，是对象的阻塞删除。它意味着服务器停止处理新命令，以便以同步方式回收与对象关联的所有内存。 # 如果删除的键与一个小对象相关联，那么执行DEL命令所需的时间非常短，可以与Redis中的大多数其他O(1)或O(log_N)命令相媲美。 # 但是，如果键与包含数百万个元素的聚合值相关联，服务器可能会阻塞很长时间(甚至几秒钟)以完成操作。 # For the above reasons Redis also offers non blocking deletion primitives # such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and # FLUSHDB commands, in order to reclaim memory in background. Those commands # are executed in constant time. Another thread will incrementally free the # object in the background as fast as possible. # # DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled. # It's up to the design of the application to understand when it is a good # idea to use one or the other. However the Redis server sometimes has to # delete keys or flush the whole database as a side effect of other operations. # Specifically Redis deletes objects independently of a user call in the # following scenarios: # # 1) On eviction, because of the maxmemory and maxmemory policy configurations, # in order to make room for new data, without going over the specified # memory limit. # 2) Because of expire: when a key with an associated time to live (see the # EXPIRE command) must be deleted from memory. # 3) Because of a side effect of a command that stores data on a key that may # already exist. For example the RENAME command may delete the old key # content when it is replaced with another one. Similarly SUNIONSTORE # or SORT with STORE option may delete existing keys. The SET command # itself removes any old content of the specified key in order to replace # it with the specified string. # 因为将数据存储在可能已经存在的键上的命令的副作用。例如，重命名命令可以删除替换为其他键内容的旧键内容。 # 类似地，SUNIONSTORE或使用STORE选项排序可以删除现有密钥。SET命令本身删除指定键的任何旧内容，以便用指定的字符串替换它。 # 4) During replication, when a replica performs a full resynchronization with # its master, the content of the whole database is removed in order to # load the RDB file just transferred. # 在复制期间，当一个副本执行与其主副本的完全重新同步时，将删除整个数据库的内容，以便加载刚刚传输的RDB文件。 # # In all the above cases the default is to delete objects in a blocking way, # like if DEL was called. However you can configure each case specifically # in order to instead release memory in a non-blocking way like if UNLINK # was called, using the following configuration directives. # 在上述所有情况下，默认情况是以阻塞的方式删除对象，就像调用DEL一样。 # 但是，您可以专门配置每种情况，以便以非阻塞的方式释放内存，就像调用UNLINK一样，使用以下配置指令。 lazyfree-lazy-eviction no # 是否开启基于lazyfree的驱逐功能 yes，表示开启。no，默认值，表示不开启。 lazyfree-lazy-expire no # 是否开启基于lazyfree的过期key删除功能， lazyfree-lazy-server-del no # RENAME、SUNIONSTORE等命令是否基于lazyfree异步删除数据 replica-lazy-flush no # It is also possible, for the case when to replace the user code DEL calls # with UNLINK calls is not easy, to modify the default behavior of the DEL # command to act exactly like UNLINK, using the following configuration # 在用UNLINK调用替换DEL调用的用户代码不容易的情况下，也可以使用以下配置修改DEL命令的默认行为，使其行为与UNLINK完全一样 # directive: # 执行DEL命令时是否基于lazyfree异步删除数据 lazyfree-lazy-user-del no ################################ THREADED I/O ################################# # 多线程 # Redis is mostly single threaded, however there are certain threaded # operations such as UNLINK, slow I/O accesses and other things that are # performed on side threads. # Redis主要是单线程的，但是也有一些特定的线程操作，比如断开链接、缓慢的I/O访问和其他在侧线程上执行的操作。 # Now it is also possible to handle Redis clients socket reads and writes # in different I/O threads. Since especially writing is so slow, normally # Redis users use pipelining in order to speedup the Redis performances per # core, and spawn multiple instances in order to scale more. Using I/O # threads it is possible to easily speedup two times Redis without resorting # to pipelining nor sharding of the instance. # # By default threading is disabled, we suggest enabling it only in machines # that have at least 4 or more cores, leaving at least one spare core. # Using more than 8 threads is unlikely to help much. We also recommend using # threaded I/O only if you actually have performance problems, with Redis # instances being able to use a quite big percentage of CPU time, otherwise # there is no point in using this feature. # 默认情况下，线程是禁用的，我们建议只在拥有至少4个或更多内核的机器上启用线程，而保留至少一个备用内核。使用8个以上的线程不太可能有太大的帮助。 # 我们还建议仅当您确实存在性能问题时才使用线程I/O，因为Redis实例能够使用相当大的CPU时间百分比，否则使用此特性是没有意义的。 # So for instance if you have a four cores boxes, try to use 2 or 3 I/O # threads, if you have a 8 cores, try to use 6 threads. In order to # enable I/O threads use the following configuration directive: # 例如，如果你有4个内核，尝试使用2或3个I/O线程，如果你有8个内核，尝试使用6个线程。为了启用I/O线程使用以下配置指令: # io-threads 4 # # Setting io-threads to 1 will just use the main thread as usually. # When I/O threads are enabled, we only use threads for writes, that is # to thread the write(2) syscall and transfer the client buffers to the # socket. However it is also possible to enable threading of reads and # protocol parsing using the following configuration directive, by setting # it to yes: # 将io线程设置为1只会像往常一样使用主线程。当启用I/O线程时，我们只使用线程进行写操作， # 即线程写系统调用并将客户端缓冲区传输到套接字。然而，也可以启用线程读取和协议解析使用以下配置指令，通过设置为yes: # # io-threads-do-reads no # # Usually threading reads doesn't help much. # 通常线程读取不会有太大帮助。哈哈，也就是百分之八十的人用不到这个玩意儿。 # NOTE 1: This configuration directive cannot be changed at runtime via # CONFIG SET. Aso this feature currently does not work when SSL is # enabled. # 这个配置指令不能在运行时通过配置集进行更改。当启用SSL时，此功能当前无法工作。 # NOTE 2: If you want to test the Redis speedup using redis-benchmark, make # sure you also run the benchmark itself in threaded mode, using the # --threads option to match the number of Redis theads, otherwise you'll not # be able to notice the improvements. # 如果您想使用Redis -benchmark测试Redis加速，请确保您也在线程模式下运行基准测试本身，使用——threads选项来匹配Redis头的数量，否则您将无法注意到这些改进。 ############################## APPEND ONLY MODE ############################### # 追加模式 # By default Redis asynchronously dumps the dataset on disk. This mode is # good enough in many applications, but an issue with the Redis process or # a power outage may result into a few minutes of writes lost (depending on # the configured save points). # 默认情况下，Redis异步地将数据转储到磁盘上。这种模式在许多应用程序中已经足够好了，但是Redis进程或断电可能会导致几分钟的写丢失(取决于配置的保存点)。 # The Append Only File is an alternative persistence mode that provides # much better durability. For instance using the default data fsync policy # (see later in the config file) Redis can lose just one second of writes in a # dramatic event like a server power outage, or a single write if something # wrong with the Redis process itself happens, but the operating system is # still running correctly. # Append Only文件是另一种持久性模式，它提供了更好的持久性。 # 例如使用默认数据fsync策略配置文件中(见后)复述,可以失去只是一秒的写在一个戏剧性的事件像一个服务器断电,或一个写如果复述过程本身出了问题,但正确操作系统仍在运行。 # # AOF and RDB persistence can be enabled at the same time without problems. # If the AOF is enabled on startup Redis will load the AOF, that is the file # with the better durability guarantees. # 可以同时启用AOF和RDB持久性，不会出现问题。如果启动时启用了AOF，则Redis将加载AOF，这是具有更好持久性保证的文件。 # Please check http://redis.io/topics/persistence for more information. # 是否开启aof持久化。默认不开启 appendonly no # The name of the append only file (default: \"appendonly.aof\") # 仅追加文件的名称(默认:“appendonly.aof”) appendfilename \"appendonly.aof\" # The fsync() call tells the Operating System to actually write data on disk # instead of waiting for more data in the output buffer. Some OS will really flush # data on disk, some other OS will just try to do it ASAP. # fsync()调用告诉操作系统实际在磁盘上写入数据，而不是等待输出缓冲区中的更多数据。一些操作系统会真正地刷新磁盘上的数据，而另一些操作系统只是试图尽快完成。 # Redis supports three different modes: # Redis支持三种不同的模式: # no: don't fsync, just let the OS flush the data when it wants. Faster. # 从不fsync，只需将数据交给操作系统即可。更快，更不安全的方法。通常，Linux使用此配置每30秒刷新一次数据，但这取决于内核的精确调整。 # always: fsync after every write to the append only log. Slow, Safest. # fsync每次将新命令附加到AOF时。非常非常慢，非常安全。 # everysec: fsync only one time every second. Compromise. # fsync每秒。速度足够快（在2.4中可能与快照速度一样快），如果发生灾难，您可能会丢失1秒的数据。 # # The default is \"everysec\", as that's usually the right compromise between # speed and data safety. It's up to you to understand if you can relax this to # \"no\" that will let the operating system flush the output buffer when # it wants, for better performances (but if you can live with the idea of # some data loss consider the default persistence mode that's snapshotting), # or on the contrary, use \"always\" that's very slow but a bit safer than # everysec. # 默认值是“everysec”，因为这通常是速度和数据安全性之间的正确折衷。由你理解如果你能放松这个“不”字,让操作系统刷新输出缓冲区时, # 为了更好的表现(但是如果你可以忍受一些数据丢失的想法考虑默认快照的持久性模式),或相反,使用“总是”非常缓慢但比everysec更安全一点。 # More details please check the following article: # http://antirez.com/post/redis-persistence-demystified.html # # If unsure, use \"everysec\". # 如果不确定，使用“everysec”。 # appendfsync always appendfsync everysec # appendfsync no # When the AOF fsync policy is set to always or everysec, and a background # saving process (a background save or AOF log background rewriting) is # performing a lot of I/O against the disk, in some Linux configurations # Redis may block too long on the fsync() call. Note that there is no fix for # this currently, as even performing fsync in a different thread will block # our synchronous write(2) call. # # In order to mitigate this problem it's possible to use the following option # that will prevent fsync() from being called in the main process while a # BGSAVE or BGREWRITEAOF is in progress. # # This means that while another child is saving, the durability of Redis is # the same as \"appendfsync none\". In practical terms, this means that it is # possible to lose up to 30 seconds of log in the worst scenario (with the # default Linux settings). # # If you have latency problems turn this to \"yes\". Otherwise leave it as # \"no\" that is the safest pick from the point of view of durability. # 如果你有延迟问题，将此选项变为“yes”。否则，从耐久性的角度来看，“no”是最安全的选择。 no-appendfsync-on-rewrite no # Automatic rewrite of the append only file. # 自动重写仅追加文件。 # Redis is able to automatically rewrite the log file implicitly calling # BGREWRITEAOF when the AOF log size grows by the specified percentage. # Redis能够在日志大小按指定百分比增长时自动重写隐式调用BGREWRITEAOF的日志文件。 # This is how it works: Redis remembers the size of the AOF file after the # latest rewrite (if no rewrite has happened since the restart, the size of # the AOF at startup is used). # 它是这样工作的:Redis记住了最近一次重写后的AOF文件的大小(如果重新启动后没有发生重写，则使用启动时的AOF大小)。 # # This base size is compared to the current size. If the current size is # bigger than the specified percentage, the rewrite is triggered. Also # you need to specify a minimal size for the AOF file to be rewritten, this # is useful to avoid rewriting the AOF file even if the percentage increase # is reached but it is still pretty small. # 这个基本大小与当前大小进行比较。如果当前大小大于指定的百分比，则会触发重写。 # 此外，您还需要为要重写的AOF文件指定最小的大小，这对于避免重写AOF文件非常有用，即使百分比会增加到达，但它仍然是相当小的。 # Specify a percentage of zero in order to disable the automatic AOF # rewrite feature. # 指定零的百分比，以禁用自动AOF重写特性。 # 当文件超过上次rewrite的百分之百的时候就会重写。 # 对于下面的我是这样认为的： # 当前AOF文件大小超过上一次重写的AOF文件大小的百分之多少才会重写 # 即为：当文件超过64mb开启重写，如果超过64的百分之百，也就是超过64*2=128的时候再次重写。 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # An AOF file may be found to be truncated at the end during the Redis # startup process, when the AOF data gets loaded back into memory. # This may happen when the system where Redis is running # crashes, especially when an ext4 filesystem is mounted without the # data=ordered option (however this can't happen when Redis itself # crashes or aborts but the operating system still works correctly). # # Redis can either exit with an error when this happens, or load as much # data as possible (the default now) and start if the AOF file is found # to be truncated at the end. The following option controls this behavior. # # If aof-load-truncated is set to yes, a truncated AOF file is loaded and # the Redis server starts emitting a log to inform the user of the event. # Otherwise if the option is set to no, the server aborts with an error # and refuses to start. When the option is set to no, the user requires # to fix the AOF file using the \"redis-check-aof\" utility before to restart # the server. # # Note that if the AOF file will be found to be corrupted in the middle # the server will still exit with an error. This option only applies when # Redis will try to read more data from the AOF file but not enough bytes # will be found. # 注意，如果AOF文件在中间被破坏，服务器仍然会带着错误退出。这个选项只适用于当Redis试图从AOF文件读取更多的数据，但没有足够的字节将被发现。 # redis在启动时可以加载被截断的AOF文件 aof-load-truncated yes # When rewriting the AOF file, Redis is able to use an RDB preamble in the # AOF file for faster rewrites and recoveries. When this option is turned # on the rewritten AOF file is composed of two different stanzas: # 当重写AOF文件时，Redis能够在AOF文件中使用一个RDB序言，以便更快地重写和恢复。当这个选项打开时，重写的AOF文件由两个不同的节组成: # [RDB file][AOF tail] # # When loading Redis recognizes that the AOF file starts with the \"REDIS\" # string and loads the prefixed RDB file, and continues loading the AOF # tail. # 当加载Redis时，它识别出AOF文件以“Redis”字符串开始并加载前缀RDB文件，然后继续加载AOF尾部。 aof-use-rdb-preamble yes ################################ LUA SCRIPTING ############################### # LUA脚本 # Max execution time of a Lua script in milliseconds. # Lua脚本的最大执行时间(以毫秒为单位) # If the maximum execution time is reached Redis will log that a script is # still in execution after the maximum allowed time and will start to # reply to queries with an error. # 如果达到最大执行时间，Redis将记录脚本在最大允许时间之后仍在执行，并开始用错误回复查询。 # When a long running script exceeds the maximum execution time only the # SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be # used to stop a script that did not yet called write commands. The second # is the only way to shut down the server in the case a write command was # already issued by the script but the user doesn't want to wait for the natural # termination of the script. # # Set it to 0 or a negative value for unlimited execution without warnings. # 将其设置为0或负值，以便在没有警告的情况下无限执行。 lua-time-limit 5000 ################################ REDIS CLUSTER ############################### # 分布式集群配置 # Normal Redis instances can't be part of a Redis Cluster; only nodes that are # started as cluster nodes can. In order to start a Redis instance as a # cluster node enable the cluster support uncommenting the following: # 正常的Redis实例不能成为Redis集群的一部分;只有作为集群节点启动的节点可以。为了启动一个Redis实例作为一个集群节点启用集群支持取消注释如下: # cluster-enabled yes # Every cluster node has a cluster configuration file. This file is not # intended to be edited by hand. It is created and updated by Redis nodes. # Every Redis Cluster node requires a different cluster configuration file. # Make sure that instances running in the same system do not have # overlapping cluster configuration file names. # 每个集群节点都有一个集群配置文件。此文件不打算手工编辑。它由Redis节点创建和更新。 # 每个Redis集群节点都需要一个不同的集群配置文件。确保在同一系统中运行的实例没有重叠的集群配置文件名。 # cluster-config-file nodes-6379.conf # Cluster node timeout is the amount of milliseconds a node must be unreachable # for it to be considered in failure state. # 群集节点超时是节点必须不可达的毫秒数，以便将其视为故障状态。 # Most other internal time limits are multiple of the node timeout. # 大多数其他内部时间限制是多个节点超时。 # cluster-node-timeout 15000 # A replica of a failing master will avoid to start a failover if its data # looks too old. # 如果数据看起来太旧，失败主服务器的副本将避免启动故障转移。 # There is no simple way for a replica to actually have an exact measure of # its \"data age\", so the following two checks are performed: # # 1) If there are multiple replicas able to failover, they exchange messages # in order to try to give an advantage to the replica with the best # replication offset (more data from the master processed). # Replicas will try to get their rank by offset, and apply to the start # of the failover a delay proportional to their rank. # # 2) Every single replica computes the time of the last interaction with # its master. This can be the last ping or command received (if the master # is still in the \"connected\" state), or the time that elapsed since the # disconnection with the master (if the replication link is currently down). # If the last interaction is too old, the replica will not try to failover # at all. # # The point \"2\" can be tuned by user. Specifically a replica will not perform # the failover if, since the last interaction with the master, the time # elapsed is greater than: # # (node-timeout * replica-validity-factor) + repl-ping-replica-period # # So for example if node-timeout is 30 seconds, and the replica-validity-factor # is 10, and assuming a default repl-ping-replica-period of 10 seconds, the # replica will not try to failover if it was not able to talk with the master # for longer than 310 seconds. # # A large replica-validity-factor may allow replicas with too old data to failover # a master, while a too small value may prevent the cluster from being able to # elect a replica at all. # # For maximum availability, it is possible to set the replica-validity-factor # to a value of 0, which means, that replicas will always try to failover the # master regardless of the last time they interacted with the master. # (However they'll always try to apply a delay proportional to their # offset rank). # # Zero is the only value able to guarantee that when all the partitions heal # the cluster will always be able to continue. # # cluster-replica-validity-factor 10 # Cluster replicas are able to migrate to orphaned masters, that are masters # that are left without working replicas. This improves the cluster ability # to resist to failures as otherwise an orphaned master can't be failed over # in case of failure if it has no working replicas. # # Replicas migrate to orphaned masters only if there are still at least a # given number of other working replicas for their old master. This number # is the \"migration barrier\". A migration barrier of 1 means that a replica # will migrate only if there is at least 1 other working replica for its master # and so forth. It usually reflects the number of replicas you want for every # master in your cluster. # # Default is 1 (replicas migrate only if their masters remain with at least # one replica). To disable migration just set it to a very large value. # A value of 0 can be set but is useful only for debugging and dangerous # in production. # # cluster-migration-barrier 1 # By default Redis Cluster nodes stop accepting queries if they detect there # is at least an hash slot uncovered (no available node is serving it). # This way if the cluster is partially down (for example a range of hash slots # are no longer covered) all the cluster becomes, eventually, unavailable. # It automatically returns available as soon as all the slots are covered again. # # However sometimes you want the subset of the cluster which is working, # to continue to accept queries for the part of the key space that is still # covered. In order to do so, just set the cluster-require-full-coverage # option to no. # # cluster-require-full-coverage yes # This option, when set to yes, prevents replicas from trying to failover its # master during master failures. However the master can still perform a # manual failover, if forced to do so. # # This is useful in different scenarios, especially in the case of multiple # data center operations, where we want one side to never be promoted if not # in the case of a total DC failure. # # cluster-replica-no-failover no # This option, when set to yes, allows nodes to serve read traffic while the # the cluster is in a down state, as long as it believes it owns the slots. # # This is useful for two cases. The first case is for when an application # doesn't require consistency of data during node failures or network partitions. # One example of this is a cache, where as long as the node has the data it # should be able to serve it. # # The second use case is for configurations that don't meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the # entire cluster without this option set, with it set there is only a write outage. # Without a quorum of masters, slot ownership will not change automatically. # # cluster-allow-reads-when-down no # In order to setup your cluster make sure to read the documentation # available at http://redis.io web site. ########################## CLUSTER DOCKER/NAT support ######################## # In certain deployments, Redis Cluster nodes address discovery fails, because # addresses are NAT-ted or because ports are forwarded (the typical case is # Docker and other containers). # # In order to make Redis Cluster working in such environments, a static # configuration where each node knows its public address is needed. The # following two options are used for this scope, and are: # # * cluster-announce-ip # * cluster-announce-port # * cluster-announce-bus-port # # Each instruct the node about its address, client port, and cluster message # bus port. The information is then published in the header of the bus packets # so that other nodes will be able to correctly map the address of the node # publishing the information. # # If the above options are not used, the normal Redis Cluster auto-detection # will be used instead. # # Note that when remapped, the bus port may not be at the fixed offset of # clients port + 10000, so you can specify any port and bus-port depending # on how they get remapped. If the bus-port is not set, a fixed offset of # 10000 will be used as usually. # # Example: # # cluster-announce-ip 10.1.1.5 # cluster-announce-port 6379 # cluster-announce-bus-port 6380 ################################## SLOW LOG ################################### # The Redis Slow Log is a system to log queries that exceeded a specified # execution time. The execution time does not include the I/O operations # like talking with the client, sending the reply and so forth, # but just the time needed to actually execute the command (this is the only # stage of command execution where the thread is blocked and can not serve # other requests in the meantime). # Redis慢日志是一个记录超过指定执行时间的查询的系统。执行时间不包括I / O操作,比如与客户端,发送应答等等, # 但就实际执行命令所需的时间(这是唯一阶段命令执行的线程被阻塞,不能同时处理其他请求)。 # You can configure the slow log with two parameters: one tells Redis # what is the execution time, in microseconds, to exceed in order for the # command to get logged, and the other parameter is the length of the # slow log. When a new command is logged the oldest one is removed from the # queue of logged commands. # 您可以使用两个参数配置慢日志:一个参数告诉Redis命令的执行时间(以微秒为单位)超过了多少， # 以便记录命令，另一个参数是慢日志的长度。当记录一个新命令时，将从记录的命令队列中删除最旧的命令。 # The following time is expressed in microseconds, so 1000000 is equivalent # to one second. Note that a negative number disables the slow log, while # a value of zero forces the logging of every command. # 下面的时间用微秒表示，所以1000000等于1秒。注意，负数会禁用慢日志，而值0则强制对每个命令进行日志记录。 slowlog-log-slower-than 10000 # There is no limit to this length. Just be aware that it will consume memory. # You can reclaim memory used by the slow log with SLOWLOG RESET. # 这个长度没有限制。请注意，它会消耗内存。您可以通过重新设置慢日志来回收慢日志使用的内存。 slowlog-max-len 128 ################################ LATENCY MONITOR ############################## # The Redis latency monitoring subsystem samples different operations # at runtime in order to collect data related to possible sources of # latency of a Redis instance. # Redis延迟监视子系统在运行时采样不同的操作，以便收集与Redis实例的潜在延迟源相关的数据。 # Via the LATENCY command this information is available to the user that can # print graphs and obtain reports. # 通过LATENCY命令，可以将此信息提供给能够打印图形和获取报告的用户。 # The system only logs operations that were performed in a time equal or # greater than the amount of milliseconds specified via the # latency-monitor-threshold configuration directive. When its value is set # to zero, the latency monitor is turned off. # 系统只记录在等于或大于通过延迟监视阈值配置指令指定的毫秒数的时间内执行的操作。当其值设置为零时，延迟监控器将被关闭。 # By default latency monitoring is disabled since it is mostly not needed # if you don't have latency issues, and collecting data has a performance # impact, that while very small, can be measured under big load. Latency # monitoring can easily be enabled at runtime using the command # \"CONFIG SET latency-monitor-threshold &lt;milliseconds>\" if needed. # 默认情况下，延迟监视是禁用的，因为如果没有延迟问题，那么基本上不需要延迟监视，而且收集数据对性能有影响，虽然非常小，但是可以在大负载下测量。 # 如果需要，可以在运行时使用“latency-monitor-threshold &lt;milliseconds>”命令轻松启用延迟监视 # 延时监控的采样时间阈值（最小值）。单位毫秒 latency-monitor-threshold 0 ############################# EVENT NOTIFICATION ############################## # 事件通知 # Redis can notify Pub/Sub clients about events happening in the key space. # This feature is documented at http://redis.io/topics/notifications # # For instance if keyspace events notification is enabled, and a client # performs a DEL operation on key \"foo\" stored in the Database 0, two # messages will be published via Pub/Sub: # # PUBLISH __keyspace@0__:foo del # PUBLISH __keyevent@0__:del foo # # It is possible to select the events that Redis will notify among a set # of classes. Every class is identified by a single character: # # K Keyspace events, published with __keyspace@&lt;db>__ prefix. # E Keyevent events, published with __keyevent@&lt;db>__ prefix. # g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ... # $ String commands # l List commands # s Set commands # h Hash commands # z Sorted set commands # x Expired events (events generated every time a key expires) # e Evicted events (events generated when a key is evicted for maxmemory) # t Stream commands # m Key-miss events (Note: It is not included in the 'A' class) # A Alias for g$lshzxet, so that the \"AKE\" string means all the events # (Except key-miss events which are excluded from 'A' due to their # unique nature). # # The \"notify-keyspace-events\" takes as argument a string that is composed # of zero or multiple characters. The empty string means that notifications # are disabled. # # Example: to enable list and generic events, from the point of view of the # event name, use: # # notify-keyspace-events Elg # # Example 2: to get the stream of the expired keys subscribing to channel # name __keyevent@0__:expired use: # # notify-keyspace-events Ex # # By default all notifications are disabled because most users don't need # this feature and the feature has some overhead. Note that if you don't # specify at least one of K or E, no events will be delivered. # 键空间通知，配置该参数后客户端可以通过Redis的订阅与发布功能，来接收那些以某种方式改动了Redis数据集的事件。 notify-keyspace-events \"\" ############################### GOPHER SERVER ################################# # Redis contains an implementation of the Gopher protocol, as specified in # the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt). # # The Gopher protocol was very popular in the late '90s. It is an alternative # to the web, and the implementation both server and client side is so simple # that the Redis server has just 100 lines of code in order to implement this # support. # # What do you do with Gopher nowadays? Well Gopher never *really* died, and # lately there is a movement in order for the Gopher more hierarchical content # composed of just plain text documents to be resurrected. Some want a simpler # internet, others believe that the mainstream internet became too much # controlled, and it's cool to create an alternative space for people that # want a bit of fresh air. # # Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol # as a gift. # # --- HOW IT WORKS? --- # # The Redis Gopher support uses the inline protocol of Redis, and specifically # two kind of inline requests that were anyway illegal: an empty request # or any request that starts with \"/\" (there are no Redis commands starting # with such a slash). Normal RESP2/RESP3 requests are completely out of the # path of the Gopher protocol implementation and are served as usually as well. # # If you open a connection to Redis when Gopher is enabled and send it # a string like \"/foo\", if there is a key named \"/foo\" it is served via the # Gopher protocol. # # In order to create a real Gopher \"hole\" (the name of a Gopher site in Gopher # talking), you likely need a script like the following: # # https://github.com/antirez/gopher2redis # # --- SECURITY WARNING --- # # If you plan to put Redis on the internet in a publicly accessible address # to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance. # Once a password is set: # # 1. The Gopher server (when enabled, not by default) will still serve # content via Gopher. # 2. However other commands cannot be called before the client will # authenticate. # # So use the 'requirepass' option to protect your instance. # # To enable Gopher support uncomment the following line and set # the option from no (the default) to yes. # # gopher-enabled no ############################### ADVANCED CONFIG ############################### # 高级配置 # Hashes are encoded using a memory efficient data structure when they have a # small number of entries, and the biggest entry does not exceed a given # threshold. These thresholds can be configured using the following directives. # 当有少量条目且最大条目不超过给定阈值时，使用内存有效数据结构对哈希进行编码。可以使用以下指令配置这些阈值。 hash-max-ziplist-entries 512 hash-max-ziplist-value 64 # Lists are also encoded in a special way to save a lot of space. # The number of entries allowed per internal list node can be specified # as a fixed maximum size or a maximum number of elements. # For a fixed maximum size, use -5 through -1, meaning: # -5: max size: 64 Kb &lt;-- not recommended for normal workloads # -4: max size: 32 Kb &lt;-- not recommended # -3: max size: 16 Kb &lt;-- probably not recommended # -2: max size: 8 Kb &lt;-- good # -1: max size: 4 Kb &lt;-- good # Positive numbers mean store up to _exactly_ that number of elements # per list node. # The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size), # but if your use case is unique, adjust the settings as necessary. list-max-ziplist-size -2 # Lists may also be compressed. # Compress depth is the number of quicklist ziplist nodes from *each* side of # the list to *exclude* from compression. The head and tail of the list # are always uncompressed for fast push/pop operations. Settings are: # 0: disable all list compression # 1: depth 1 means \"don't start compressing until after 1 node into the list, # going from either the head or tail\" # So: [head]->node->node->...->node->[tail] # [head], [tail] will always be uncompressed; inner nodes will compress. # 2: [head]->[next]->node->node->...->node->[prev]->[tail] # 2 here means: don't compress head or head->next or tail->prev or tail, # but compress all nodes between them. # 3: [head]->[next]->[next]->node->node->...->node->[prev]->[prev]->[tail] # etc. list-compress-depth 0 # Sets have a special encoding in just one case: when a set is composed # of just strings that happen to be integers in radix 10 in the range # of 64 bit signed integers. # The following configuration setting sets the limit in the size of the # set in order to use this special memory saving encoding. set-max-intset-entries 512 # Similarly to hashes and lists, sorted sets are also specially encoded in # order to save a lot of space. This encoding is only used when the length and # elements of a sorted set are below the following limits: zset-max-ziplist-entries 128 zset-max-ziplist-value 64 # HyperLogLog sparse representation bytes limit. The limit includes the # 16 bytes header. When an HyperLogLog using the sparse representation crosses # this limit, it is converted into the dense representation. # # A value greater than 16000 is totally useless, since at that point the # dense representation is more memory efficient. # # The suggested value is ~ 3000 in order to have the benefits of # the space efficient encoding without slowing down too much PFADD, # which is O(N) with the sparse encoding. The value can be raised to # ~ 10000 when CPU is not a concern, but space is, and the data set is # composed of many HyperLogLogs with cardinality in the 0 - 15000 range. hll-sparse-max-bytes 3000 # Streams macro node max size / items. The stream data structure is a radix # tree of big nodes that encode multiple items inside. Using this configuration # it is possible to configure how big a single node can be in bytes, and the # maximum number of items it may contain before switching to a new node when # appending new stream entries. If any of the following settings are set to # zero, the limit is ignored, so for instance it is possible to set just a # max entires limit by setting max-bytes to 0 and max-entries to the desired # value. stream-node-max-bytes 4096 stream-node-max-entries 100 # Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in # order to help rehashing the main Redis hash table (the one mapping top-level # keys to values). The hash table implementation Redis uses (see dict.c) # performs a lazy rehashing: the more operation you run into a hash table # that is rehashing, the more rehashing \"steps\" are performed, so if the # server is idle the rehashing is never complete and some more memory is used # by the hash table. # # The default is to use this millisecond 10 times every second in order to # actively rehash the main dictionaries, freeing memory when possible. # # If unsure: # use \"activerehashing no\" if you have hard latency requirements and it is # not a good thing in your environment that Redis can reply from time to time # to queries with 2 milliseconds delay. # 当你的使用场景中，有非常严格的实时性需要，不能够接受Redis时不时的对请求有2毫秒的延迟的话，把这项配置为no # use \"activerehashing yes\" if you don't have such hard requirements but # want to free memory asap when possible. # 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存 activerehashing yes # The client output buffer limits can be used to force disconnection of clients # that are not reading data from the server fast enough for some reason (a # common reason is that a Pub/Sub client can't consume messages as fast as the # publisher can produce them). # # The limit can be set differently for the three different classes of clients: # # normal -> normal clients including MONITOR clients # replica -> replica clients # pubsub -> clients subscribed to at least one pubsub channel or pattern # # The syntax of every client-output-buffer-limit directive is the following: # # client-output-buffer-limit &lt;class> &lt;hard limit> &lt;soft limit> &lt;soft seconds> # # A client is immediately disconnected once the hard limit is reached, or if # the soft limit is reached and remains reached for the specified number of # seconds (continuously). # So for instance if the hard limit is 32 megabytes and the soft limit is # 16 megabytes / 10 seconds, the client will get disconnected immediately # if the size of the output buffers reach 32 megabytes, but will also get # disconnected if the client reaches 16 megabytes and continuously overcomes # the limit for 10 seconds. # # By default normal clients are not limited because they don't receive data # without asking (in a push way), but just after a request, so only # asynchronous clients may create a scenario where data is requested faster # than it can read. # # Instead there is a default limit for pubsub and replica clients, since # subscribers and replicas receive data in a push fashion. # # Both the hard or the soft limit can be disabled by setting them to zero. client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 # Client query buffers accumulate new commands. They are limited to a fixed # amount by default in order to avoid that a protocol desynchronization (for # instance due to a bug in the client) will lead to unbound memory usage in # the query buffer. However you can configure it here if you have very special # needs, such us huge multi/exec requests or alike. # # client-query-buffer-limit 1gb # In the Redis protocol, bulk requests, that are, elements representing single # strings, are normally limited ot 512 mb. However you can change this limit # here. # # proto-max-bulk-len 512mb # Redis calls an internal function to perform many background tasks, like # closing connections of clients in timeout, purging expired keys that are # never requested, and so forth. # # Not all tasks are performed with the same frequency, but Redis checks for # tasks to perform according to the specified \"hz\" value. # # By default \"hz\" is set to 10. Raising the value will use more CPU when # Redis is idle, but at the same time will make Redis more responsive when # there are many keys expiring at the same time, and timeouts may be # handled with more precision. # # The range is between 1 and 500, however a value over 100 is usually not # a good idea. Most users should use the default of 10 and raise this up to # 100 only in environments where very low latency is required. hz 10 # Normally it is useful to have an HZ value which is proportional to the # number of clients connected. This is useful in order, for instance, to # avoid too many clients are processed for each background task invocation # in order to avoid latency spikes. # # Since the default HZ value by default is conservatively set to 10, Redis # offers, and enables by default, the ability to use an adaptive HZ value # which will temporary raise when there are many connected clients. # # When dynamic HZ is enabled, the actual configured HZ will be used # as a baseline, but multiples of the configured HZ value will be actually # used as needed once more clients are connected. In this way an idle # instance will use very little CPU time while a busy instance will be # more responsive. dynamic-hz yes # When a child rewrites the AOF file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. aof-rewrite-incremental-fsync yes # When redis saves RDB file, if the following option is enabled # the file will be fsync-ed every 32 MB of data generated. This is useful # in order to commit the file to the disk more incrementally and avoid # big latency spikes. rdb-save-incremental-fsync yes # Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good # idea to start with the default settings and only change them after investigating # how to improve the performances and how the keys LFU change over time, which # is possible to inspect via the OBJECT FREQ command. # # There are two tunable parameters in the Redis LFU implementation: the # counter logarithm factor and the counter decay time. It is important to # understand what the two parameters mean before changing them. # # The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis # uses a probabilistic increment with logarithmic behavior. Given the value # of the old counter, when a key is accessed, the counter is incremented in # this way: # # 1. A random number R between 0 and 1 is extracted. # 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1). # 3. The counter is incremented only if R &lt; P. # # The default lfu-log-factor is 10. This is a table of how the frequency # counter changes with a different number of accesses with different # logarithmic factors: # # +--------+------------+------------+------------+------------+------------+ # | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits | # +--------+------------+------------+------------+------------+------------+ # | 0 | 104 | 255 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 1 | 18 | 49 | 255 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 10 | 10 | 18 | 142 | 255 | 255 | # +--------+------------+------------+------------+------------+------------+ # | 100 | 8 | 11 | 49 | 143 | 255 | # +--------+------------+------------+------------+------------+------------+ # # NOTE: The above table was obtained by running the following commands: # # redis-benchmark -n 1000000 incr foo # redis-cli object freq foo # # NOTE 2: The counter initial value is 5 in order to give new objects a chance # to accumulate hits. # # The counter decay time is the time, in minutes, that must elapse in order # for the key counter to be divided by two (or decremented if it has a value # less &lt;= 10). # 计数器衰减时间是键计数器除以2(如果值小于&lt;= 10，则衰减)所必须经过的时间(以分钟为单位)。 # The default value for the lfu-decay-time is 1. A Special value of 0 means to # decay the counter every time it happens to be scanned. # lfu-decay-time的默认值是1。一个特殊的值0表示在每次扫描计数器时对其进行衰减。 # lfu-log-factor 10 # lfu-decay-time 1 ########################### ACTIVE DEFRAGMENTATION ####################### # # What is active defragmentation? # ------------------------------- # # Active (online) defragmentation allows a Redis server to compact the # spaces left between small allocations and deallocations of data in memory, # thus allowing to reclaim back memory. # # Fragmentation is a natural process that happens with every allocator (but # less so with Jemalloc, fortunately) and certain workloads. Normally a server # restart is needed in order to lower the fragmentation, or at least to flush # away all the data and create it again. However thanks to this feature # implemented by Oran Agra for Redis 4.0 this process can happen at runtime # in an \"hot\" way, while the server is running. # # Basically when the fragmentation is over a certain level (see the # configuration options below) Redis will start to create new copies of the # values in contiguous memory regions by exploiting certain specific Jemalloc # features (in order to understand if an allocation is causing fragmentation # and to allocate it in a better place), and at the same time, will release the # old copies of the data. This process, repeated incrementally for all the keys # will cause the fragmentation to drop back to normal values. # # Important things to understand: # # 1. This feature is disabled by default, and only works if you compiled Redis # to use the copy of Jemalloc we ship with the source code of Redis. # This is the default with Linux builds. # # 2. You never need to enable this feature if you don't have fragmentation # issues. # # 3. Once you experience fragmentation, you can enable this feature when # needed with the command \"CONFIG SET activedefrag yes\". # # The configuration parameters are able to fine tune the behavior of the # defragmentation process. If you are not sure about what they mean it is # a good idea to leave the defaults untouched. # Enabled active defragmentation # activedefrag no # Minimum amount of fragmentation waste to start active defrag # active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag # active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort # active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage, to be used when the lower # threshold is reached # active-defrag-cycle-min 1 # Maximal effort for defrag in CPU percentage, to be used when the upper # threshold is reached # active-defrag-cycle-max 25 # Maximum number of set/hash/zset/list fields that will be processed from # the main dictionary scan # active-defrag-max-scan-fields 1000 # Jemalloc background thread for purging will be enabled by default jemalloc-bg-thread yes # It is possible to pin different threads and processes of Redis to specific # CPUs in your system, in order to maximize the performances of the server. # This is useful both in order to pin different Redis threads in different # CPUs, but also in order to make sure that multiple Redis instances running # in the same host will be pinned to different CPUs. # # Normally you can do this using the \"taskset\" command, however it is also # possible to this via Redis configuration directly, both in Linux and FreeBSD. # # You can pin the server/IO threads, bio threads, aof rewrite child process, and # the bgsave child process. The syntax to specify the cpu list is the same as # the taskset command: # # Set redis server/io threads to cpu affinity 0,2,4,6: # server_cpulist 0-7:2 # # Set bio threads to cpu affinity 1,3: # bio_cpulist 1,3 # # Set aof rewrite child process to cpu affinity 8,9,10,11: # aof_rewrite_cpulist 8-11 # # Set bgsave child process to cpu affinity 1,10,11 # bgsave_cpulist 1,10-11","categories":[{"name":"Redis","slug":"Redis","permalink":"https://gmaya.top/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://gmaya.top/tags/Redis/"},{"name":"新特性","slug":"新特性","permalink":"https://gmaya.top/tags/%E6%96%B0%E7%89%B9%E6%80%A7/"}]},{"title":"Yapi的安装与使用","slug":"20200518","date":"2020-05-18T09:16:21.000Z","updated":"2020-05-22T03:29:41.782Z","comments":true,"path":"2020/20200518/","link":"","permalink":"https://gmaya.top/2020/20200518/","excerpt":"","text":"前言 YApi 是高效、易用、功能强大的 api 管理平台，旨在为开发、产品、测试人员提供更优雅的接口管理服务。可以帮助开发者轻松创建、发布、维护 API，YApi 还为用户提供了优秀的交互体验，开发人员只需利用平台提供的接口数据写入工具以及简单的点击操作就可以实现接口的管理。 简单来说，就是提供写接口文档的地方，后端人员写接口文档，前端人员直接在上面进行接口查看与开发。官网链接 安装环境linux下安装Yapi需要先安装它的依赖环境 安装nodejs（7.6+) 下载nodejs安装包https://nodejs.org/en/download/ 然后通过ftp上传到linux上。 或者直接在linux下载 # wget https://nodejs.org/dist/v12.16.3/node-v12.16.3-linux-x64.tar.xz 解压 # tar xf node-v12.16.3-linux-x64.tar.xz 解压文件的 bin 目录底下包含了 node、npm 等命令，我们可以使用 ln 命令来设置软连接： # ln -s /usr/local/node-v12.16.3-linux-x64/bin/npm /usr/local/bin # ln -s /usr/local/node-v12.16.3-linux-x64/bin/node /usr/local/bin 检查 检查是否安装正确 # node -v # npm -v 安装mongodb（2.6+）下载mongodb安装包https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.18.tgz 也可以去官网下载其他版本https://www.mongodb.com/download-center/community然后通过ftp上传到linux上。解压 # tar -zxvf mongodb-linux-x86_64-4.0.18.tgz 拷贝到指定目录 # mv mongodb-linux-x86_64-4.0.18 /usr/local/mongodb 配置环境变量 # vi /etc/profile 在最下面添加 export PATH=$PATH:/usr/local/mongodb/bin 创建文件夹一个存放数据，一个存放日志 # mkdir -p /data/db # mkdir -p /logs 创建mongodb配置文件进入bin目录 # cd /usr/local/mongodb/bin 创建mongodb.conf配置文件 # vi mongodb.conf 直接拷贝写进去 dbpath = /data/db # 数据文件存放目录 logpath = /logs/mongodb.log # 日志文件存放目录 port = 27017 # 端口 fork = true # 以守护程序的方式启用，即在后台运行 noauth = true # 不进行安全验证 # auth=true # 需要认证。如果放开注释，就必须创建MongoDB的账号，使用账号与密码才可远程 访问，第一次安装建议注释 bind_ip=0.0.0.0 # 允许远程访问，或者直接注释，127.0.0.1是只允许本地访问 启动mongodb # ./mongod -f mongodb.conf 检查是否启动成功 # netstat -nltp|grep mongod 或者 # ps -ef | grep mongod 安装yapi-cli直接在线安装 npm install -g yapi-cli --registry https://registry.npm.taobao.org 配置yapi # ln -s /usr/local/node-v12.16.3-linux-x64/bin/yapi /usr/bin/yapi 启动 yapi server 如果访问没效果，请打开9090端口，重启防火墙永久打开端口 # firewall-cmd --zone=public --add-port=9090/tcp --permanent 重启防火墙 # firewall-cmd --reload 然后再次启动yapi server 写上公司名称，其余一般默认，直接部署即可。 部署成功，请切换到部署目录，输入： “node vendors/server/app.js“ 指令启动服务器, 然后在浏览器打开 http://127.0.0.1:3000 访问 切换目录 # cd /usr/local/mongodb/bin/my-yapi/ 启动服务器 # node vendors/server/app.js 或者 以守护进程启动 # node vendors/server/app.js &amp; 打开http://127.0.0.1:3000 访问记得开3000端口，重启防火墙！！！very good！！！ 使用yapi使用默认账号登录账号：admin@admin.com密码：ymfe.org 只有超级管理员可以管理分组 创建分组 创建新项目 部分接口预览其中还有Mock地址等，前端直接访问。爽歪歪的，后端不管功能有没有做好，只要把接口定义好了，前端可以直接对比着开发。多爽，再也不用和前端说话了。/滑稽 新成员加入当技术老大，也就是admin把项目弄好， 所有开发都可以页面注册账号，然后查看接口和维护接口了。 所有成员注册账号然后管理员把他们都拉近项目组此时所有成员都可以看到接口啦。登录成员账号查看 安装浏览器插件其实写完接口的同时，可以直接进行浏览器访问本地服务，进行测试，就好比postman接口自测一样。这个可以直接点击截图里面的教程，安装浏览器插件此时本地启动我的项目（这个是前几篇写微服务写的，拿来用用） 测试一个 get请求user/test 直接省去了postman。哈哈 数据导出如果项目开发完，是备份也好，是给别人看也好，还是第三方使用也好，直接导出！导出效果查看 其实还有好多的功能，没有完全写到。慢慢体会吧。","categories":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Yapi","slug":"Yapi","permalink":"https://gmaya.top/tags/Yapi/"}]},{"title":"Spring Cloud Gateway网关整合","slug":"20200514","date":"2020-05-14T05:38:11.000Z","updated":"2020-08-12T08:04:17.396Z","comments":true,"path":"2020/20200514/","link":"","permalink":"https://gmaya.top/2020/20200514/","excerpt":"","text":"介绍 Spring Cloud Gateway为Spring生态系统上的一个API网关组件，主要提供一种简单而有效的方式路由映射到指定的API，并为他们提供安全性、监控和限流等等。 创建项目创建一个gmaya-gateway项目。 修改pom文件 &lt;!--gateway网关,内置webflux 依赖--> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-gateway&lt;/artifactId> &lt;/dependency> &lt;!--eureka客户端--> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId> &lt;/dependency> &lt;!--hystrix容错降级--> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-hystrix&lt;/artifactId> &lt;/dependency> &lt;!--健康监控--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-actuator&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;optional>true&lt;/optional> &lt;/dependency> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>fastjson&lt;/artifactId> &lt;/dependency> 启动类修改@SpringBootApplication @EnableDiscoveryClient 配置文件修改server: port: 9200 spring: application: name: gmaya-geteway cloud: gateway: discovery: locator: # 开启服务注册和发现 # 如果为true，访问路径有两个： # 1.ip:9200/gmaya-service-admin/user/test # 2.ip:9200/admin/user/test (这个是Path自己定义的/admin/**) # 如果为false，访问路径有一个ip:9200/admin/user/test enabled: false # 服务名配置为小写 lower-case-service-id: true routes: # 系统服务 - id: gmaya-service-admin # 不重复即可 uri: lb://gmaya-service-admin # 需要转发到的服务名称 predicates: # 以 /admin/开头的路径全部转发到lb://gmaya-service-admin的服务上，此时gmaya-service-admin可以负载均衡 - Path=/admin/** filters: # 去掉前面1个前缀,也就是真正转发访问的时候不带/admin/ - StripPrefix=1 # 注册服务中心 eureka: instance: # 心跳时间，即服务续约间隔时间（缺省为30s） lease-renewal-interval-in-seconds: 5 # 发呆时间，即服务续约到期时间（缺省为90s） lease-expiration-duration-in-seconds: 10 client: service-url: defaultZone: http://localhost:8000/eureka/ healthcheck: enabled: true # 开启健康检查 # 表示eureka client间隔多久去拉取服务注册信息，默认为30秒 registry-fetch-interval-seconds: 5 # 监控 management: endpoints: web: exposure: # 通过HTTP公开所有的端点， 默认是info,health include: '*' endpoint: health: # 显示完整信息，#默认是never（简要信息） show-details: always 简单测试注册中心：gmaya-service-center ：8000系统服务：gmaya-service-admin ：9001网关服务：gmaya-gateway ：9200 启动三个项目，访问： http://localhost:9200/admin/user/test 没问题！ 测一下gateway 的负载均衡，现在再把系统服务启动一个9011端口。 -Dserver.port=9011 启动之后查看效果 此时已经有了一个服务名一样，但是端口不一样的两个服务了。再次访问接口。查看后端打印情况，确实是轮询方式实现了负载均衡。 添加hystrix熔断降级 在分布式系统中，网关作为流量的入口，因此会有大量的请求进入网关，向其他服务发起调用，其他服务不可避免的会出现调用失败（超时、异常），失败时不能让请求堆积在网关上，需要快速失败并返回给客户端，想要实现这个要求，就必须在网关上做熔断、降级操作。 有两种方式进行熔断、降级 webflux方式添加HystrixFallbackHandlerpackage top.gmaya.gmayagateway.handler; import com.alibaba.fastjson.JSON; import lombok.extern.slf4j.Slf4j; import org.springframework.http.HttpStatus; import org.springframework.http.MediaType; import org.springframework.stereotype.Component; import org.springframework.web.reactive.function.BodyInserters; import org.springframework.web.reactive.function.server.HandlerFunction; import org.springframework.web.reactive.function.server.ServerRequest; import org.springframework.web.reactive.function.server.ServerResponse; import reactor.core.publisher.Mono; import java.util.HashMap; import java.util.Map; import java.util.Optional; import static org.springframework.cloud.gateway.support.ServerWebExchangeUtils.GATEWAY_ORIGINAL_REQUEST_URL_ATTR; /** * 熔断、降级类 * @author GMaya * @dateTime 2020/5/13 16:19 */ @Component @Slf4j public class HystrixFallbackHandler implements HandlerFunction&lt;ServerResponse> { @Override public Mono&lt;ServerResponse> handle(ServerRequest serverRequest) { Optional&lt;Object> originalUris = serverRequest.attribute(GATEWAY_ORIGINAL_REQUEST_URL_ATTR); originalUris.ifPresent(originalUri -> log.error(\"网关执行请求:{}失败,hystrix服务降级处理\", originalUri)); // 可以根据自己的工具类返回统一格式 Map&lt;String,String> map = new HashMap(); map.put(\"code\",\"500\"); map.put(\"msg\",\"服务出现异常，降级操作。\"); return ServerResponse.status(HttpStatus.INTERNAL_SERVER_ERROR.value()).contentType(MediaType.APPLICATION_JSON) .body(BodyInserters.fromValue(JSON.toJSONString(map))); } } 添加GatewayRoutesConfigpackage top.gmaya.gmayagateway.config; import lombok.AllArgsConstructor; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.http.MediaType; import org.springframework.web.reactive.function.server.RequestPredicates; import org.springframework.web.reactive.function.server.RouterFunction; import org.springframework.web.reactive.function.server.RouterFunctions; import top.gmaya.gmayagateway.handler.HystrixFallbackHandler; /** * 路由配置信息 * @author GMaya * @dateTime 2020/5/13 16:19 */ @Configuration // 全参构造方法 @AllArgsConstructor public class GatewayRoutesConfig { private final HystrixFallbackHandler hystrixFallbackHandler; @Bean public RouterFunction&lt;?> routerFunction(){ return RouterFunctions.route(RequestPredicates.path(\"/defaultFallback\").and(RequestPredicates.accept( MediaType.TEXT_PLAIN)),hystrixFallbackHandler); } } 修改配置文件大部分都省略了，和上面的一样。 spring: cloud: gateway: routes: # 系统服务 - id: gmaya-service-admin # 不重复即可 filters: # 去掉前面1个前缀,也就是真正转发访问的时候不带/admin/ - StripPrefix=1 # 降级配置 - name: Hystrix args: name: default fallbackUri: 'forward:/defaultFallback' hystrix: command: default: #default全局有效 execution: timeout: enabled: true isolation: thread: timeoutInMilliseconds: 3000 #断路器超时时间，默认1000ms 测试系统服务接口中添加延迟，模拟超时此时访问，3秒后返回设置好的熔断信息 或者将系统服务直接关闭。 web请求方式这种方式和正常controller中方法一样 新建DefaultFallbackControllerpackage top.gmaya.gmayagateway.controller; import lombok.extern.slf4j.Slf4j; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import java.util.HashMap; import java.util.Map; /** * web方式进行熔断降级 * @author GMaya * @dateTime 2020/5/13 17:05 */ @RestController @Slf4j public class DefaultFallbackController { @GetMapping(\"defaultFallback\") public Map&lt;String,String> defaultFallback(){ // 可以根据自己的工具类返回统一格式 Map&lt;String,String> map = new HashMap(); map.put(\"code\",\"500\"); map.put(\"msg\",\"服务出现异常，降级操作。2\"); log.error(\"网关执行请求失败，web方式记录\"); return map; } } 将第一种方式配置先去掉，重启项目 测试浏览器访问注意： 如果两个都开启了， 就会默认使用第一种方式。 添加限流 在高并发的系统中，往往需要在系统中做限流，一方面是为了防止大量的请求使服务器过载，导致服务不可用，另一方面是为了防止网络攻击。 常见的限流纬度有比如通过Ip来限流、通过uri来限流、通过用户访问频次来限流。 采用springcloud gateway 为我们提供了限流过滤器RequestRateLimiterGatewayFilterFactory。使用Redis和lua脚本实现了令牌桶的方式限流。 修改pom引入 &lt;!-- redis配置 --> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-data-redis-reactive&lt;/artifactId> &lt;/dependency> &lt;!--使用redis的lettuce连接池使用到，如果不用可不加--> &lt;dependency> &lt;groupId>org.apache.commons&lt;/groupId> &lt;artifactId>commons-pool2&lt;/artifactId> &lt;/dependency> 添加配置类创建KeyResolverConfig类 package top.gmaya.gmayagateway.config; import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import reactor.core.publisher.Mono; /** * 路由限流配置 * 配置文件中使用的那种限流 就是真正的限流方式，三种任选其一，不能同时存在 * @author GMaya * @dateTime 2020/5/14 9:19 */ @Configuration public class KeyResolverConfig { /** * 根据ip限流 * @return */ @Bean(value = \"hostAddrKeyResolver\") public KeyResolver hostAddrKeyResolver(){ return exchange -> Mono.just(exchange.getRequest().getRemoteAddress().getAddress().getHostAddress()); } /** * 根据路径限流 * @return */ /* @Bean(value = \"uriKeyResolver\") public KeyResolver uriKeyResolver(){ return exchange -> Mono.just(exchange.getRequest().getURI().getPath()); }*/ /** * 根据用户限流，参数中必须有user字段 * @return */ /*@Bean(value = \"userKeyResolver\") public KeyResolver userKeyResolver(){ return exchange -> Mono.just(exchange.getRequest().getQueryParams().getFirst(\"user\")); }*/ } 修改配置文件spring: cloud: gateway: routes: # 系统服务 - id: gmaya-service-admin # 不重复即可 uri: lb://gmaya-service-admin # 需要转发到的服务名称 predicates: # 以 /admin/开头的路径全部转发到lb://gmaya-service-admin的服务上，此时gmaya-service-admin可以负载均衡 - Path=/admin/** filters: # 去掉前面1个前缀,也就是真正转发访问的时候不带/admin/ - StripPrefix=1 # 降级配置 - name: Hystrix args: name: default fallbackUri: 'forward:/defaultFallback' # 限流配置 - name: RequestRateLimiter args: # 用于限流的键的解析器的 Bean 对象的名字 key-resolver: '#{@hostAddrKeyResolver}' # 令牌桶每秒填充平均速率（实际情况适当加大即可：10） redis-rate-limiter.replenishRate: 1 # 令牌桶总容量（实际情况适当加大即可：20） redis-rate-limiter.burstCapacity: 3 测试使用测试工具测试，每秒两次访问。查看redis中的值将令牌桶总容量中的3个值消耗完毕，再多次访问即页面就是一片空白。 TODO ： 后续有时间，研究一下自定义限流返回信息。","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"SpringCloud Gateway","slug":"SpringCloud-Gateway","permalink":"https://gmaya.top/tags/SpringCloud-Gateway/"}]},{"title":"Spring Boot Admin 监控","slug":"20200509","date":"2020-05-09T07:03:11.000Z","updated":"2020-08-12T08:04:23.076Z","comments":true,"path":"2020/20200509/","link":"","permalink":"https://gmaya.top/2020/20200509/","excerpt":"","text":"简介 Spring Boot Admin 用于监控基于 Spring Boot 的应用，它是在 Spring Boot Actuator 的基础上提供简洁的可视化 WEB UI。Spring Boot Admin 提供了很多功能，如显示 name、id 和 version，显示在线状态，Loggers 的日志级别管理，Threads 线程管理，Environment 管理等。 具体有什么好处，有什么作用，官网说的很清楚。github地址 spring boot amdin 的服务端新增项目gmaya-springbootadmin。作为spring boot amdin 的服务端 pom文件新增pom文件 &lt;dependency> &lt;groupId>de.codecentric&lt;/groupId> &lt;artifactId>spring-boot-admin-starter-server&lt;/artifactId> &lt;version>2.2.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId> &lt;/dependency> 启动类@SpringBootApplication @EnableDiscoveryClient @EnableAdminServer 配置文件修改application.yml server: port: 9100 spring: application: name: gmaya-springbootadmin eureka: client: service-url: defaultZone: http://localhost:8000/eureka/ healthcheck: enabled: true # 开启健康检查 # 监控 management: endpoints: web: exposure: # 通过HTTP公开所有的端点， 默认是info,health include: '*' endpoint: health: # 显示完整信息，#默认是never（简要信息） show-details: always 因为是使用的Eureka注册中心，所以不需要单独客户端了， 直接在想要监控的客户端直接暴露端点即可。 客户端修改比如此时，我想要监控gmaya-wepapi-admin，gmaya-service-admin只需要在这两个项目的配置类新增 # 监控 management: endpoints: web: exposure: # 通过HTTP公开所有的端点， 默认是info,health include: '*' endpoint: health: # 显示完整信息，#默认是never（简要信息） show-details: always 查看效果启动注册中心，admin服务端，两个客户端。 注册中心页面 spring boot admin 页面点进去，可以查看服务的详细信息 添加登录页面在spring boot amdin 服务端修改 可参考官方示例 pom新增 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-security&lt;/artifactId> &lt;/dependency> 配置文件 spring: application: name: gmaya-springbootadmin security: user: name: \"gmaya\" password: \"gmaya\" eureka: client: service-url: defaultZone: http://localhost:8000/eureka/ healthcheck: enabled: true # 开启健康检查 instance: metadata-map: user.name: ${spring.security.user.name} user.password: ${spring.security.user.password} 新增配置类SecuritySecureConfig，直接将官方给出的示例改造一下拿过来了。 package top.gmaya.gmayaspringbootadmin.config; import org.springframework.context.annotation.Configuration; import org.springframework.http.HttpMethod; import org.springframework.security.config.Customizer; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter; import org.springframework.security.web.authentication.SavedRequestAwareAuthenticationSuccessHandler; import org.springframework.security.web.csrf.CookieCsrfTokenRepository; import org.springframework.security.web.util.matcher.AntPathRequestMatcher; import de.codecentric.boot.admin.server.config.AdminServerProperties; /** * spring boot admin 官网示例 * @author GMaya * @dateTime 2020/5/9 10:30 */ @Configuration public class SecuritySecureConfig extends WebSecurityConfigurerAdapter { private final String adminContextPath; public SecuritySecureConfig(AdminServerProperties adminServerProperties) { this.adminContextPath = adminServerProperties.getContextPath(); } @Override protected void configure(HttpSecurity http) throws Exception { SavedRequestAwareAuthenticationSuccessHandler successHandler = new SavedRequestAwareAuthenticationSuccessHandler(); successHandler.setTargetUrlParameter(\"redirectTo\"); successHandler.setDefaultTargetUrl(this.adminContextPath + \"/\"); http.authorizeRequests((authorizeRequests) -> authorizeRequests .antMatchers(this.adminContextPath + \"/assets/**\").permitAll() .antMatchers(this.adminContextPath + \"/login\").permitAll().anyRequest().authenticated()) .formLogin((formLogin) -> formLogin.loginPage(this.adminContextPath + \"/login\") .successHandler(successHandler)) .logout((logout) -> logout.logoutUrl(this.adminContextPath + \"/logout\")) .httpBasic(Customizer.withDefaults()) .csrf((csrf) -> csrf.csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse()) .ignoringRequestMatchers( new AntPathRequestMatcher(this.adminContextPath + \"/instances\", HttpMethod.POST.toString()), new AntPathRequestMatcher(this.adminContextPath + \"/instances/*\", HttpMethod.DELETE.toString()), new AntPathRequestMatcher(this.adminContextPath + \"/actuator/**\"))); } } 重启，查看效果 邮件通知如果服务下线，会进行邮件通知在spring boot amdin 服务端修改 pom新增 &lt;!--邮件通知--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-mail&lt;/artifactId> &lt;/dependency> 配置文件application.yml修改 spring: application: name: gmaya-springbootadmin security: user: name: \"gmaya\" password: \"gmaya\" mail: # 发件人使用的qq邮箱服务 host: smtp.qq.com username: gmaya@qq.com # 授权码，不是密码，在qq邮箱设置-账号里面有生成授权码 password: ceevfekeeeeeeeee boot: admin: notify: mail: # 收件人，多个中间用,分隔 to: xxx@xxx.com # 发件人 from: gmaya@qq.com 重启服务，然后将一个客户端关闭","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"SpringBoot Admin","slug":"SpringBoot-Admin","permalink":"https://gmaya.top/tags/SpringBoot-Admin/"}]},{"title":"有关Druid的详细配置","slug":"20200507","date":"2020-05-07T06:07:14.000Z","updated":"2020-08-12T08:24:40.292Z","comments":true,"path":"2020/20200507/","link":"","permalink":"https://gmaya.top/2020/20200507/","excerpt":"","text":"简介 Druid 是 Java 语言中最好的数据库连接池。 Druid 能够提供强大的监控和扩展功能。 官方链接https://github.com/alibaba/druid 以下所有配置基于springboot2.2.6配置 怎么使用maven项目直接引用 &lt;!--阿里数据库连接池 --> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>druid-spring-boot-starter&lt;/artifactId> &lt;version>1.1.14&lt;/version> &lt;/dependency> Druid的一些常用配置spring: datasource: type: com.alibaba.druid.pool.DruidDataSource druid: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/gmaya?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=Asia/Shanghai username: root password: root # 初始化时建立物理连接的个数 initial-size: 5 # 最大连接池数量，并不是连接池越大越好。 max-active: 10 # 最小连接池数量 min-idle: 5 # 获取连接时最大等待时间，单位毫秒。 max-wait: 60000 # 是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大. pool-prepared-statements: true # 要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100 max-pool-prepared-statement-per-connection-size: 20 # 申请连接时会执行validationQuery检测连接是否有效,开启会降低性能.官方写的默认true，但是看源码里面好像是默认false test-on-borrow: false # 归还连接时会执行validationQuery检测连接是否有效,开启会降低性能,默认为false test-on-return: false # 合并多个DruidDataSource的监控数据 # 设置为true会和time-between-log-stats-millis冲突，启动项目失败 use-global-data-source-stat: false # 配置扩展插件 #监控统计用的filter:stat #日志用的filter:log4j #防御sql注入的filter:wall filters: stat,wall,slf4j # 用来检测连接是否有效的sql 必须是一个查询语句 # mysql中为 select 'x' # oracle中为 select 1 from dual validation-query: select 'x' # 通过connectProperties属性来打开mergeSql功能；慢SQL记录 connect-properties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000 # 定时输出统计信息到日志中，并每次输出日志会导致清零（reset）连接池相关的计数器。 time-between-log-stats-millis: 300000 #验证连接的有效性,建议配置为true，不影响性能，并且保证安全性。 test-while-idle: true #1) Destroy线程会检测连接的间隔时间，如果连接空闲时间大于等于minEvictableIdleTimeMillis则关闭物理连接。 #2) testWhileIdle的判断依据，详细看testWhileIdle属性的说明 time-between-eviction-runs-millis: 60000 # 连接保持空闲而不被驱逐的最小时间(连接池空闲连接的有效时间) min-evictable-idle-time-millis: 300000 # 浏览器监控页面 stat-view-servlet: # 是否启用监控页面，启用建议设置密码或白名单以保障安全 enabled: true url-pattern: '/druid/*' # IP白名单(没有配置或者为空，则允许所有访问) allow: 127.0.0.1,192.168.0.1 # IP黑名单 (存在共同时，deny优先于allow) deny: 192.168.0.1 login-username: gmaya login-password: gmaya # 配置DruidStatFilter web-stat-filter: enabled: true url-pattern: '/*' # 排除一些不必要的url，比如.js,/css/等等 exclusions: '*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*' 注意：建议配置白名单，密码访问。test-on-borrow默认值问题：官方文档解释：然而自己查看实际效果却不一样，不知道是不是我的问题。在源码中发现 dataMap.put(\"TestOnBorrow\", this.isTestOnBorrow()); dataMap.put(\"TestOnReturn\", this.isTestOnReturn()); this.testOnBorrow = false; this.testOnReturn = false; public static final boolean DEFAULT_TEST_ON_BORROW = false; public static final boolean DEFAULT_TEST_ON_RETURN = false; 或者把这两个参数注释。启动查看页面 数据库密码加密找到maven仓库中druid 的jar包 D:\\maven-repository\\com\\alibaba\\druid\\1.1.14 然后打开cmd输入 java -cp druid-1.1.14.jar com.alibaba.druid.filter.config.ConfigTools 你的密码 然后得到 privateKey:MIIBVgIBADANBgkqhkiG9w0BAQEFAASCAUAwggE8AgEAAkEAty+xzmC/4596DAoD+OI0zThfJq6P5ZdcBx+iTrBisD9ViQSwOUktzYi1o4ZiJ+Akv2rR9Q47+CpEvjH123nvQIDAQABAkEAj+IycrujfRKCNnqXbCGg/6BLfIKYG+mrKx05yzzYBz4e8R1/c1HCOn8D5YWDtKEXX5o1FKmJPHFyd0/OL8MhsQIhAP/P35Vitx7jN1B8gmgcmRhKYTgc6bVrkwNG8oLZaYr/AiEAt1IobJLPTOZffLkAY9K365IsK9Ky7a7uq4KniN8yOUMCIHWMbP41PVZ5LHrUrAvxUDxEOYZSauZNx2FTIa8U/pW7AiEArQv47MSIuuacLHfWTmSTCOQqnhN+o68FzRDAZQuLtVECIQCdBeUAddQSBtT7n32N36PWZS3oKnQ1sGVY2xCRC8o5Iw== publicKey:MFwwDQYJKoZIhvcNAQEBBQADS32sc5gv+OfegwKA/jiNM04Xyauj+WXXAcfok6wYrA/VYkEsDlJLc2ItaOGYifgJL9q0fUOO/gqRL4x0K1gJ70CAwEAAQ== password:FsnBV+3RyPMUBHjG4unvXV21Z1tC+xi19Xy5f/JSm0ktb5m17+B5fOzyDNTcFACmhx3zg/lMaFOIy0UfOA== 然后更改配置将明文密码替换 password: gMs08dqeyRQC3fsgEvb4pgmr2W/xVDgewGJLOJFZZOm8wfsVI0397uXGIe1W0rXJQl/meyB/GhqLt82fD8kiZw== 新增密码解密，同时config.decrypt.key=上面生成的publicKey. # ConfigFilter,开启密码加密功能 filter: config: enabled: true # 配置参数，让ConfigFilter解密密码 connection-properties: config.decrypt=true;config.decrypt.key=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBAK23puqpHZjqJ7+DQkJBWKIYdc0nkSqZeLVrQtwMvRwXCRgtG5+/IKO8xhMTSGwhjglloiTiVnJ9IynvpyFVpgUCAwEAAQ==","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gmaya.top/tags/SpringBoot/"},{"name":"Druid","slug":"Druid","permalink":"https://gmaya.top/tags/Druid/"}]},{"title":"给个人博客添加腾讯cos存储","slug":"20200426","date":"2020-04-26T03:10:14.000Z","updated":"2020-08-12T08:04:39.222Z","comments":true,"path":"2020/20200426/","link":"","permalink":"https://gmaya.top/2020/20200426/","excerpt":"","text":"前言图片，视频等一些静态资源放到自己网站会有一定的访问延迟。 开通存储桶在控制台找到对象存储,选择和云服务器一个地方的对象存储。按照步骤一步一步创建即可。 创建用户一般都是专门创建一个用户来操作的，当然至高无上的主账号也没啥毛病。将新建的用户加上几个策略 QcloudCOSGetServiceAccess 对象存储（COS）存储桶列表访问权限 QcloudCOSFullAccess 对象存储（COS）全读写访问权限 QcloudCOSDataFullControl 对象存储（COS）数据读、写、删除、列出的访问权限 然后在列表中权限管理设置上， 公有读私有写。数据读取、数据写入权限。 域名管理默认的链接一般都是https://blog-xxx.cos.ap-shanghai.myqcloud.com/imgs/MegellanicCloud_ZH-CN5132305226_1920x1080.jpg如果想要使用自己的子域名，比如：https://cos.gmaya.top/imgs/MegellanicCloud_ZH-CN5132305226_1920x1080.jpg 添加自定义域名 设置nginx的https代理 首先在DNS 解析 中 解析一个 cos 指向自己服务器ip的子域名，然后去申请一个免费的证书。 然后在域名管理里面添加自定义域名 之后修改nginx配置。因为我全站都是https的，不想因为几个图片没有https，导致全站又变成不安全的了。强迫症 server { listen 443 ssl; server_name cos.gmaya.top; ssl on; ssl_certificate 1_cos.gmaya.top_bundle.crt; # 你的证书 ssl_certificate_key 2_cos.gmaya.top.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / { access_log off; expires 30d; # 缓存30天，自己决定 root /data/www/; # 必须 proxy_pass https://blog-xxx.cos.ap-shanghai.myqcloud.com; # 你的桶路径 proxy_set_header Host $http_host; } error_page 404 /404.html; } 防盗链设置如果不开启防盗链，那么哪个网站都可以引用你的图片。 客户端工具在控制台直接进行下载在用户管理里面找到刚才新开的账户，然后根据API 密钥登录接口。以后上传图片就可以直接拖拽了","categories":[{"name":"随笔","slug":"随笔","permalink":"https://gmaya.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"cos","slug":"cos","permalink":"https://gmaya.top/tags/cos/"},{"name":"存储","slug":"存储","permalink":"https://gmaya.top/tags/%E5%AD%98%E5%82%A8/"}]},{"title":"添加feign熔断降级之后启动报错","slug":"20200425","date":"2020-04-25T08:16:14.000Z","updated":"2020-08-12T08:04:44.498Z","comments":true,"path":"2020/20200425/","link":"","permalink":"https://gmaya.top/2020/20200425/","excerpt":"","text":"问题在springcloud中，我使用feign自带的Hystrix实现熔断降级，但是却报错了。 Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled. [2020-04-25 15:49:59 ERROR main] org.springframework.boot.SpringApplication - Application run failed org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'userController': Unsatisfied dependency expressed through field 'userFacade'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'gmaya.service.admin.api.facade.UserFacade': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalStateException: No fallbackFactory instance of type class gmaya.service.admin.api.facade.factory.UserFallbackFactory found for feign client gmaya-service-admin at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:643) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:130) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessProperties(AutowiredAnnotationBeanPostProcessor.java:399) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1422) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:594) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:882) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:878) ~[spring-context-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.6.RELEASE.jar:2.2.6.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) [spring-boot-2.2.6.RELEASE.jar:2.2.6.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.2.6.RELEASE.jar:2.2.6.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-2.2.6.RELEASE.jar:2.2.6.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.2.6.RELEASE.jar:2.2.6.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) [spring-boot-2.2.6.RELEASE.jar:2.2.6.RELEASE] at top.gmaya.gmayawepapiadmin.GmayaWepapiAdminApplication.main(GmayaWepapiAdminApplication.java:15) [classes/:na] Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'gmaya.service.admin.api.facade.UserFacade': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalStateException: No fallbackFactory instance of type class gmaya.service.admin.api.facade.factory.UserFallbackFactory found for feign client gmaya-service-admin at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:178) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:101) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1818) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.getObjectForBeanInstance(AbstractAutowireCapableBeanFactory.java:1266) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:260) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1515) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1472) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1253) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1210) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:640) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] ... 19 common frames omitted Caused by: java.lang.IllegalStateException: No fallbackFactory instance of type class gmaya.service.admin.api.facade.factory.UserFallbackFactory found for feign client gmaya-service-admin at org.springframework.cloud.openfeign.HystrixTargeter.getFromContext(HystrixTargeter.java:81) ~[spring-cloud-openfeign-core-2.2.2.RELEASE.jar:2.2.2.RELEASE] at org.springframework.cloud.openfeign.HystrixTargeter.targetWithFallbackFactory(HystrixTargeter.java:63) ~[spring-cloud-openfeign-core-2.2.2.RELEASE.jar:2.2.2.RELEASE] at org.springframework.cloud.openfeign.HystrixTargeter.target(HystrixTargeter.java:53) ~[spring-cloud-openfeign-core-2.2.2.RELEASE.jar:2.2.2.RELEASE] at org.springframework.cloud.openfeign.FeignClientFactoryBean.loadBalance(FeignClientFactoryBean.java:253) ~[spring-cloud-openfeign-core-2.2.2.RELEASE.jar:2.2.2.RELEASE] at org.springframework.cloud.openfeign.FeignClientFactoryBean.getTarget(FeignClientFactoryBean.java:282) ~[spring-cloud-openfeign-core-2.2.2.RELEASE.jar:2.2.2.RELEASE] at org.springframework.cloud.openfeign.FeignClientFactoryBean.getObject(FeignClientFactoryBean.java:262) ~[spring-cloud-openfeign-core-2.2.2.RELEASE.jar:2.2.2.RELEASE] at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:171) ~[spring-beans-5.2.5.RELEASE.jar:5.2.5.RELEASE] ... 30 common frames omitted Process finished with exit code 1 首先先去看了几个点： 降级实现类是否加注解@Component 是否开启了Hystrix熔断支持 feign: hystrix: enabled: true 发现都没问题。 反正是知道加上UserFallbackFactory才报错的，之前没加就没报错。 觉得是spring注入不了这个bean。然后突然想起来，之前说项目的所有包都要在springboot启动类下面或者同级。 然后我就去看了一下我的项目结构这包名不一样啊，那么我启动web项目能发现在api项目中注册的bean？注解 springbootapplication源码中解释： 自动扫描所在包及其子包，会将有注解的加入到spring容器中。 然后就在web暴露接口项目的启动类中添加 @SpringBootApplication(scanBasePackages = { \"top.gmaya.gmayawepapiadmin\", \"gmaya.service.admin.api\"}) 第一个扫描自己的包，第二个扫描feign接口包 然后就启动成功了，奇怪之前不加为什么可以呢。然后看了一下，之前是@EnableFeignClients已经将接口扫描注册了。 @EnableFeignClients(basePackages = {\"gmaya.service.admin.api.facade\"}) 测试启动，访问，没问题。关闭impl实现项目，访问，出现熔断，降级。 顺便测测负载均衡，默认是开启的。刚才启动的impl实现项目端口是9001，然后使用 java -Dserver.port=9003 -jar gmaya-service-admin-impl-0.0.1-SNAPSHOT.jar server.port=9003 再次启动一个9003端口的项目。这个时候再次访问接口，发现这两个impl实现项目交替打印。因为默认负载均衡方式是轮询。","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"feign熔断","slug":"feign熔断","permalink":"https://gmaya.top/tags/feign%E7%86%94%E6%96%AD/"}]},{"title":"撞库、脱库和洗库","slug":"20200424","date":"2020-04-24T02:55:14.000Z","updated":"2020-08-12T08:04:49.830Z","comments":true,"path":"2020/20200424/","link":"","permalink":"https://gmaya.top/2020/20200424/","excerpt":"","text":"撞库“撞库”一般就是指黑客拿到已经泄露的用户信息，然后去各个网站进行登录，一旦你的账号密码在各个网站都一样，那么就相当于撞库成功。建议：各个网站的登录账号和密码不要一样。 脱库“拖库”就是黑客通过技术手段，盗取数据库信息的过程。建议：做防sql注入，文件上传漏洞等。 洗库“洗库”就是将得到的数据信息进行贩卖。变现。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://gmaya.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://gmaya.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"泰山版《Java开发手册》","slug":"20200422","date":"2020-04-22T05:49:14.000Z","updated":"2020-08-12T08:04:54.627Z","comments":true,"path":"2020/20200422/","link":"","permalink":"https://gmaya.top/2020/20200422/","excerpt":"","text":"每次阿里推出最新的开发手册，自己都会去喵一眼，看一下。养成好习惯泰山版：新增5条日期时间规约；新增2条表别名sql规约；新增统一错误码规约。 官网地址：https://developer.aliyun.com/topic/java2020","categories":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"分布式雪花算法工具类","slug":"20200421","date":"2020-04-21T03:30:14.000Z","updated":"2020-08-12T08:05:01.313Z","comments":true,"path":"2020/20200421/","link":"","permalink":"https://gmaya.top/2020/20200421/","excerpt":"","text":"SnowFlake 算法，是 Twitter 开源的分布式 id 生成算法。 SnowFlake算法的优点： 高性能高可用：生成时不依赖于数据库，完全在内存中生成。 容量大：每秒中能生成数百万的自增ID。 ID自增：存入数据库中，索引效率高。 SnowFlake算法的缺点：依赖与系统时间的一致性，如果系统时间被回调，或者改变，可能会造成id冲突或者重复。 SnowFlakeUtil工具类 package com.gmaya.springbootrabbitmq.utils; import lombok.extern.slf4j.Slf4j; import java.util.Map; import java.util.concurrent.ConcurrentHashMap; /** * 雪花算法 分布式 id生成工具类 * @author GMaya * @dateTime 2020/4/21 9:36 */ @Slf4j public class SnowFlakeUtil { private final long id; /** * 时间起始标记点，作为基准，一般取系统的最近时间 */ private final long epoch = 1587433423721L; /** * 机器标识位数 */ private final long workerIdBits = 10L; /** * 机器ID最大值: 1023 */ private final long maxWorkerId = -1L ^ -1L &lt;&lt; this.workerIdBits; /** * 0，并发控制 */ private long sequence = 0L; /** * 毫秒内自增位 */ private final long sequenceBits = 12L; /** * 12 */ private final long workerIdShift = this.sequenceBits; /** * 22 */ private final long timestampLeftShift = this.sequenceBits + this.workerIdBits; /** * 4095,111111111111,12位 */ private final long sequenceMask = -1L ^ -1L &lt;&lt; this.sequenceBits; /** * 记录产生时间毫秒数，判断是否是同1毫秒 */ private long lastTimestamp = -1L; /** * 传入机器id * @param id */ private SnowFlakeUtil(long id) { if (id > this.maxWorkerId || id &lt; 0) { throw new IllegalArgumentException(String.format(\"机器id不能大于%d或小于0\", this.maxWorkerId)); } this.id = id; } public synchronized long nextId() { // 获取当前时间毫秒数 long timestamp = timeGen(); if (this.lastTimestamp == timestamp) { //如果上一个timestamp与新产生的相等，则sequence加一(最大4095) this.sequence = this.sequence + 1 &amp; this.sequenceMask; if (this.sequence == 0) { // 超过最大值进行按位与，结果为0，也就是当这一毫秒序号超过最大值，就会循环等待下一毫秒 timestamp = this.tilNextMillis(this.lastTimestamp); } } else { this.sequence = 0; } // 如果时间回退，则报错或者返回-1，调用端进行判断 if (timestamp &lt; this.lastTimestamp) { log.error(String.format(\"时钟回退，拒绝 %d 毫秒内生成雪花id\", (this.lastTimestamp - timestamp))); return -1; } this.lastTimestamp = timestamp; // 当前时间-初始时间，然后左移timestampLeftShift。 // 将机器id左移workerIdShift // | 是按位或运算符，例如：x | y，只有当x，y都为0的时候结果才为0，其它情况结果都为1。 return timestamp - this.epoch &lt;&lt; this.timestampLeftShift | this.id &lt;&lt; this.workerIdShift | this.sequence; } /** * 如果说几十年后id重复了，把机器id加1，再用几十年 */ private static SnowFlakeUtil flowIdWorker = new SnowFlakeUtil(1); public static long getSnowFlakeId() { return flowIdWorker.nextId(); } /** * 等待下一个毫秒的到来, 保证返回的毫秒数在参数lastTimestamp之后 */ private long tilNextMillis(long lastTimestamp) { long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) { timestamp = timeGen(); } return timestamp; } /** * 获得系统当前毫秒数 */ private static long timeGen() { return System.currentTimeMillis(); } public static void main(String[] args) { //判断生成的记录是否有重复记录 final Map&lt;Long, Integer> map = new ConcurrentHashMap(); for (int i = 0; i &lt; 100; i++) { new Thread(() -> { for (int s = 0; s &lt; 2000; s++) { long snowFlakeId = SnowFlakeUtil.getSnowFlakeId(); log.info(\"生成雪花ID={}\",snowFlakeId); Integer put = map.put(snowFlakeId, 1); if (put != null) { throw new RuntimeException(\"主键重复\"); } } }).start(); } } }","categories":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"},{"name":"雪花算法","slug":"雪花算法","permalink":"https://gmaya.top/tags/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95/"}]},{"title":"SpringBoot+RabbitMQ削峰入门","slug":"20200420","date":"2020-04-20T10:06:14.000Z","updated":"2020-08-12T08:05:06.588Z","comments":true,"path":"2020/20200420/","link":"","permalink":"https://gmaya.top/2020/20200420/","excerpt":"","text":"前言当大量的客户访问请求打到后端，去访问数据库等，瞬间会爆炸的。经过前端或者其他的方案进行限流外。还是有大量的请求，这个时候需要削峰了。 简单的削峰例子先设置小一点，然后循环往队列里面放消息，消费的时候延迟2秒 spring: rabbitmq: host: 192.168.21.129 port: 5672 username: guest password: guest virtual-host: / listener: type: simple simple: prefetch: 1 # 消费者每次从队列获取的消息数量 concurrency: 1 # 消费者数量 max-concurrency: 1 # 启动消费者最大数量 调用生产者 for (int i=0;i&lt;10;i++){ // 调用消息生产者 sender.sender(msg+i); } 消费者 @RabbitListener(queues = MQConfig.QUEUE) public void receiver(String msg) throws InterruptedException { Thread.sleep(2000L); // 模拟处理需要2秒 log.info(\"消费者消费。。。。。。{}\", msg); } 页面访问： http://localhost:8088/hello?msg=GMaya 此时消息会全部放到列队，但是会一条一条消费。简单的实现了削峰处理 调整消费者的数量 prefetch: 1 # 消费者每次从队列获取的消息数量 concurrency: 2 # 消费者数量 max-concurrency: 10 # 启动消费者最大数量 此时就会有两个消费者同时去消费队列中的消息。所以这个消费者数量需要根据实际的情况去设置所能承受的一个值，也就是峰值。 重试策略如果说消费者在消费的过程中失败了，那么会一直消费，一直到成功为止。 但是也可以添加重试策略，比如失败三次就不在消费了。 listener: type: simple simple: prefetch: 1 # 消费者每次从队列获取的消息数量 concurrency: 2 # 消费者数量 max-concurrency: 10 # 启动消费者最大数量 # 重试策略相关配置 retry: enabled: true #开启消费者重试 max-attempts: 2 #最大重试次数 initial-interval: 2000 #重试间隔时间 模拟异常 @RabbitListener(queues = MQConfig.QUEUE) public void receiver(String msg) throws InterruptedException { Thread.sleep(2000L); // 模拟处理需要2秒 if(\"GMaya8\".equals(msg)){ System.out.println(1/0); } log.info(\"消费者消费。。。。。。{}\", msg); } 此时的结果就是其他的消息被消费，但是这个GMaya8没了，失败三次之后就没了。队列中也消失了。 如何保证rabbitmq消息不丢失丢失数据场景： 生产者没有生产成功，即生产者丢失 rabbitmq丢失了 消费端丢失，即消费端没消费成功。 开启confirm回调，启动手动确定消息消费。 server: port: 8088 spring: rabbitmq: host: 192.168.21.129 port: 5672 username: guest password: guest virtual-host: / publisher-returns: true # 实现ReturnCallback接口，如果消息从交换器发送到对应队列失败时触发 publisher-confirm-type: correlated listener: type: simple simple: acknowledge-mode: manual # 消息消费确认，可以手动确认 prefetch: 1 # 消费者每次从队列获取的消息数量 concurrency: 2 # 消费者数量 max-concurrency: 10 # 启动消费者最大数量 # 重试策略相关配置 retry: enabled: true #开启消费者重试 max-attempts: 3 #最大重试次数 initial-interval: 2000 #重试间隔时间 template: #在消息没有被路由到合适队列情况下会将消息返还给消息发布者 #当mandatory标志位设置为true时，如果exchange根据自身类型和消息routingKey无法找到一个合适的queue存储消息， # 那么broker会调用basic.return方法将消息返还给生产者;当mandatory设置为false时， # 出现上述情况broker会直接将消息丢弃;通俗的讲，mandatory标志告诉broker代理服务器至少将消息route到一个队列中， # 否则就将消息return给发送者; #: true # 启用强制信息 mandatory: true 使用交换机模式生产者 @Component @Slf4j public class Sender implements RabbitTemplate.ConfirmCallback{ @Autowired private RabbitTemplate rabbitTemplate; public Sender(RabbitTemplate rabbitTemplate){ rabbitTemplate.setConfirmCallback(this); } /** * fanout 模式 * @param msg */ public void fanoutSender(String msg) { log.info(\"fanout生产者生产消息。。。。。{}\", msg); // 会把消息发送给 所有绑定到此交换机的全部列队；routing_key会被忽略。 CorrelationData correlationData = new CorrelationData(UUID.randomUUID().toString()); this.rabbitTemplate.convertAndSend(MQConfig.FANOUT_EXCHANGE, \"\", msg, correlationData); } @Override public void confirm(CorrelationData correlationData, boolean b, String s) { if(b){ log.info(\"消息生产成功\"); }else{ log.info(\"消息生产失败\"); // 可以自己写重新发送消息 } } } 消费者 @RabbitListener(queues = MQConfig.QUEUE_A) public void receiverA(String msg,Message message, Channel channel) throws IOException { log.info(\"QUEUE_A消费者消费。。。。。。{}\", msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); } @RabbitListener(queues = MQConfig.QUEUE_B) public void receiverB(String msg,Message message, Channel channel) throws IOException { log.info(\"QUEUE_B消费者消费。。。。。。{}\", msg); channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); } 这个时候已经解决出现问题，倒是消息消失的情况。当处理好问题之后，重启就会重新消费","categories":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://gmaya.top/categories/RabbitMQ/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://gmaya.top/tags/RabbitMQ/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gmaya.top/tags/SpringBoot/"}]},{"title":"给hexo博客加404公益页面","slug":"20200419","date":"2020-04-19T10:24:14.000Z","updated":"2020-08-12T08:05:11.791Z","comments":true,"path":"2020/20200419/","link":"","permalink":"https://gmaya.top/2020/20200419/","excerpt":"","text":"腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！效果如：https://gmaya.top/404.html 目的：如果访问网站不存在的页面，则返回腾讯公益404页面 使用方法，新建 404.html 页面，放到主题的 source 目录下，内容如下： &lt;!DOCTYPE HTML> &lt;html> &lt;head> &lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/> &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" /> &lt;meta name=\"robots\" content=\"all\" /> &lt;meta name=\"robots\" content=\"index,follow\"/> &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://qzone.qq.com/gy/404/style/404style.css\"> &lt;/head> &lt;body> &lt;script type=\"text/plain\" src=\"https://www.qq.com/404/search_children.js\" charset=\"utf-8\" homePageUrl=\"/\" homePageName=\"回到我的主页\"> &lt;/script> &lt;script src=\"https://qzone.qq.com/gy/404/data.js\" charset=\"utf-8\">&lt;/script> &lt;script src=\"https://qzone.qq.com/gy/404/page.js\" charset=\"utf-8\">&lt;/script> &lt;/body> &lt;/html> 修改nginx配置，只要访问的页面不存在则跳转到404公益页面。 在nginx.conf的443端口下添加 error_page 404 /404.html; 重启即可","categories":[{"name":"优化","slug":"优化","permalink":"https://gmaya.top/categories/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"优化","slug":"优化","permalink":"https://gmaya.top/tags/%E4%BC%98%E5%8C%96/"},{"name":"公益","slug":"公益","permalink":"https://gmaya.top/tags/%E5%85%AC%E7%9B%8A/"}]},{"title":"win10系统hexo博客自动发布自动拉取","slug":"20200417-2","date":"2020-04-17T09:34:14.000Z","updated":"2020-08-12T08:05:17.261Z","comments":true,"path":"2020/20200417-2/","link":"","permalink":"https://gmaya.top/2020/20200417-2/","excerpt":"","text":"我之前的流程是这样发布个人博客的。 本地(win10)编写博客 本地上传博客到github 服务器（win10）通过git拉取最新代码 因为是nginx代理的，所以这个时候直接域名访问就是最新的了。 问题：每次本地上传完都需要远程控制服务器，然后手动拉取代码。 更改后实现的效果： 本地(win10)编写博客 本地上传博客到github 其实就是 本地编写完，然后 hexo s 查看没问题，然后 hexo g hexo d ，完事！ 本地（win）无需做任何修改，hexo d 只要吧博客上传到github就行。 服务器（win）做以下更改：存放博客根目录新建文件 pull.bat 名称随意里面加上一句命令 git pull 对，就是这么简单粗暴的一个命令。 然后win系统新增任务计划点击创建任务 常规设置触发器设置看自己想法设置间隔时间，博客没那么频繁，所以一个小时拉取一次操作设置 保存！哈哈。 我服务器有点东西必须用win，没办法。 等到时候用linux在弄一个，原理应该都是这样！","categories":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"win10","slug":"win10","permalink":"https://gmaya.top/tags/win10/"},{"name":"自动拉取git","slug":"自动拉取git","permalink":"https://gmaya.top/tags/%E8%87%AA%E5%8A%A8%E6%8B%89%E5%8F%96git/"}]},{"title":"Linux下RabbitMQ另外一种延迟功能实现","slug":"20200417","date":"2020-04-17T07:00:14.000Z","updated":"2020-08-12T08:05:23.668Z","comments":true,"path":"2020/20200417/","link":"","permalink":"https://gmaya.top/2020/20200417/","excerpt":"","text":"前言前面写了使用死信对列实现了：订单下单之后 30 分钟后，用户如果没有付钱，系统需要自动取消订单。今天在研究一下另外一种rabbitmq-delayed-message-exchange插件 注意：延迟插件 rabbitmq-delayed-message-exchange 是在 RabbitMQ 3.5.7 及以上的版本才支持的，依赖 Erlang/OPT 18.0 及以上运行环境。 完整代码链接https://gitee.com/GMaya/springboot-rabbitmq 安装插件首先找到RabbitMQ插件目录我的是在 /usr/lib/rabbitmq/lib/rabbitmq_server-3.6.15/plugins 然后下载插件https://www.rabbitmq.com/community-plugins.html下载需要的版本，我下载的是3.6.x解压之后把rabbitmq_delayed_message_exchange-20171215-3.6.x.ez文件上传到plugins目录 然后开启插件 rabbitmq-plugins enable rabbitmq_delayed_message_exchange 查看开启的插件列表 rabbitmq-plugins list 重启服务 集成到spring boot配置类/** * 使用rabbitmq-delayed-message-exchange 构建一个的延迟队列 * @author GMaya * @dateTime 2020/4/17 14:17 */ @Configuration public class DelayMQConfig { /** * 创建Queue * @return */ @Bean public Queue delayedQueue() { return new Queue(\"delayedQueue\", true); } /** * 创建延迟交换机 * @return */ @Bean CustomExchange customExchange() { Map&lt;String, Object> args = new HashMap&lt;String, Object>(); args.put(\"x-delayed-type\", \"direct\"); //参数二为类型：必须是x-delayed-message return new CustomExchange(\"delayed-Exchange\", \"x-delayed-message\", true, false, args); } /** * 绑定队列到交换器 * @param queue * @param exchange * @return */ @Bean Binding binding() { return BindingBuilder.bind(delayedQueue()).to(customExchange()).with(\"delayedQueue\").noargs(); } } 生产者/** * delayed消息生产着 * @author GMaya * @dateTime 2020/4/17 14:20 */ @Slf4j @Component public class DelaySender { @Autowired private AmqpTemplate amqpTemplate; public void sendMsg(String msg) { log.info(\"生产者===订单号：{}\", msg); // 这里把消息生产出来，指定交换机和queue amqpTemplate.convertAndSend(\"delayed-Exchange\", \"delayedQueue\", msg, new MessagePostProcessor() { @Override public Message postProcessMessage(Message message) throws AmqpException { MessageProperties messageProperties = message.getMessageProperties(); // 这里是延迟时间，单位ms，这里延迟10s messageProperties.setHeader(\"x-delay\",10000); return message; } }); } } 消费者/** * 消费者 * @author GMaya * @dateTime 2020/4/17 14:27 */ @Slf4j @Component public class DelayReceiver { @RabbitListener(queues = \"delayedQueue\") public void orderMsg(String msg) throws IOException { log.info(\"消费者===订单号：{}\", msg); } } 测试在HelloController中注入调用启动项目访问测试http://localhost:8088/hello?msg=hello,delay完美延迟！ 总结使用RabbitMQ两种延迟消息方法， 通过消息过期后进入死信交换器，再由交换器转发到延迟消费队列，实现延迟功能 使用 rabbitmq-delayed-message-exchange 插件实现延迟功能。如果版本合适，第二种方法比第一个简单点，没那么绕。。","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://gmaya.top/tags/RabbitMQ/"},{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/tags/Linux/"}]},{"title":"记一次优化我的个人博客","slug":"20200414-2","date":"2020-04-14T06:32:14.000Z","updated":"2020-08-12T08:05:28.716Z","comments":true,"path":"2020/20200414-2/","link":"","permalink":"https://gmaya.top/2020/20200414-2/","excerpt":"","text":"前言优化一下自己的博客访问速度等！不要求画面多么炫酷，但是，必须要快！快！！我的博客：https://gmaya.top欢迎访问哟！ 效果话不多说，先看效果优化前：优化后： Hexo内部优化我的个人博客站点使用hexo搭建，使用next模板。模板配置文件搜索：motionenable：是否开启页面动画，就是刚进来有没有那个慢吞吞的特效async：是否开启异步加载，就是你的页面内容和特效是一起加载的。 Nginx优化访问我的博客静态页面是通过Nginx代理的 修改worker_connections默认是1024，相对而言扩大5倍，根据自己服务器决定 events { worker_connections 5024; } 修改http配置需要哪个就加上哪个 http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; # 关闭日志 access_log off; # 隐藏响应头中的有关操作系统和web server（Nginx）版本号的信息，这样对于安全性是有好处的。 server_tokens off; sendfile on; # 等数据包累计到一定大小发送，启用 sendfile 生效 tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; # 开启gzip gzip on; # 启用gzip压缩的最小文件；小于设置值的文件将不会被压缩 gzip_min_length 1k; # gzip 压缩级别 1-10 gzip_comp_level 2; # 禁用IE 6 gzip gzip_disable \"MSIE [1-6]\\.\"; gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php; # 是否在http header中添加Vary: Accept-Encoding，建议开启 gzip_vary on; } 开启https访问不喜欢网站一直显示不安全。如果是http请求，将转发到https server { listen 80; server_name gmaya.top; rewrite ^(.*)$ https://$host:443/$1 permanent; } 静态资源缓存如果不是https，直接把内容加到80端口即可 server { listen 443 ssl; server_name gmaya.top; # 添加自己的证书 ssl_certificate xxx.crt; ssl_certificate_key xxx.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location ~* \\.(css|js|ico|gif|jpg|jpeg|png)$ { # 同上，通配所有以.css/.js/...结尾的请求 access_log off; #忽略头部禁止缓存申明，类似与CDN的强制缓存功能 proxy_ignore_headers \"Cache-Control\" \"Expires\" \"Set-Cookie\"; # 开启缓存，时间864000秒， add_header Cache-Control \"public,max-age=864000\"; root C:\\dev\\blog; index index.html index.htm; } location ~* \\.(html|xml)$ { access_log off; # max-age&lt;=0 时向server发送http请求确认 ,该资源是否有修改, 有的话 返回200 , 无的话 返回304。 add_header Cache-Control no-cache; root C:\\dev\\blog; index index.html index.htm; } location / { access_log off; root C:\\dev\\blog; index index.html index.htm; } } 我博客上还有很多外链，哎，懒得换咯！（如果有一天外链挂了，再迁移过来。） 虽然一天到晚，总浏览量还不到20，哈哈。","categories":[{"name":"优化","slug":"优化","permalink":"https://gmaya.top/categories/%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://gmaya.top/tags/Nginx/"},{"name":"优化","slug":"优化","permalink":"https://gmaya.top/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"Linux下Nginx的入门学习记录","slug":"20200414","date":"2020-04-14T01:00:00.000Z","updated":"2020-05-16T05:09:27.496Z","comments":true,"path":"2020/20200414/","link":"","permalink":"https://gmaya.top/2020/20200414/","excerpt":"","text":"前言前两篇文章已经把环境等都安装完了。 反向代理demo1进入tomcat/webapps新建java文件夹，里面放入一个html访问效果http://192.168.21.129:8080/java/index.html一般情况下都是直接访问域名+路径，没有端口号的。这就该nginx上场了修改nginx配置进入conf文件夹 vi nginx.conf server { listen 80; server_name localhost; location / { proxy_pass http://127.0.0.1:8080; } 然后重启nginx ./nginx -s reload 访问http://192.168.21.129/java/index.html完美实现！ 反向代理demo2复制一个tomcat，然后修改端口为8081 cp -r tomcat tomcat8081 修改tomcat8081的端口，进入conf文件夹 vi server.xml 直接输入/8080搜索将8080 修改为8081关闭端口 8005 修改为 8006.修改一下此tomcat/webapps的文件将之前的java/inde.html,修改为gmaya，内容也修改一下，用于区别。访问不了，不要忘记开启8081端口哟最终为：http://192.168.21.129:8080/java/index.htmlhttp://192.168.21.129:8081/gmaya/index.html 修改nginx配置重启，访问看效果 Nginx 配置-负载均衡修改配置还是上面的两个tomcat，直接修改nginx http { upstream myserver { server 127.0.0.1:8080; server 127.0.0.1:8081; } server { listen 80; location / { proxy_pass http://myserver; } } } 默认轮询的方式，每次打开新窗口，8080和8081会交替出现。提示：修改webapps/ROOR下index文件会看的效果更明显 Nginx 分配策略： 轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除 权重，默认1，权重越高被分配的客户端越多 指定轮询几率，weight 和访问比率成正比。比如一台服务器性能好，一台不好，那就分配性能好的多一点 ip_hash 每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题。 fair（第三方） 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 Nginx 高可用tomcat搞了多个，也就是咱们web服务器搞了集群，不用担心某个tomcat挂了。但是现在只有一台Nginx，如果Nginx挂了，也是gg所以我们需要两台以上Nginx来实现故障转移和高可用。 keepalived简介 Keepalived的作用是检测服务器的状态，如果有一台web服务器宕机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。 安装keepalived yum install keepalived -y 查看网卡ifconfig如果ifconfig使用不了 yum install net-tools CentOS7里面的网卡名称默认不再是eth0，而是ens33 我没有那么多服务器，所以我是使用的虚拟机。做到这一步，然后克隆一个一模一样的。一个主机为：192.168.21.129一个从机为：192.168.21.130 修改主从keepalived配置主机#主机 #检测脚本 vrrp_script chk_http_port { script \"/usr/local/src/check_nginx.sh\" #心跳执行的脚本，检测nginx是否启动 interval 2 #（检测脚本执行的间隔，单位是秒） weight 2 #权重 } #vrrp 实例定义部分 vrrp_instance VI_1 { state MASTER # 指定keepalived的角色，MASTER为主，BACKUP为从 interface ens33 # 当前进行vrrp通讯的网络接口卡(当前centos的网卡) 用ifconfig查看你具体的网卡 virtual_router_id 52 # 虚拟路由编号，主从要一直 priority 100 # 优先级，数值越大，获取处理请求的优先级越高 advert_int 1 # 检查间隔，默认为1s(vrrp组播周期秒数) #授权访问 authentication { auth_type PASS #设置验证类型和密码，MASTER和BACKUP必须使用相同的密码才能正常通信 auth_pass 1111 } track_script { chk_http_port #（调用检测脚本） } virtual_ipaddress { 192.168.21.133 # 定义虚拟ip(VIP)，可多设，每行一个 } } 从机#从机 #检测脚本 vrrp_script chk_http_port { script \"/usr/local/src/check_nginx.sh\" #心跳执行的脚本，检测nginx是否启动 interval 2 #（检测脚本执行的间隔，单位是秒） weight 2 #权重 } #vrrp 实例定义部分 vrrp_instance VI_1 { state BACKUP # 指定keepalived的角色，MASTER为主，BACKUP为从 interface ens33 # 当前进行vrrp通讯的网络接口卡(当前centos的网卡) 用ifconfig查看你具体的网卡 virtual_router_id 52 # 虚拟路由编号，主从要一直 priority 99 # 优先级，数值越大，获取处理请求的优先级越高 advert_int 1 # 检查间隔，默认为1s(vrrp组播周期秒数) #授权访问 authentication { auth_type PASS #设置验证类型和密码，MASTER和BACKUP必须使用相同的密码才能正常通信 auth_pass 1111 } track_script { chk_http_port #（调用检测脚本） } virtual_ipaddress { 192.168.21.133 # 定义虚拟ip(VIP)，可多设，每行一个 } } 检测nginx脚本在/usr/local/src/check_nginx.sh路径创建 #!/bin/bash #检测nginx是否启动了 A=`ps -C nginx --no-header |wc -l` if [ $A -eq 0 ];then #如果nginx没有启动就启动nginx systemctl start nginx #重启nginx if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then #nginx重启失败，则停掉keepalived服务，进行VIP转移 killall keepalived fi fi 修改文件访问权限 chmod 775 check_nginx.sh 启动主从tomcat，nginx启动主从keepalived systemctl start keepalived.service 重启keepalived命令 service keepalived restart 问题如果启动http://192.168.21.133/ 发现访问不了，可能影响的原因有 默认的global_defs配置中 ，将vrrp_strict注释！ 定义的virtual_ipaddress虚拟ip，要和自己本身的虚拟机ip段一样，比如：我的虚拟机ip为：http://192.168.21.130/ ，http://192.168.21.129/ ； 那么我的虚拟ip 也在http://192.168.21.*/ 范围内，我刚开始配置的192.168.200.100，就一直ping不通，改成192.168.21.133，解决。 virtual_router_id 默认的是51， 可以修改成52，然后重启一下看看。 priority 主从的优先级不一样。 测试现在已经能通过虚拟ip访问了， 现在直接把主机关闭。再次访问，发现还是可以访问。说明咱们的高可用ok了。 如果发现检测nginx脚本，中的启动命令不能够直接启动Nginx，所以还要把自己服务器上的systemctl start nginx 配置一下。 Nginx优化 Nginx默认采用多进程工作方式，Nginx启动后，会运行一个master进程和多个worker进程。其中master充当整个进程组与用户的交互接口，同时对进程进行监护，管理worker进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。worker用来处理基本的网络事件，worker之间是平等的，他们共同竞争来处理来自客户端的请求 调整worker_processesworker 数和服务器的 cpu 数相等是最为适宜 默认：worker_processes: 1 调大：worker_processes: CPU核心数，(双核4线程，可以设置为4) 最大化worker_connectionsworker_processes与worker_connections 设置好合适大小，可以提示nginx处理性能，非常重要。数字越大，能同时处理的连接越多 默认：worker_connections: 1024 调大：worker_connections: 100000，（调大到10万连接） connections不是随便设置的，而是与两个指标有重要关联，一是内存，二是操作系统级别的“进程最大可打开文件数”。具体可以翻阅资料查看。不再絮叨了。 启用Gzip压缩压缩文件大小，减少了客户端http的传输带宽，因此提高了页面加载速度 为静态文件启用缓存根据具体情况区别要不要开启 禁用access_logs访问日志记录，它记录每个nginx请求，因此消耗了大量CPU资源，从而降低了nginx性能","categories":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://gmaya.top/tags/Nginx/"}]},{"title":"Linux安装Nginx","slug":"20200413-3","date":"2020-04-13T06:20:11.000Z","updated":"2020-05-16T05:05:44.706Z","comments":true,"path":"2020/20200413-3/","link":"","permalink":"https://gmaya.top/2020/20200413-3/","excerpt":"","text":"前言上篇安装完jdk，tomcat 就在来一个Nginx。 安装Nginx进入你想要安装的目录 cd /usr/local/ 创建nginx文件夹 mkdir nginx 进入nginx目录下 cd nginx/ 下载nginx在线下载或者自己下载好然后上传过来。 wget http://nginx.org/download/nginx-1.16.1.tar.gz 提示: 如果你没有wget,请先安装wget yum -y install wget 安装所需的插件 yum install gcc c++ yum install -y pcre pcre-devel yum install -y zlib zlib-devel yum install -y openssl openssl-devel 用于编译c、c++代码的GCC；用c语言编写的正则表达式函数库Pcre(使用rewrite模块)；用于数据压缩的函式库的Zlib；安全套接字层密码库OpenSSL（启用SSL支持） 解压，编译按照顺序执行 tar -zxvf nginx-1.16.1.tar.gz cd nginx-1.16.1 ./configure make &amp;&amp; sudo make install 启动nginx启动 cd sbin/ ./nginx 停用 ./nginx -s stop 重启 ./nginx -s reload 检查修改的nginx.conf配置是否正确 ./nginx -t 启动完之后，浏览器访问http://192.168.21.129/如果没有开启80端口，是不行的开启80端口我使用的是centos7加入80规则–permanent #永久生效，没有此参数设置，重启后会失效。 firewall-cmd --zone=public --add-port=80/tcp --permanent 重启防火墙 firewall-cmd --reload 刷新页面http://192.168.21.129/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://gmaya.top/tags/Nginx/"}]},{"title":"Linux安装Tomcat","slug":"20200413-2","date":"2020-04-13T05:45:11.000Z","updated":"2020-05-16T05:04:42.638Z","comments":true,"path":"2020/20200413-2/","link":"","permalink":"https://gmaya.top/2020/20200413-2/","excerpt":"","text":"前言上篇安装完jdk，就在来一个Tomcat。 安装Tomcat进入你想要安装的目录 cd /usr/local/ 创建tomcat文件夹 mkdir tomcat 进入tomcat目录下 cd tomcat/ 下载tomcat 8在线下载或者自己下载好然后上传过来。 wget https://mirror.bit.edu.cn/apache/tomcat/tomcat-8/v8.5.54/bin/apache-tomcat-8.5.54.tar.gz 提示: 如果你没有wget,请先安装wget yum -y install wget 解压 tar -zxvf apache-tomcat-8.5.54.tar.gz 开启默认8080端口我使用的是centos7加入8080 规则--permanent #永久生效，没有此参数设置，重启后会失效。 firewall-cmd --zone=public --add-port=8080/tcp --permanent 重启防火墙 firewall-cmd --reload 启动 ./bin/startup.sh 浏览器访问： http://192.168.21.129:8080/","categories":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/tags/Linux/"},{"name":"Tomcat","slug":"Tomcat","permalink":"https://gmaya.top/tags/Tomcat/"}]},{"title":"Linux安装jdk1.8","slug":"20200413","date":"2020-04-13T05:09:11.000Z","updated":"2020-05-16T05:03:57.716Z","comments":true,"path":"2020/20200413/","link":"","permalink":"https://gmaya.top/2020/20200413/","excerpt":"","text":"前言一直都想写一遍这个，虽然不是经常用到。这次准备搞一下Nginx,所以就从0开始，记录一下。 安装jdk1.8进入你想要安装的目录 cd /usr/local/ 创建jdk文件夹 mkdir jdk1.8 进入jdk目录下 cd jdk1.8/ 下载jdk1.8 在线下载或者自己下载好然后上传过来。 wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz 耐心等待。。。解压 tar -zxvf jdk-8u131-linux-x64.tar.gz 配置环境变量先找到你的jdk安装目录记录一下 /usr/local/jdk1.8/jdk1.8.0_131 添加环境变量 vi /etc/profile 输入a，在文本最后面加上 export JAVA_HOME=/usr/local/jdk1.8/jdk1.8.0_131 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 按 esc，然后：wq保存退出。加载环境变量 source /etc/profile 查看版本号，看是否安装成功！ java -version","categories":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/tags/Linux/"},{"name":"jdk","slug":"jdk","permalink":"https://gmaya.top/tags/jdk/"}]},{"title":"分享一下Typora工具","slug":"20200411","date":"2020-04-11T13:11:11.000Z","updated":"2020-05-16T05:02:45.292Z","comments":true,"path":"2020/20200411/","link":"","permalink":"https://gmaya.top/2020/20200411/","excerpt":"","text":"分享 一个自己正在使用的工具 Typora 是一款支持实时预览的 Markdown 文本编辑器。 Typora官网https://www.typora.io/简直是编辑博客的不二之选！免费免费免费！！！同时你也可以编写文档，编写简历，支持导出多种模式。太香了","categories":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"Spring Boot整合RabbitMQ入门","slug":"20200410","date":"2020-04-10T09:22:31.000Z","updated":"2020-05-16T05:02:25.257Z","comments":true,"path":"2020/20200410/","link":"","permalink":"https://gmaya.top/2020/20200410/","excerpt":"","text":"前言 RabbitMQ 即一个消息队列，主要是用来实现应用程序的异步和解耦，同时也能起到消息缓冲，消息分发的作用。 安装完MQ，就该写个例子研究研究了。新建Spring Boot 项目。 修改pom文件新增RabbitMQ 支持 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-amqp&lt;/artifactId> &lt;/dependency> 修改配置文件默认网页guest用户是不允许访问的，需要修改一下权限。上篇有写 server: port: 8088 spring: rabbitmq: host: 192.168.21.129 port: 5672 username: guest password: guest virtual-host: / 四种交换机交换机的功能主要是接收消息并且转发到绑定的队列，交换机不存储消息。 交换机有四种类型：Direct, topic, Headers and Fanout Direct是RabbitMQ默认的交换机模式,也是最简单的模式.即创建消息队列的时候,指定一个BindingKey.当发送者发送消息的时候,指定对应的Key.当Key和消息队列的BindingKey一致的时候,消息将会被发送到该消息队列中. topic转发信息主要是依据通配符,队列和交换机的绑定主要是依据一种模式(通配符+字符串),而当发送消息的时候,只有指定的Key和该模式相匹配的时候,消息才会被发送到该消息队列中. headers也是根据一个规则进行匹配,在消息队列和交换机绑定的时候会指定一组键值对规则,而发送消息的时候也会指定一组键值对规则,当两组键值对规则相匹配的时候,消息会被发送到匹配的消息队列中. Fanout是路由广播的形式,将会把消息发给绑定它的全部队列,即便设置了key,也会被忽略. Direct创建配置类 /** * 消息队列配置类 * @author GMaya * @dateTime 2020/4/9 16:02 */ @Configuration public class MQConfig { public static final String QUEUE = \"hello-top-gmaya\"; /** * Direct Exchange（交换机最简单的方式） */ @Bean public Queue queue() { // durable = true 开启持久化 return new Queue(QUEUE, true); } } 创建生产者用来提供消息产生的 /** * 消息队列生产者,比如订单下完,需要生产一个邮件推送的消息 * @author GMaya * @dateTime 2020/4/9 15:42 */ @Component @Slf4j public class Sender { @Autowired private AmqpTemplate amqpTemplate; public void sender(String msg) { log.info(\"生产者生产消息。。。。。{}\", msg); // 向key = MQConfig.QUEUE 中发送消息 this.amqpTemplate.convertAndSend(MQConfig.QUEUE, msg); } } 创建消费者用来消费消息的 /** * 消息列队 , 消费者, 用来消费队列中的消息 * @author GMaya * @dateTime 2020/4/9 15:58 */ @Component @Slf4j public class Receiver { // 接收key = MQConfig.QUEUE 中消息 @RabbitListener(queues = MQConfig.QUEUE) public void receiver(String msg) { log.info(\"消费者消费。。。。。。{}\", msg); } } 测试写个hello，然后调用。 @RestController @Slf4j public class HelloController { @Autowired private Sender sender; @RequestMapping(\"hello\") public String getHello(@RequestParam String msg) { log.info(\"hello自己的业务。。。\"); sender.sender(msg); return \"hello rabbitmq!!!\"; } } 浏览器访问 http://localhost:8088/hello?msg=你好啊 topic修改配置类 /** * 消息队列配置类 * @author GMaya * @dateTime 2020/4/9 16:02 */ @Configuration public class MQConfig { public static final String QUEUE = \"hello-top-gmaya\"; public static final String QUEUE_A = \"topic-a\"; public static final String QUEUE_B = \"topic-b\"; public static final String ROUTING_KEYA = \"topic.gmaya\"; public static final String ROUTING_KEYB = \"topic.#\"; public static final String TOPIC_EXCHANGE = \"TOPIC_EXCHANGE\"; // topic 交换机名称 // Direct Exchange（交换机最简单的方式）----begin @Bean public Queue queue() { // durable = true 开启持久化 return new Queue(QUEUE, true); } // Direct Exchange（交换机最简单的方式）----end // Topic Exchange 可以用通配符 ---begin @Bean public Queue topicQueueA() { return new Queue(QUEUE_A, true); } @Bean public Queue topicQueueB() { return new Queue(QUEUE_B, true); } // 定义topic交换机 @Bean public TopicExchange topicExchange(){ return new TopicExchange(TOPIC_EXCHANGE); } // routing_key为topic.gmaya,就是完全匹配 // 其实就相当于，你发送的时候 routing_key 和我的一样， 我就进入我的队列中，不然就不进 @Bean public Binding topicBindingA(){ return BindingBuilder.bind(topicQueueA()).to(topicExchange()).with(ROUTING_KEYA); } // routing_key为topic.#,就是模糊匹配 // 其实就相当于，你发送的时候 routing_key 满足我的routing_key， 就进入我的队列中，不然就不进 @Bean public Binding topicBindingB(){ return BindingBuilder.bind(topicQueueB()).to(topicExchange()).with(ROUTING_KEYB); } // Topic Exchange 可以用通配符 ---begin } 修改生产者/** * 消息队列生产者,比如订单下完,需要生产一个邮件推送的消息 * @author GMaya * @dateTime 2020/4/9 15:42 */ @Component @Slf4j public class Sender { @Autowired private AmqpTemplate amqpTemplate; public void sender(String msg) { log.info(\"生产者生产消息。。。。。{}\", msg); // 向key = MQConfig.QUEUE 中发送消息 this.amqpTemplate.convertAndSend(MQConfig.QUEUE, msg); } public void topicSender(String msg) { log.info(\"topic生产者生产消息。。。。。{}\", msg); // 交换机中发送消息 // 这次是完全匹配， 结果应该a和b都能接收到 // 因为b是模糊， 此条件满足模糊的条件，所有b会接收 this.amqpTemplate.convertAndSend(MQConfig.TOPIC_EXCHANGE,MQConfig.ROUTING_KEYA, msg + \"完全匹配\"); // 这次是模糊匹配，不满足a的条件，所以只有b模糊可以接收 // this.amqpTemplate.convertAndSend(MQConfig.TOPIC_EXCHANGE,\"topic.xixihaha\", msg + \"模糊匹配\"); // 这次是routing_key 不满足a，也不满足b。所以两个都接收不到。 // this.amqpTemplate.convertAndSend(MQConfig.TOPIC_EXCHANGE,\"gmaya.top\", msg + \"不存在\"); } } 修改消费者/** * 消息列队 , 消费者, 用来消费队列中的消息 * @author GMaya * @dateTime 2020/4/9 15:58 */ @Component @Slf4j public class Receiver { // 接收key = MQConfig.QUEUE 中消息 @RabbitListener(queues = MQConfig.QUEUE) public void receiver(String msg) { log.info(\"消费者消费。。。。。。{}\", msg); } @RabbitListener(queues = MQConfig.QUEUE_A) public void receiverA(String msg) { log.info(\"QUEUE_A消费者消费。。。。。。{}\", msg); } @RabbitListener(queues = MQConfig.QUEUE_B) public void receiverB(String msg) { log.info(\"QUEUE_B消费者消费。。。。。。{}\", msg); } } 测试完全匹配模糊匹配 Fanout修改配置类在topic配置类基础上新增代码 // Fanout Exchange 消息广播的模式 ---begin // 定义Fanout交换机名称 // 不管路由键或者是路由模式，会把消息发给绑定给它的全部队列，如果配置了routing_key会被忽略。 @Bean public FanoutExchange fanoutExchange(){ return new FanoutExchange(FANOUT_EXCHANGE); } @Bean public Binding fanoutBindingA(){ return BindingBuilder.bind(topicQueueA()).to(fanoutExchange()); } @Bean public Binding fanoutBindingB(){ return BindingBuilder.bind(topicQueueB()).to(fanoutExchange()); } // Fanout Exchange 消息广播的模式 ---end 消费者不需要更改（因为是使用的topic中的两个，已经写过了） 修改生产者 /** * fanout 模式 * @param msg */ public void fanoutSender(String msg) { log.info(\"fanout生产者生产消息。。。。。{}\", msg); // 会把消息发送给 所有绑定到此交换机的全部列队；routing_key会被忽略。 this.amqpTemplate.convertAndSend(MQConfig.FANOUT_EXCHANGE, \"\", msg); } 测试@RestController @Slf4j public class HelloController { @Autowired private Sender sender; @RequestMapping(\"hello\") public String getHello(@RequestParam String msg) { log.info(\"hello自己的业务。。。\"); // sender.sender(msg); // sender.topicSender(msg); sender.fanoutSender(msg); return \"hello rabbitmq!!!\"; } } 延迟队列（死信队列）模拟订单创建完，如果30分钟后还没有支付，则取消订单。要实现的办法很多， 使用RabbitMQ延迟队列是其中一个办法！ 使用死信队列大致思路： 订单MQ配置类 /** * 订单业务配置 * 死信对列使用的 orderTTLExchange，orderTTLQueue，orderTTLKey * 正常对列使用的 orderExchange，orderQueue，orderKey * 在orderTTLQueue中设置，当消息设置的时间到了，消失了，那么我就去调用orderQueue，orderKey，通知它。 * 其实没有对应的rderTTLQueue消费类，肯定到期就没了，然后就实现了一定时间后把消息传给某个队列。 * @author GMaya * @dateTime 2020/4/10 15:37 */ @Configuration public class OrderMQConfig { /** * 创建延迟队列(死信对列)交换机orderTTLExchange * @return */ @Bean public DirectExchange orderTTLExchange() { return new DirectExchange(\"orderTTLExchange\", true, false); } /** * 创建实际消费交换机orderExchange * @return */ @Bean public DirectExchange orderExchange() { return new DirectExchange(\"orderExchange\", true, false); } /** * 创建延迟队列(死信对列)orderTTLQueue * @return */ @Bean public Queue orderTTLQueue() { Map&lt;String, Object> map = new HashMap&lt;>(); map.put(\"x-dead-letter-exchange\", \"orderExchange\"); // 到期后转发的交换机 map.put(\"x-dead-letter-routing-key\", \"orderKey\"); // 到期后转发的路由key return QueueBuilder.durable(\"orderTTLQueue\").withArguments(map).build(); } /** * 创建实际orderQueue * @return */ @Bean public Queue orderQueue() { return new Queue(\"orderQueue\", true); } /** * 将（延迟队列orderTTLQueue）和（交换机orderTTLExchange）绑定 * @return */ @Bean public Binding orderTTLBinding() { return BindingBuilder.bind(orderTTLQueue()).to(orderTTLExchange()).with(\"orderTTLKey\"); } /** * 将（延迟队列orderQueue）和（交换机orderExchange）绑定 * @return */ @Bean public Binding orderBinding() { return BindingBuilder.bind(orderQueue()).to(orderExchange()).with(\"orderKey\"); } } 订单生产者/** * 订单消息生产着 * @author GMaya * @dateTime 2020/4/10 15:48 */ @Slf4j @Component public class OrderSender { @Autowired private AmqpTemplate amqpTemplate; public void sendMsg(String msg) { log.info(\"生产者===订单号：{}\", msg); // 这里把消息生产出来，传到TTL的队列中去，那么到期还没有被消费，就认为死信息，就会调用设置好的队列了 amqpTemplate.convertAndSend(\"orderTTLExchange\", \"orderTTLKey\", msg, new MessagePostProcessor() { @Override public Message postProcessMessage(Message message) throws AmqpException { // 设置失效时间，毫秒，一般订单设置30分钟,30*60*1000 // 过期直接转发到指定的路由 // 由于模拟设置20秒 MessageProperties messageProperties = message.getMessageProperties(); messageProperties.setContentEncoding(\"utf-8\"); messageProperties.setExpiration(\"20000\"); return message; } }); } } 订单消费者/** * 订单消息消费者 * 主要用于检查订单状态是否支付 * @author GMaya * @dateTime 2020/4/10 15:54 */ @Slf4j @Component public class OrderReceiver { // 这里只是消费了orderQueue。没有人去消费orderTTLQueue.一旦写了TTL那么就是实时消费了，不叫死信了。 @RabbitListener(queues = \"orderQueue\") public void orderMsg(String msg) throws IOException { log.info(\"消费者===订单号：{}\", msg); // TODO 处理判断此订单状态是否支付，如果没有支付则取消订单！ } } 调用测试/** * * @author GMaya * @dateTime 2020/4/9 15:37 */ @RestController @Slf4j public class OrderController { @Autowired private OrderSender orderSender; @RequestMapping(\"order\") public String getOrder(@RequestParam String msg) { log.info(\"Order创建成功。。。\"); orderSender.sendMsg(msg); return \"order创建成功！\"; } } 模拟20秒后进行实际调用处理将20秒换成30分钟，实现超过30分钟还未支付的订单处理上述代码全部上传到gitee。项目地址：https://gitee.com/GMaya/springboot-rabbitmq欢迎star","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gmaya.top/categories/SpringBoot/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://gmaya.top/tags/RabbitMQ/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gmaya.top/tags/SpringBoot/"}]},{"title":"Linux下安装RabbitMQ入门级别","slug":"20200409","date":"2020-04-09T12:18:12.000Z","updated":"2020-05-16T05:00:53.063Z","comments":true,"path":"2020/20200409/","link":"","permalink":"https://gmaya.top/2020/20200409/","excerpt":"","text":"Linux安装RabbitMQ安装erlang环境 由于RabbitMQ是基于Erlang（面向高并发的语言）语言开发，所以在安装RabbitMQ之前，需要先安装Erlang。 yum -y install epel-release yum -y update yum -y install erlang socat 查看erlang版本 erl -version 安装RabbitMQ下载RabbitMQ wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-3.6.15-1.el7.noarch.rpm 提示: 如果你没有wget,请先安装wget yum -y install wget 安装RabbitMQ yum install rabbitmq-server-3.6.15-1.el7.noarch.rpm 配置添加开机启动RabbitMQ服务 systemctl enable rabbitmq-server.service 启动web控制台 rabbitmq-plugins enable rabbitmq_management 防火墙开放15672和5672端口 /sbin/iptables -I INPUT -p tcp --dport 15672 -j ACCEPT /sbin/iptables -I INPUT -p tcp --dport 5672 -j ACCEPT 默认网页guest用户是不允许访问的，需要修改一下权限 修改配置文件 vi /usr/lib/rabbitmq/lib/rabbitmq_server-3.6.5/ebin/rabbit.app 启动, 停止 , 重启 service rabbitmq-server start service rabbitmq-server stop service rabbitmq-server restart 注：有可能起不来 。。。 进入 cd /usr/lib/rabbitmq/bin 启动 加 -detached 后台启动 ./rabbitmq-server start -detached 浏览器输入ip:15672输入guest guest 服气，各种版本不一致问题，提不起来，项目访问不到，端口不通，这个安装感觉比oracle都麻烦！！！","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://gmaya.top/tags/RabbitMQ/"},{"name":"Linux","slug":"Linux","permalink":"https://gmaya.top/tags/Linux/"}]},{"title":"Java中的多重for循环怎么跳出最外层","slug":"20200408","date":"2020-04-08T06:01:12.000Z","updated":"2020-05-16T05:00:05.407Z","comments":true,"path":"2020/20200408/","link":"","permalink":"https://gmaya.top/2020/20200408/","excerpt":"","text":"笔记记录不加任何代码,最里层的break,只能跳出最内层的循环, 如果想要跳出最外面的,就要新增一个标记.在想要跳出循环的地方加标记,然后break标记. 同理换成continue 标记; 效果一样. /** * 测试多重for循环 break */ public static void forTest() { int a = 3; tab: for (int i = 0; i &lt; a; i++) { System.out.println(\"我是i....\" + i); for (int j = 0; j &lt; a; j++) { System.out.println(\"我是j....\" + j); if (j == 1) { break tab; } } } }","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"循环","slug":"循环","permalink":"https://gmaya.top/tags/%E5%BE%AA%E7%8E%AF/"}]},{"title":"Python一键获取所有高清无码王者皮肤图片","slug":"20200406","date":"2020-04-06T08:20:33.000Z","updated":"2020-05-16T04:58:59.895Z","comments":true,"path":"2020/20200406/","link":"","permalink":"https://gmaya.top/2020/20200406/","excerpt":"","text":"前言突然看到王者荣耀每个英雄的图片很不错,所有就一张一张保存了. 分析1.打开王者荣耀官方英雄列表2.分析接口3.实际操作4.调整代码5.开始启动程序 开始其中中间发现,获取的页面中并没有直接找到图片的地址, 所以先拿到皮肤名称拼接成图片链接,然后真正拿到高清无码皮肤图片 # -*- coding: gbk -*-# # ------------------------------------------------------- # Name: WZRYImpageDemo # Description: 多线程获取王者荣耀所有英雄高清皮肤 # Author: GMaya # Date: 2020/4/6 14:56 # ------------------------------------------------------- import threading import requests, json, re # 创建会话对象 session = requests.session() headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36', 'origin': 'https://pvp.qq.com' } # 获取所有英雄列表 def get_hero_list(): # 请求接口 result = session.get('https://pvp.qq.com/web201605/js/herolist.json', headers=headers) # 解析json结果 resJson = json.loads(result.text) for res in resJson: # 英雄名称 cname = res.get('cname') # 英雄详情页id ename = res.get('ename') # 多线程获取详细页 myThread(cname, ename).start() # 获取英雄详情页面信息 def get_hero_tail(ename, cname): # 请求接口 result = session.get('https://pvp.qq.com/web201605/herodetail/%s.shtml' % ename, headers=headers) result.encoding = 'gbk' zz = \"\"\"&lt;ul class=\"pic-pf-list pic-pf-list3\" data-imgname=\"(.*?)\">\"\"\" # 由于爬取的页面和网页上看到的实际效果不一致,所以这里拿取所有皮肤名称,直接去皮肤服务器拿取 res_tail = re.findall(zz, result.text)[0] res_tail_list = res_tail.split('|') count = 1 for tail in res_tail_list: imgName = cname + '-' + tail # 下载具体图片 get_hero_skin(ename, count, imgName) count += 1 # 保存下载图片 def get_hero_skin(ename, i, imgName): url = \"http://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/%s/%s-bigskin-%d.jpg\" % (ename, ename, i) # 拼接最后的图片地址,然后保存 res_skin = session.get(url) file = 'D:\\download\\爬虫图片目录\\王者荣耀图片\\%s.jpg' % imgName # wb : 以二进制格式打开一个文件只用于写入。一般用于非文本文件如图片等。 with open(file, 'wb') as f: f.write(res_skin.content) # 创建线程类 class myThread(threading.Thread): def __init__(self, cname, ename): threading.Thread.__init__(self) self.cname = cname self.ename = ename def run(self): print(\"开始线程：\" + self.cname) try: get_hero_tail(self.ename, self.cname) except Exception: print('线程出现意外....' + self.name) print(\"退出线程：\" + self.cname) if __name__ == '__main__': # 开始 get_hero_list() 也没有具体的计算, 这三百多个将近四百个, 其中包含英雄本身的图片,所以真正的皮肤应该是200多个.应该是全了.说实话,300kb左右的图片, 已经很清楚了","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"Python听说网易云音乐的评论很有意思","slug":"20200405","date":"2020-04-05T15:22:33.000Z","updated":"2020-08-12T08:05:47.386Z","comments":true,"path":"2020/20200405/","link":"","permalink":"https://gmaya.top/2020/20200405/","excerpt":"","text":"前言听歌哪个平台好不知道， 但是要说评论，那就是网易云音乐。看看云音乐热歌榜所有歌曲的评论吧。 分析其实重点是分析各个页面和接口， 只要分析到位，没有拿不到的信息。 代码其中headers和data 的信息自己从浏览器拿一下就好了. # -*- coding: utf-8 -*-# # ------------------------------------------------------- # Name: Music163 # Description: 获取网易云音乐精彩评论 # Author: GMaya # Date: 2020/4/5 21:45 # ------------------------------------------------------- import requests, re, json, datetime # 1.创建会话对象 session = requests.session() headers = { 'User-Agent': 'xxx'} data = { \"params\": \"xxx\" , \"encSecKey\": \"xxx\"} # 文件保存位置 file = 'D:\\download\\music163.txt' # 根据歌曲id 获取所有精彩评论 def get_music_comment(songid): url = \"https://music.163.com/weapi/v1/resource/comments/R_SO_4_%d?csrf_token=\" % songid result = session.post(url, headers=headers, data=data) res = json.loads(result.text) hotComments = res.get('hotComments') for hot in hotComments: # 热评内容 content = hot.get('content').replace('\\n', '') # 昵称 nickname = hot.get('user').get('nickname') # 评论时间 hottime = int(hot.get('time')) / 1000 hottime = datetime.datetime.fromtimestamp(hottime).strftime('%Y-%m-%d %H:%M:%S') # 点赞人数 likedCount = hot.get('likedCount') consequence = '[昵称:]%s,[评论时间:]%s,[点赞人数:]%d,[热评内容:]%s' % (nickname, hottime, likedCount, content) # 保存评论信息 with open(file, 'a+', encoding='utf-8') as f: f.write(consequence + '\\n') # 加\\n换行显示 # 获取热歌排行榜所有歌曲id def get_song(): url = \"https://music.163.com/m/discover/toplist?id=3778678\" result = session.get(url, headers=headers) res = \"\"\"&lt;ul class=\"f-hide\">(.*?)&lt;/ul>\"\"\" resul = re.findall(res, result.text) print(resul) for ress in resul: res2 = \"\"\"&lt;li>&lt;a href=\"\\/song\\?id=(.*?)\">(.*?)&lt;/a>&lt;/li>\"\"\" resul2 = re.findall(res2, ress) # print(resul2) for r in resul2: songid = int(r[0]) songname = r[1] print(\"开始下载[%s]歌曲评论\" % songname) consequence = '===========歌曲名称: %s===============' % songname with open(file, 'a+', encoding='utf-8') as f: f.write(consequence + '\\n') # 加\\n换行显示 # 开始下载评论 get_music_comment(songid) if __name__ == '__main__': get_song() 最后文件是保存到本地磁盘, 还是数据库,根据自己的方式决定 思路:我之前写过获取qq音乐分享链接可以根据歌名 获得到歌曲id等链接, 这个网易云根据歌名获取歌曲id也不难,这个时候应该可以做一个对接qq或者微信的机器人, 用户在qq群输入歌名,直接返回最热的一条评论!","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"Python一键珍藏我的动漫妹子们","slug":"20200404","date":"2020-04-04T10:02:31.000Z","updated":"2020-05-16T04:58:05.343Z","comments":true,"path":"2020/20200404/","link":"","permalink":"https://gmaya.top/2020/20200404/","excerpt":"","text":"前言获取点动漫图片,还要一张一张保存,这多难受.话不多说,练练手 分析获取图片步骤: 找到图片来源网站 分析网站页面数据结构 分析下一页结果 分析数据处理 保存 本次找到的图片来源是http://pic.netbian.com/4kdongman/index.html都是高清4k动漫大妹子 然后F12找到图片位置的标签等,然后使用正则匹配 点击下一个看到,后缀 index_2 index_3 所以,下一页数据只需要下划线加本页页数即可 开鲁代码 展示真正的技术# -*- coding=gbk -*- # @Author: GMaya # 爬取动漫图片,嘎嘎 import requests, time, re # 1.创建会话对象 session = requests.session() # 获取目标网页结果 def get_result(url): # 请求接口 result = session.get(url) # 编码格式 result.encoding = 'gbk' \"\"\" 正则表达式,匹配对应数据 注意:href=\"(.*?)\" , 如果 .*? 被括号包裹, 则输出括号内的内容 如果没有括号, 则输出整个匹配的内容 \"\"\" res = \"\"\"&lt;a href=\"(.*?)\" target=\"_blank\">&lt;img src=\".*?\" alt=\".*?\" />&lt;b>.*?&lt;/b>&lt;/a>\"\"\" # 匹配符合正则内容 return re.findall(res, result.text) # 解析详情页数据 def two_result(contents): for con in contents: # 去第详情页,获取清晰度比较高的图片 result2 = session.get('http://pic.netbian.com/' + con) result2.encoding = 'gbk' res2 = \"\"\"&lt;a href=\"\" id=\"img\">&lt;img src=\"(.*?)\" data-pic=\".*?\" alt=\"(.*?)\" title=\".*?\">&lt;/a>\"\"\" # 匹配符合正则内容 contents2 = re.findall(res2, result2.text) # 真正处理数据并保存 save_result(contents2) # 真正处理数据并保存 def save_result(contents2): for con2 in contents2: path = con2[0] # 图片真正的高清图片地址 name = con2[1] # 图片名称 # 拼接最后的图片地址,然后保存 result3 = session.get('http://pic.netbian.com/' + path) file = 'D:\\download\\dongmanImg\\%s.jpg' % name # wb : 以二进制格式打开一个文件只用于写入。一般用于非文本文件如图片等。 with open(file, 'wb') as f: f.write(result3.content) # 安全点就等待0.5秒再次爬取 time.sleep(0.5) if __name__ == '__main__': print('下载动漫图片开始--------------') # 爬取前一百页内容 pageno = 100 # 1.请求网页,获取结果 for i in range(1, pageno): print('解析处理第%d页数据 请稍等...' % i) # 第一个和其他页有点不同,所以分开处理 url = '' if i == 1: url = \"http://pic.netbian.com/4kdongman/index.html\" else: url = \"http://pic.netbian.com/4kdongman/index_%d.html\" % i # 得到网页响应结果 result = get_result(url) # 处理详情页数据 two_result(result) print('第%d页数据共%d个,处理完毕.' % (i, len(result))) # 每次请求间隔0.5秒 time.sleep(0.5) print('下载动漫图片结束--------------') 喝陪咖啡,静等一会 说明: 只是用作技术练习.请勿恶意使用 多线程爬取上面单线程爬取了两杯咖啡时间, 到30页的时候,我停止了,太慢了,咋办,多线程搞上! # -*- coding=gbk -*- # @Author: GMaya # 爬取动漫图片,多线程例子 import requests, time, re import threading # 1.创建会话对象 session = requests.session() # 获取目标网页结果 def get_result(url): # 请求接口 result = session.get(url) # 编码格式 result.encoding = 'gbk' \"\"\" 正则表达式,匹配对应数据 注意:href=\"(.*?)\" , 如果 .*? 被括号包裹, 则输出括号内的内容 如果没有括号, 则输出整个匹配的内容 \"\"\" res = \"\"\"&lt;a href=\"(.*?)\" target=\"_blank\">&lt;img src=\".*?\" alt=\".*?\" />&lt;b>.*?&lt;/b>&lt;/a>\"\"\" # 匹配符合正则内容 return re.findall(res, result.text) # 解析详情页数据 def two_result(contents): for con in contents: # 去第详情页,获取清晰度比较高的图片 result2 = session.get('http://pic.netbian.com/' + con) result2.encoding = 'gbk' res2 = \"\"\"&lt;a href=\"\" id=\"img\">&lt;img src=\"(.*?)\" data-pic=\".*?\" alt=\"(.*?)\" title=\".*?\">&lt;/a>\"\"\" # 匹配符合正则内容 contents2 = re.findall(res2, result2.text) # 真正处理数据并保存 save_result(contents2) # 真正处理数据并保存 def save_result(contents2): for con2 in contents2: path = con2[0] # 图片真正的高清图片地址 name = con2[1] # 图片名称 # 拼接最后的图片地址,然后保存 result3 = session.get('http://pic.netbian.com/' + path) file = 'D:\\download\\dongmanThreadImg\\%s.jpg' % name # wb : 以二进制格式打开一个文件只用于写入。一般用于非文本文件如图片等。 with open(file, 'wb') as f: f.write(result3.content) # 安全点就等待0.5秒再次爬取 time.sleep(0.5) # 创建线程类 class myThread(threading.Thread): def __init__(self, threadID, name, url): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.url = url def run(self): print(\"开始线程：\" + self.name) try: # 得到网页响应结果 result = get_result(self.url) # 处理详情页数据 two_result(result) except Exception: print('线程出现意外....' + self.name) print(\"退出线程：\" + self.name) if __name__ == '__main__': print('下载动漫图片开始--------------') # 爬取前一百页内容 pageno = 100 # 1.请求网页,获取结果 for i in range(1, pageno): print('解析处理第%d页数据 请稍等...' % i) # 第一个和其他页有点不同,所以分开处理 url = '' if i == 1: url = \"http://pic.netbian.com/4kdongman/index.html\" else: url = \"http://pic.netbian.com/4kdongman/index_%d.html\" % i # 开启线程,爬取几页就开启几个线程,量力而行 myThread(i, \"thread-\" + str(i), url).start() # 每次请求间隔0.5秒 time.sleep(0.5) print('下载动漫图片结束--------------') 秒开100线程,每个线程独自爬取,这还没反应过来, 就结束了,我只能说好快 提示:最好还是把代码try 一下, 如果中间有意外,可以忽略,","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"Python的第一个web项目","slug":"20200403-2","date":"2020-04-03T13:15:12.000Z","updated":"2020-08-12T08:05:54.172Z","comments":true,"path":"2020/20200403-2/","link":"","permalink":"https://gmaya.top/2020/20200403-2/","excerpt":"","text":"前言来一个hello python 吧 创建项目项目名称: pythonwebdemo应用名称: webdemo 完成之后,二话不多说,启动!!! 然后点击默认地址 ,访问 成功! hello页面urls.py 文件修改页面请求hello路径, 找到views中的hello接口 from django.contrib import admin from django.urls import path from webdemo import views urlpatterns = [ path('admin/', admin.site.urls), path(r'hello/', views.hello) ] views.py 文件修改 from django.shortcuts import render # Create your views here. # 将请求定位到hello.html中 def hello(request): return render(request, 'hello.html') templates 文件夹下新增hello.heml页面 &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;title>hello&lt;/title> &lt;/head> &lt;body> &lt;h1>Hello Pyhton!!!&lt;/h1> &lt;h2>Hello Pyhton!!!&lt;/h2> &lt;h3>Hello Pyhton!!!&lt;/h3> &lt;h4>Hello Pyhton!!!&lt;/h4> &lt;h5>Hello Pyhton!!!&lt;/h5> &lt;/body> &lt;/html> 启动,访问http://127.0.0.1:8000/hello/ 加上数据库操作settings.py 文件修改数据库默认的sqlite,其实这个数据库也不错,简单的保存也不需要密码啥的,就是性能不中当初在写epl插件的时候,一般都是使用的sqlite,客户不需要安装数据库,直接本地生成了一个.db文件,简单了好多 下面时区和语言在下面搜索一下即可 DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'python', 'USER': 'root', 'PASSWORD': 'root', 'HOST': '127.0.0.1', 'PORT': 3306 } } LANGUAGE_CODE = 'zh-hans' TIME_ZONE = 'Asia/Shanghai' urls.py 文件修改 path(r'helloMysql/', views.helloMysql) views.py 文件修改简简单单来个返回结果封装 def helloMysql(request): data = {} # '-createtime' 前面加-为倒序 userList = models.User.objects.all().order_by('-createtime').values() data['code'] = 200 # 方式一 # data['result'] = json.loads(serializers.serialize('json', userList)) # 方式二 data['result'] = list(userList) return JsonResponse(data, safe=False, json_dumps_params={'ensure_ascii': False}) 项目路径找到models.py,修改 from django.db import models # Create your models here. class User(models.Model): id = models.AutoField(primary_key=True) name = models.CharField(max_length=32) age = models.IntegerField() sex = models.IntegerField() createtime = models.DateTimeField() # 指定数据库表名称 class Meta(): db_table = \"tb_user\" 启动!!! 访问 http://127.0.0.1:8000/helloMysql/查看结果","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"Python连接Mysql数据库Demo","slug":"20200403","date":"2020-04-03T05:29:41.000Z","updated":"2020-08-12T08:06:00.056Z","comments":true,"path":"2020/20200403/","link":"","permalink":"https://gmaya.top/2020/20200403/","excerpt":"","text":"前言这个必须要了解一下,老保存到本地根本不是办法简单的查询,新增,先了解基本写法 创建数据库就算使用java也不会在代码创建数据库啥啥的,所以这里都不去了解怎么在代码创建数据库,表之类的直接手动去创建 DROP TABLE IF EXISTS `tb_user`; CREATE TABLE `tb_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键', `name` varchar(32) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT '名字', `age` int(3) NULL DEFAULT NULL COMMENT '年龄', `sex` int(1) NULL DEFAULT 0 COMMENT '性别,0未知,1男,2女', `createtime` datetime NOT NULL COMMENT '创建时间', PRIMARY KEY (`id`) USING BTREE ) ENGINE = InnoDB AUTO_INCREMENT = 6 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '用户测试表' ROW_FORMAT = Compact; 简简单单搞点数据 INSERT INTO `tb_user` VALUES (1, '张三', 12, 1, '2020-04-03 11:03:03'); INSERT INTO `tb_user` VALUES (2, '李四', 34, 0, '2020-04-03 11:03:13'); INSERT INTO `tb_user` VALUES (3, '王五', 45, 2, '2020-04-03 11:03:24'); INSERT INTO `tb_user` VALUES (5, '小红', 12, 1, '2020-04-03 13:39:44'); 编写代码 # @Author: GMaya import pymysql import datetime # 打开数据库连接 db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='python', charset='utf8') # 查询方法 def seleteUser(): # 使用cursor方法创建一个游标 cursor = db.cursor() sql = \"select * from tb_user\" cursor.execute(sql) data = cursor.fetchall() for d in data: id = d[0] name = d[1] age = d[2] sex = d[3] createtime = d[4] print(id, name, age, sex, createtime) cursor.close() db.close() return data def insertUser(name, age, sex, createTime): # 使用cursor方法创建一个游标 cursor = db.cursor() sql = \"insert into tb_user(name,age,sex,createtime) values (%s,%s,%s,%s) \" try: # 执行sql语句;使用构造参数防止sql注入! row = cursor.execute(sql, (name, age, sex, createTime)) print(\"影响条数:%s\" % row) # 提交到数据库执行 db.commit() except: # 发生错误时回滚 db.rollback() # 关闭 cursor.close() db.close() if __name__ == '__main__': # 查询 # data = seleteUser() # 新增 insertUser('小红', 12, 1, datetime.datetime.now()) 总结datetime 模块 获取当前时间年月日时分秒now(…)：返回当前日期时间的datetime对象 datetime.datetime.now() pymysql模块获取新增主键id cursor.lastrowid 获取查询全部结果 cursor.fetchall() 获取查询结果第一条 cursor.fetchone()","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"Python爬取全球疫情Demo","slug":"20200402-2","date":"2020-04-02T14:09:11.000Z","updated":"2020-05-16T04:55:46.946Z","comments":true,"path":"2020/20200402-2/","link":"","permalink":"https://gmaya.top/2020/20200402-2/","excerpt":"","text":"前言仅仅学习而已,不做其他非法操作.习惯用小demo来熟练各种方法与技巧 分析进入腾讯新闻找到疫情入口https://news.qq.com/zt2020/page/feiyan.htm#/global然后F12分析接口.这对于java老鸟来说,分分钟找到. https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist 然后分析json,取什么值, 用什么. 撸代码装工具由于是从0开始学的python,所以什么也没有,只有一个python环境,所以直接来到jb家jetbrains,撸一个pycharm, 用过idea的都知道,直接和idea安装方式一样,一毛一样!!! hello进来不多说, 一个hello例子必须要有 # @Author: GMaya print('hello word!') 右键执行 导包想要访问互联网, 你不得一个requests请求么?刚好,导入这个在python安装目录 –&gt; Scripts 包下,进入cmd输入 pip install requests 等待安装完成!然后在ide里面设置引入 访问接口# @Author: GMaya import requests # 创建会话对象 session = requests.session() # 请求接口 result = session.get('https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist') # 打印结果 print(result.text) 握草,真是方便快捷狠啊 走一步学一部,看下这个requests包是啥 Requests为Python处理了所有HTTP/1.1操作， 与Web服务的无缝集成。不需要为URL手动添加查询字符串或POST数据进行表单处理。基于urllib3, 能自动处理Keep-alive和HTTP连接池。 此处 自行百度学习了小半个小时,以后用到复杂的再说. 处理结果# @Author: GMaya import requests, json, jsonpath # 创建会话对象 session = requests.session() # 请求接口 result = session.get('https://api.inews.qq.com/newsqa/v1/automation/foreign/country/ranklist') # 打印结果 print(result.text) # 解析json结果 resJson = json.loads(result.text) data = jsonpath.jsonpath(resJson, '$.data.*') for d in data: res = '日期:' + d['date'] + '--' + d['continent'] + '--' + d['name'] + '--' + '新增确诊:' + str( d['confirmAdd']) + '累计确诊:' + str(d['confirm']) + '治愈:' + str(d['heal']) + '死亡:' + str(d['dead']) # 保存数据到我的d盘 file = 'D:\\download\\global-yq.txt' with open(file, 'a+',encoding='utf-8') as f: f.write(res + '\\n') # 加\\n换行显示 结果已经保存到本地文本里面了.有不懂的命令直接文档里面找,还是很容易的文档连接","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"Python你好!","slug":"20200402","date":"2020-04-02T11:32:11.000Z","updated":"2020-05-16T04:55:00.398Z","comments":true,"path":"2020/20200402/","link":"","permalink":"https://gmaya.top/2020/20200402/","excerpt":"","text":"前言这年头,谁不是熟悉好几个语言!HTML是不是语言! 超文本标记语言&lt;/邪笑&gt; 下载百度搜索Python进入官网https://www.python.org/ 安装双击打开安装包, 注意选择第二个,自定义安装, 谁吧我的软件安装到c盘,我跟谁急下面还有一个add python 勾上! 这样就不需要配置环境变量了.其他的都默认,go!完成! 测试cmd进入,输入python输入 print(\"hello python!!\") 退出按ctrl+z,回车","categories":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://gmaya.top/tags/Python/"}]},{"title":"定时任务@Scheduled入门","slug":"20200330","date":"2020-03-30T06:12:11.000Z","updated":"2020-05-16T04:54:21.925Z","comments":true,"path":"2020/20200330/","link":"","permalink":"https://gmaya.top/2020/20200330/","excerpt":"","text":"一个最简单的例子:启动类添加注解 @EnableScheduling // 开启定时任务 编写单线程demo cron 表达式/** * cron 表达式 * 每2秒执行一次 * @throws InterruptedException */ @Scheduled(cron = \"0/2 * * * * *\") public void test() throws InterruptedException { // 经过测试,使用cron表达式,定时任务第二次会等待第一次执行完毕再开始! Thread.sleep(5000L); log.info(\"定时任务测试cron:\" + new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date())); } fixedDelay /** * fixedDelay: * 第一次执行完毕才会执行第二次，时间间隔变为了7秒 * @throws InterruptedException */ @Scheduled(fixedDelay = 2000L) public void test2() throws InterruptedException { Thread.sleep(5000L); log.info(\"定时任务测试fixedDelay:\" + new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date())); } fixedRate /** * fixedRate: * 每隔2秒就会执行, 但是因为单线程,所以在5秒后会输出,间隔就是5秒 * @throws InterruptedException */ @Scheduled(fixedRate = 2000L) public void test3() throws InterruptedException { Thread.sleep(5000L); log.info(\"定时任务测试fixedRate:\" + new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date())); } 如果是一起执行这三个定时任务,那么会一个一个的来, 因为只有一个线程. 多线程/** * * @author GMaya */ @Configuration @EnableAsync public class ScheduleConfig { @Bean public TaskScheduler taskScheduler() { ThreadPoolTaskScheduler taskScheduler = new ThreadPoolTaskScheduler(); taskScheduler.setPoolSize(50); // 设置线程池大小 return taskScheduler; } } 如果只是加这一个配置类, 确实是使用了多线程, 每个定时任务都互相不影响.但是一个线程第一次阻塞了,第二次就不行了,所以在定时任务上再加 @Async 就是说你这次失败了, 不要影响我下次的运行","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"定时任务","slug":"定时任务","permalink":"https://gmaya.top/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"Scheduled","slug":"Scheduled","permalink":"https://gmaya.top/tags/Scheduled/"}]},{"title":"Spring Cloud 整合最新版seata分布式事务 (六)","slug":"20200327-2","date":"2020-03-27T07:12:11.000Z","updated":"2020-05-16T04:53:55.845Z","comments":true,"path":"2020/20200327-2/","link":"","permalink":"https://gmaya.top/2020/20200327-2/","excerpt":"","text":"前言现在我有 订单系统, 支付系统.注册中心每个系统都是单独的,事务也都是本地事务,也是独立的.那么问题来了.我—&gt;订单系统–&gt;支付系统.–&gt;订单系统—&gt;我支付系统成功了 ,自己的事务也提交了.到订单系统结果失败了.(结果就是钱付了,订单还未支付.)这样就存在了问题呀.更多的问题也存在库存系统,加库存,改订单状态,支付等等,每个单独的模块事务怎么保持统一呢!! seata 介绍 Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案. seata各种官方例子连接:https://github.com/seata/seata-samplesseata服务端下载链接:https://github.com/seata/seata/releases 根据例子,我搞了好久好久好久好久好久才把例子跑起来.我也是服了,中间各种问题,网上的答案都尼玛一模一样!,人家都能成功,我就失败.郁闷死了. 思路1.下载官方的服务端seata-server2.下载官方客户端例子(我选的是springcloud-eureka-feign-mybatis-seata)3.修改配置.4.启动.5.哪里错改哪里!前4步也就半个小时,顶多俩小时,ok了.第5步鬼知道多长时间. 我结合官方demo,整合到自己的项目中去.以及中间遇到的问题记录 seata服务端下载seata服务端seata服务端下载链接:https://github.com/seata/seata/releases我选的是最新的v1.1.0版本,下载到本地即可 修改seata配置我这里就说win下怎么启动:首先修改目录下的conf中的配置file.conf其中我没有使用db方式, 使用的默认file方式,应该不需要修改 ## transaction log store, only used in seata-server store { ## store mode: file、db mode = \"file\" ## file store property file { ## store location dir dir = \"sessionStore\" # branch session size , if exceeded first try compress lockkey, still exceeded throws exceptions maxBranchSessionSize = 16384 # globe session size , if exceeded throws exceptions maxGlobalSessionSize = 512 # file buffer size , if exceeded allocate new buffer fileWriteBufferCacheSize = 16384 # when recover batch read size sessionReloadReadSize = 100 # async, sync flushDiskMode = async } ## database store property db { ## the implement of javax.sql.DataSource, such as DruidDataSource(druid)/BasicDataSource(dbcp) etc. datasource = \"dbcp\" ## mysql/oracle/h2/oceanbase etc. dbType = \"mysql\" driverClassName = \"com.mysql.jdbc.Driver\" url = \"jdbc:mysql://127.0.0.1:3306/seata\" user = \"mysql\" password = \"mysql\" minConn = 1 maxConn = 10 globalTable = \"global_table\" branchTable = \"branch_table\" lockTable = \"lock_table\" queryLimit = 100 } } registry.conf我使用的类型是eureka,配置上之前的注册中心地址 registry { # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = \"eureka\" eureka { serviceUrl = \"http://server01:8761/eureka/\" application = \"default\" weight = \"1\" } } config { # file、nacos 、apollo、zk、consul、etcd3 type = \"file\" file { name = \"file.conf\" } } 启动seata服务端修改完毕,首先启动你的注册中心eureka,然后启动这个seata服务端首先进入bin目录seata-server.bat 这个是win命令,在这个目录进入cmd, 将这个文件拖进窗口,输入 -h 127.0.0.1 -p 8091 -m file 最终效果:脚本参数:-p:指定启动seata server的端口号。-h:指定seata server所绑定的主机-m:指定事务日志、事务执行信息存储的方式，目前支持file（文件方式）、db（数据库方式) 这个地方我遇到的问题:第一个:我没有指定主机,然后和后面的配置里面对应不上,显示连接不上seata server服务. 导入jar &lt;!--seata--> &lt;dependency> &lt;groupId>com.alibaba.cloud&lt;/groupId> &lt;artifactId>spring-cloud-alibaba-seata&lt;/artifactId> &lt;version>2.1.0.RELEASE&lt;/version> &lt;/dependency> 修改自己项目的配置新增两个配置文件,每个模块里面都需要,先修改一个,然后拷贝过去就行了 file.conf 文件其中大部分是默认的,只需要修改一个地方 service { #transaction service group mapping vgroup_mapping.tx = \"default\" #only support when registry.type=file, please don't set multiple addresses default.grouplist = \"127.0.0.1:8091\" #degrade, current not support enableDegrade = false #disable seata disableGlobalTransaction = false } 注意,这个tx这个名字可以自己改,但是application.yml中一定要与之对应,往下看 完整的file.conf 文件 transport { # tcp udt unix-domain-socket type = \"TCP\" #NIO NATIVE server = \"NIO\" #enable heartbeat heartbeat = true # the client batch send request enable enableClientBatchSendRequest = true #thread factory for netty threadFactory { bossThreadPrefix = \"NettyBoss\" workerThreadPrefix = \"NettyServerNIOWorker\" serverExecutorThread-prefix = \"NettyServerBizHandler\" shareBossWorker = false clientSelectorThreadPrefix = \"NettyClientSelector\" clientSelectorThreadSize = 1 clientWorkerThreadPrefix = \"NettyClientWorkerThread\" # netty boss thread size,will not be used for UDT bossThreadSize = 1 #auto default pin or 8 workerThreadSize = \"default\" } shutdown { # when destroy server, wait seconds wait = 3 } serialization = \"seata\" compressor = \"none\" } service { #transaction service group mapping vgroup_mapping.tx = \"default\" #only support when registry.type=file, please don't set multiple addresses default.grouplist = \"127.0.0.1:8091\" #degrade, current not support enableDegrade = false #disable seata disableGlobalTransaction = false } client { rm { asyncCommitBufferLimit = 10000 lock { retryInterval = 10 retryTimes = 30 retryPolicyBranchRollbackOnConflict = true } reportRetryCount = 5 tableMetaCheckEnable = false reportSuccessEnable = false } tm { commitRetryCount = 5 rollbackRetryCount = 5 } undo { dataValidation = true logSerialization = \"jackson\" logTable = \"undo_log\" } log { exceptionRate = 100 } } registry.conf文件注册选择type = eureka,然后配置eureka地址即可 registry { # file 、nacos 、eureka、redis、zk、consul、etcd3、sofa type = \"eureka\" nacos { serverAddr = \"localhost\" namespace = \"\" cluster = \"default\" } eureka { serviceUrl = \"http://server01:8761/eureka/\" application = \"default\" weight = \"1\" } redis { serverAddr = \"localhost:6379\" db = \"0\" password = \"\" cluster = \"default\" timeout = \"0\" } zk { cluster = \"default\" serverAddr = \"127.0.0.1:2181\" session.timeout = 6000 connect.timeout = 2000 username = \"\" password = \"\" } consul { cluster = \"default\" serverAddr = \"127.0.0.1:8500\" } etcd3 { cluster = \"default\" serverAddr = \"http://localhost:2379\" } sofa { serverAddr = \"127.0.0.1:9603\" application = \"default\" region = \"DEFAULT_ZONE\" datacenter = \"DefaultDataCenter\" cluster = \"default\" group = \"SEATA_GROUP\" addressWaitTime = \"3000\" } file { name = \"file.conf\" } } config { # file、nacos 、apollo、zk、consul、etcd3、springCloudConfig type = \"file\" nacos { serverAddr = \"localhost\" namespace = \"\" group = \"SEATA_GROUP\" } consul { serverAddr = \"127.0.0.1:8500\" } apollo { app.id = \"seata-server\" apollo.meta = \"http://192.168.1.204:8801\" namespace = \"application\" } zk { serverAddr = \"127.0.0.1:2181\" session.timeout = 6000 connect.timeout = 2000 username = \"\" password = \"\" } etcd3 { serverAddr = \"http://localhost:2379\" } file { name = \"file.conf\" } } 修改application.yml第一个方式:注意,这个tx-service-group: 后面的tx一定要和上面配置中的tx一样.一定一定要一样 spring: cloud: alibaba: seata: tx-service-group: tx 第二个方式: application.yml这个文件不指定tx-service-group,那么file.conf这个文件的vgroup_mapping.tx就必须修改为seata默认的形式在这个源码中可以看到,如果你没有定义,那么你这个file.conf配置中必须设置为applicationName + “-fescar-service-group“举例:我这个项目就必须设置为order-fescar-service-group spring: application: name: order 都是坑踩出来的………….. 新增数据源配置一开始我最下面的mybatis的配置没有注释,就会发生添加语句执行找不到mapper注释掉就可以了. package com.gmaya.order.config; import com.alibaba.druid.pool.DruidDataSource; import io.seata.rm.datasource.DataSourceProxy; import javax.sql.DataSource; import org.apache.ibatis.session.SqlSessionFactory; import org.mybatis.spring.SqlSessionFactoryBean; import org.mybatis.spring.transaction.SpringManagedTransactionFactory; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Primary; import org.springframework.core.io.support.PathMatchingResourcePatternResolver; /** * 数据源代理 * @author GMaya */ @Configuration public class DataSourceConfiguration { @Bean @ConfigurationProperties(prefix = \"spring.datasource\") public DataSource druidDataSource(){ DruidDataSource druidDataSource = new DruidDataSource(); return druidDataSource; } @Primary @Bean(\"dataSource\") public DataSourceProxy dataSource(DataSource druidDataSource){ return new DataSourceProxy(druidDataSource); } /* @Bean public SqlSessionFactory sqlSessionFactory(DataSourceProxy dataSourceProxy)throws Exception{ SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSourceProxy); sqlSessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(\"classpath*:/mapper/*.xml\")); sqlSessionFactoryBean.setTransactionFactory(new SpringManagedTransactionFactory()); return sqlSessionFactoryBean.getObject(); }*/ } 修改启动类@SpringBootApplication(exclude = DataSourceAutoConfiguration.class) 修改完将以上操作在pay中复制. 数据库新增表因为我是模拟的, 所以,只要是操作数据库就行了,就没弄多个库,在一个数据库,一个表tb_user此时在加一个seata需要的表undo_log -- ---------------------------- -- Table structure for undo_log -- ---------------------------- DROP TABLE IF EXISTS `undo_log`; CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `context` varchar(128) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime NOT NULL, `log_modified` datetime NOT NULL, `ext` varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE, UNIQUE INDEX `ux_undo_log`(`xid`, `branch_id`) USING BTREE ) ENGINE = InnoDB AUTO_INCREMENT = 7 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact; -- ---------------------------- -- Records of undo_log -- ---------------------------- SET FOREIGN_KEY_CHECKS = 1; 测试order中调用pay创建一条数据,然后自己在创建一条语句.在order方法上加上注解 @GlobalTransactional 启动order 启动pay,测试! pay日志:添加数据成功没错,但是结果回滚了order日志:最终数据库还是没有添加进去 再测:调用订单–&gt;先本地添加数据, 然后在调用支付,让支付失败,看订单是否回滚测试结果还是回滚.自己一步一步一步一步敲出来的,希望我下次不要遇到这些问题.","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"seata","slug":"seata","permalink":"https://gmaya.top/tags/seata/"}]},{"title":"Spring Cloud 整合mybatis-plus (五)","slug":"20200327","date":"2020-03-27T04:26:43.000Z","updated":"2020-05-16T04:51:16.330Z","comments":true,"path":"2020/20200327/","link":"","permalink":"https://gmaya.top/2020/20200327/","excerpt":"","text":"前言其实这个整合,和spring 或者 spring boot 整合一样的其实都是一键生成的,自己把模板配好就行,从controller到service,dao,实体,增删改查我只是拷贝过来几个测试这个项目之前刚接触mybatis-plus的时候也写过自动生成的文章 引入jar &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>3.0.7.1&lt;/version> &lt;exclusions> &lt;exclusion> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-generator&lt;/artifactId> &lt;/exclusion> &lt;/exclusions> &lt;/dependency> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;optional>true&lt;/optional> &lt;/dependency> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;/dependency> 修改配置文件我mysql数据库8.0以上的,如果你的是5.+的版本,自己该数据源即可 spring: application: name: order datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/gmaya?allowMultiQueries=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=GMT%2B8 username: root password: admin #mybatis-plus mybatis-plus: mapper-locations: classpath:/mapper/*.xml typeAliasesPackage: com.gmaya.order.entity global-config: db-config: id-type: AUTO field-strategy: NOT_NULL column-underline: true logic-delete-value: 1 # 数据库删除字段,1代表删除 logic-not-delete-value: 0 # 0代表没删除 banner: false 新增配置类package com.gmaya.order.config; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import com.baomidou.mybatisplus.core.injector.ISqlInjector; import com.baomidou.mybatisplus.extension.injector.LogicSqlInjector; import com.baomidou.mybatisplus.extension.plugins.OptimisticLockerInterceptor; import com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor; /** * mybatis-plus配置 * * @author GMaya */ @Configuration public class MybatisPlusConfig { /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } /** * 控制逻辑删除 * @return */ @Bean public ISqlInjector sqlInjector() { return new LogicSqlInjector(); } /** * 控制版本号 * @return */ @Bean public OptimisticLockerInterceptor optimisticLockerInterceptor() { return new OptimisticLockerInterceptor(); } } 实体类package com.gmaya.order.entity; import java.io.Serializable; import com.baomidou.mybatisplus.annotation.TableId; import com.baomidou.mybatisplus.annotation.TableLogic; import com.baomidou.mybatisplus.annotation.TableName; import com.baomidou.mybatisplus.annotation.Version; import lombok.Data; /** * DO * * @author GMaya */ @Data @TableName(\"tb_user\") public class UserEntity implements Serializable { private static final long serialVersionUID = 1L; /** * 用户id */ @TableId(value = \"id\") private Integer id; /** * 用户名称 */ private String name; /** * 用户年龄 */ private Integer age; /** * 是否删除。0未删除，1删除 */ @TableLogic private Integer isDtl; /** * 版本号 */ @Version private Integer version; } daopackage com.gmaya.order.dao; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.gmaya.order.entity.UserEntity; import org.apache.ibatis.annotations.Mapper; import org.springframework.stereotype.Component; @Mapper public interface UserDao extends BaseMapper&lt;UserEntity> { } servicepackage com.gmaya.order.service; import com.alibaba.fastjson.JSONObject; import com.baomidou.mybatisplus.extension.service.IService; import com.gmaya.order.entity.UserEntity; /** * 服务类 * @author GMaya */ public interface UserService extends IService&lt;UserEntity> { int add(JSONObject jsonObject); } implpackage com.gmaya.order.service.impl; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import com.gmaya.order.dao.UserDao; import com.gmaya.order.entity.UserEntity; import com.gmaya.order.service.UserService; import lombok.extern.slf4j.Slf4j; /** * * @author GMaya */ @Service(\"userService\") @Slf4j public class UserServiceImpl extends ServiceImpl&lt;UserDao, UserEntity> implements UserService { @Override public int add(JSONObject jsonObject) { log.info(\"log...订单开始......\"); UserEntity userEntity = new UserEntity(); userEntity.setAge(1); userEntity.setIsDtl(0); userEntity.setVersion(1); userEntity.setName(\"订单模拟\"); int insert = baseMapper.insert(userEntity); log.info(\"log...订单结束......\"); return insert; } }","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"Mybatis-Plus","slug":"Mybatis-Plus","permalink":"https://gmaya.top/tags/Mybatis-Plus/"}]},{"title":"使用redis实现序列号工具类","slug":"20200323","date":"2020-03-23T10:43:11.000Z","updated":"2020-05-16T04:50:33.330Z","comments":true,"path":"2020/20200323/","link":"","permalink":"https://gmaya.top/2020/20200323/","excerpt":"","text":"前言项目中难免会带有字符的序列号,有的是每月从1开始, 有的是每天等等.以前也使用过存表的方式,每次获取之后加1.这次又了解了一个.记录一下redis为单线程，不存在线程安全问题. 序列号工具类 package com.gmaya.EurekaClient.util; import java.text.SimpleDateFormat; import java.util.Date; import java.util.concurrent.TimeUnit; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.support.atomic.RedisAtomicLong; import org.springframework.stereotype.Service; /** * 序列号工具类 * @author GMaya */ @Service public class SequenceUtil { @Autowired private RedisTemplate redisTemplate; /** * 获取序列号 * 例子：假如需要获取订单号NO20200320000001等数据，前缀NO，后面的每天从1开始自增 * 调用getCode(\"NO\") 返回 202003200000001,0的个数根据自己业务量设置 * 自己将返回的字符串再次拼接上NO即可！序列号 = 'NO'+getCode(\"NO\") * @param keyPrefix 关键词前缀 * @return 序列号 */ public String getCode(String keyPrefix) { // 当前日期加填充完0的值 return new SimpleDateFormat(\"yyyyMMdd\").format(new Date()) + getSequence(keyPrefix); } /** * 根据关键词前缀设置redis的key * @param keyPrefix * @return */ private String getSequence(String keyPrefix) { // 如果你的序列号需要每个月从1开始计算，那么时间就变为yyyyMM。 String currentDate = new SimpleDateFormat(\"yyyyMMdd\").format(new Date()); // 得到这个key的value值 Long num = getIncrementNum(\"seq:\" + keyPrefix + currentDate); // 填充这个值，一般情况下一天的序列号3位差不多，所以前面几位需要填充0 return getSequence(num); } /** * 查询redis中这个key，并且加1返回 * @param key * @return */ private Long getIncrementNum(String key) { // 不存在准备创建 键值对 RedisAtomicLong entityIdCounter = new RedisAtomicLong(key, redisTemplate.getConnectionFactory()); Long counter = entityIdCounter.incrementAndGet(); // 自增加1 if ((null == counter || counter.longValue() == 1)) { // 初始设置过期时间 entityIdCounter.expire(2, TimeUnit.DAYS);// 单位天 } return counter; } static final int DEFAULT_LENGTH = 8; // 填充0的总位数 /** * 将数值前面填充0 * @param seq * @return */ private static String getSequence(long seq) { String str = String.valueOf(seq); int len = str.length(); // 一般业务量不会超过8位数 if (len >= DEFAULT_LENGTH) { return str; } // 前面填充0，保证数值一直是8位数 int rest = DEFAULT_LENGTH - len; StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; rest; i++) { sb.append('0'); } sb.append(str); return sb.toString(); } } 测试 package com.gmaya.EurekaClient; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import com.gmaya.EurekaClient.util.SequenceUtil; @SpringBootTest class EurekaClientApplicationTests { @Autowired private SequenceUtil sequenceUtil; @Test public void getCode() { String no = \"NO\" + sequenceUtil.getCode(\"NO\"); String no2 = \"NO\" + sequenceUtil.getCode(\"NO\"); String no3 = \"NO\" + sequenceUtil.getCode(\"NO\"); String no4 = \"NO\" + sequenceUtil.getCode(\"NO\"); System.out.println(no); System.out.println(no2); System.out.println(no3); System.out.println(no4); } } 结果: NO2020032300000005 NO2020032300000006 NO2020032300000007 NO2020032300000008","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"},{"name":"redis","slug":"redis","permalink":"https://gmaya.top/tags/redis/"}]},{"title":"分享一下uTools工具","slug":"20200321","date":"2020-03-21T07:22:11.000Z","updated":"2020-05-16T04:50:04.043Z","comments":true,"path":"2020/20200321/","link":"","permalink":"https://gmaya.top/2020/20200321/","excerpt":"","text":"持续加班ing..分享 一个自己正在使用的工具uTools你的生产力工具集 uTools是一个极简、插件化、跨平台的现代桌面软件。通过自由选配丰富的插件，打造你得心应手的工具集合。当你熟悉它后，能够为你节约大量时间，让你可以更加专注地改变世界。 具体优点自己体会吧.","categories":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"IDEA常用插件","slug":"20200319","date":"2020-03-19T14:32:11.000Z","updated":"2020-05-16T04:49:21.922Z","comments":true,"path":"2020/20200319/","link":"","permalink":"https://gmaya.top/2020/20200319/","excerpt":"","text":"分享自己平时常用的idea插件 1.Mybatis Log Plugin 功能:把 mybatis 输出的sql日志还原成完整的sql语句。 将日志输出的sql语句中的问号 ? 替换成真正的参数值 直接在软件Plugins中下载即可,注意下载完要重启idea 在Tools中点击 即可进入 2.Rainbow Brackets 功能:彩虹方括号 让你的括号变得魔力转圈圈 3.Free Mybatis plugin 功能:在serviceImpl直接去到mapper.xml中的该方法 选中方法名,快捷键Ctrl + T","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"idea","slug":"idea","permalink":"https://gmaya.top/tags/idea/"},{"name":"插件","slug":"插件","permalink":"https://gmaya.top/tags/%E6%8F%92%E4%BB%B6/"}]},{"title":"Spring Cloud Hystrix断路器（四）","slug":"20200318","date":"2020-03-18T04:54:16.000Z","updated":"2020-05-16T04:48:34.866Z","comments":true,"path":"2020/20200318/","link":"","permalink":"https://gmaya.top/2020/20200318/","excerpt":"","text":"前言 在微服务场景中，通常会有很多层的服务调用。如果一个底层服务出现问题，故障会被向上传播给用户。我们需要一种机制，当底层服务不可用时，可以阻断故障的传播。这就是断路器的作用。他是系统服务稳定性的最后一重保障。 常常会遇到正在使用某个系统，点击到某一个功能的时候显示该模块正在升级或者请稍后查看等。怎么实现呢？？ 使用Feign整合Hystrix默认是不开启的，需要修改配置文件application.yml，添加下面的代码 feign: hystrix: #开启Hystrix熔断，默认false enabled: true 新增支付模块熔断器 /** * @ Description : 支付模块熔断器 * @ Author : GMaya * @ CreateDate : 2020/3/18 9:35 * @ Version : 1.0 */ @Component public class PayClientFallBack implements PayClient { @Override public String getPay() { return \"支付模块正在抢修，请稍后！\" +\" :feign演示\"; } } 修改PayClient接口 @FeignClient(name = \"pay\",fallback = PayClientFallBack.class) // 服务名称 启动服务端，启动pay客户端，启动order客户端。访问http://localhost:8082/getOrder然后把pay客户端关掉！ 使用RestTemplat整合Hystrixpom添加j依赖 &lt;!--熔断器--> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-hystrix&lt;/artifactId> &lt;/dependency> 启动类新增注解 @EnableCircuitBreaker 或者 @SpringBootApplication+@EnableDiscoveryClient+@EnableCircuitBreaker也可以使用@SpringCloudApplication 一个顶替仨 SpringCloudApplication注解 源码： 修改订单接口 /** * @ Description : 订单接口接口 * @ Author : GMaya * @ CreateDate : 2020/3/16 11:03 * @ Version : 1.0 */ @RestController public class OrderController { // @Autowired // private PayClient payClient; // 注入支付模块服务 @Autowired private RestTemplate restTemplate; @RequestMapping(\"/getOrder\") @HystrixCommand(fallbackMethod = \"getRestTemplateFallBack\") public String getOrder(){ String pay = restTemplate.getForObject(\"http://pay/getPay\", String.class); // String pay = payClient.getPay(); return pay; } private String getRestTemplateFallBack(){ return \"不好啦，机房着火了，restTemplate演示！\"; } } 当然，当接口多的时候，还可以改造 /** * @ Description : 订单接口接口 * @ Author : GMaya * @ CreateDate : 2020/3/16 11:03 * @ Version : 1.0 */ @RestController @DefaultProperties(defaultFallback = \"defaultFallback\") public class OrderController { // @Autowired // private PayClient payClient; // 注入支付模块服务 @Autowired private RestTemplate restTemplate; @RequestMapping(\"/getOrder\") // @HystrixCommand(fallbackMethod = \"getRestTemplateFallBack\") @HystrixCommand // 使用默认熔断，加一个注解就行 public String getOrder(){ String pay = restTemplate.getForObject(\"http://pay/getPay\", String.class); // String pay = payClient.getPay(); return pay; } private String getRestTemplateFallBack(){ return \"不好啦，机房着火了，restTemplate演示！\"; } private String defaultFallback(){ return \"默认熔断！\"; } } 重启查看 最后：额外配置，如需更多，请查看专业解释 如果设置和默认值一样，没必要写出来配置 feign: hystrix: #开启Hystrix熔断，默认false enabled: true hystrix: command: # 全局设置 default: execution: isolation: # 命令执行超时时间，默认1000ms，只在线程池隔离中有效。 thread: timeoutInMilliseconds: 1000 # 发生超时是是否中断，默认true，只在线程池隔离中有效。 interruptOnTimeout: true # 执行是否启用超时，默认启用true，只在线程池隔离中有效。 timeout: enabled: true","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"Hystrix","slug":"Hystrix","permalink":"https://gmaya.top/tags/Hystrix/"}]},{"title":"Short 类型直接和数值1做对比","slug":"20200317","date":"2020-03-17T04:34:16.000Z","updated":"2020-05-16T04:47:47.460Z","comments":true,"path":"2020/20200317/","link":"","permalink":"https://gmaya.top/2020/20200317/","excerpt":"","text":"今天写项目遇到了Short 返回1 需要做个判断，所以就多想了一下，研究了一下大声的告诉我，是true还是true还是true！！！ public static void main(String[] args) { Short a = 1; System.out.println(a.equals(1)); System.out.println(a==1); } 一开始觉得包装类型用==和equals应该都可以的，但是结果却是，false，true。第二个就不说了，都知道。所以就研究了一下第一个equals扒拉源码！ctrl + o 找到equals 方法！ /** * Compares this object to the specified object. The result is * {@code true} if and only if the argument is not * {@code null} and is a {@code Short} object that * contains the same {@code short} value as this object. * * @param obj the object to compare with * @return {@code true} if the objects are the same; * {@code false} otherwise. */ public boolean equals(Object obj) { if (obj instanceof Short) { return value == ((Short)obj).shortValue(); } return false; } 搜嘎，原来不属于Short类型的，就是false那这个直接凭空写的这个1是什么类型啊。有去查了一下java的基本类型默认值，什么四类八种的 int初值为0 long初值为0L byte初值为(byte)0 short初值为(short)0 所以1我觉得是int型的 同时又看了一下Short 和Integer一样，都有缓存的Short private static class ShortCache { private ShortCache(){} static final Short cache[] = new Short[-(-128) + 127 + 1]; static { for(int i = 0; i &lt; cache.length; i++) cache[i] = new Short((short)(i - 128)); } } Integer private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it. } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high >= 127; } private IntegerCache() {} }","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"Short","slug":"Short","permalink":"https://gmaya.top/tags/Short/"}]},{"title":"Spring Cloud Eureka服务通信Ribbon/Feign（三）","slug":"20200316-2","date":"2020-03-16T14:12:12.000Z","updated":"2020-05-16T04:46:25.265Z","comments":true,"path":"2020/20200316-2/","link":"","permalink":"https://gmaya.top/2020/20200316-2/","excerpt":"","text":"问题每一个客户端就是一个完整的项目，是一个模块，假如我有订单模块，支付模块，用户模块，等等，那么我这些模块之间怎么相互调用呢？采用HttpClient工具类访问？ Ribbon 和 Feign 简介Ribbon 简介 Ribbon 是 Netflix 发布的开源项目，主要功能是提供客户端的软件负载均衡算法，将Netflix的中间层服务连接在一起。Ribbon 客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出 Load Balancer 后面所有的机器，Ribbon 会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们也很容易使用Ribbon实现自定义的负载均衡算法。简单地说，Ribbon 是一个客户端负载均衡器。 Feign 简介 Feign 是一个声明式的 web service 客户端，它使得编写 web service 客户端更为容易。创建接口，为接口添加注解，即可使用Feign。Feign可以使用Feign注解或者JAX-RS注解，还支持热插拔的编码器和解码器。Spring Cloud 为Feign添加了Spring MVC的注解支持，并整合了Ribbon和Eureka来为使用 Feign 时提供负载均衡。 创建支付模块（客户端）起名pay项目，也可以去spring官网 https://start.spring.io/按照之前的办法创建再介绍一个方法，直接在idea中创建下一步填写组织和项目名称搜索添加下一步完成然后按照客户端的修改操作进行修改修改application.yml因为是小demo，所以就不使用集群了，电脑有点卡。真实情况每个服务端和客户端有可能都不在一个服务器上 spring: application: name: pay server: port: 8081 eureka: client: service-url: #注册中心路径，表示我们向这个注册中心注册服务，如果向多个注册中心注册，用“，”进行分隔 #defaultZone: http://server01:8761/eureka/,http://server02:8762/eureka/ defaultZone: http://server01:8761/eureka/ 修改启动项添加注解@EnableDiscoveryClient添加一个正常情况下的最简洁支付接口 /** * @ Description : 支付接口 * @ Author : GMaya * @ CreateDate : 2020/3/16 10:53 * @ Version : 1.0 */ @RestController public class PayController { @RequestMapping(\"/getPay\") public String getPay(){ // 返回支付信息 return \"this pay！！！\"; } } 创建订单模块（客户端）跟上面支付模块一模一样的操作。。。注意：创建订单最简洁订单的接口 /** * @ Description : 订单接口 * @ Author : GMaya * @ CreateDate : 2020/3/16 11:03 * @ Version : 1.0 */ @RestController public class OrderController { @RequestMapping(\"/getOrder\") public String getOrder(){ // 返回订单信息 return \"this order！！！\"; } } 此时两个正常的项目已经创建完毕，那么我订单模块怎么去调用支付模块呢？ 方式一：RestTemplate调用修改订单模块接口 @RequestMapping(\"/getOrder\") public String getOrder(){ RestTemplate restTemplate = new RestTemplate(); String forObject = restTemplate.getForObject(\"http://localhost:8081/getPay\", String.class); return forObject; } 启动服务端，启动支付端，启动订单端调用支付接口调用订单接口可以看出调用成功，但是缺点也很明显，必须要知道对方的url地址，对于多个实例那就难受了，所以是不可取的。 方式二：通过 @LoadBalanced添加RestTemplateConfig配置 /** * @ Description : RestTemplate配置类 * @ Author : GMaya * @ CreateDate : 2020/3/16 11:24 * @ Version : 1.0 */ @Component public class RestTemplateConfig { @Bean @LoadBalanced public RestTemplate restTemplate(){ return new RestTemplate(); } } 修改订单接口此时只需要填写调用的项目名就好了，跟ip无关。 /** * @ Description : 订单接口接口 * @ Author : GMaya * @ CreateDate : 2020/3/16 11:03 * @ Version : 1.0 */ @RestController public class OrderController { @Autowired private RestTemplate restTemplate; @RequestMapping(\"/getOrder\") public String getOrder(){ String forObject = restTemplate.getForObject(\"http://PAY/getPay\", String.class); return forObject; } } 重启订单项目查看 @LoadBalanced 源码注解源码注释中说，用来标记restTemplate使之配置使用LoadBalancerClient /** * Annotation to mark a RestTemplate or WebClient bean to be configured to use a * LoadBalancerClient. * @author Spencer Gibb */ @Target({ ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD }) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Qualifier public @interface LoadBalanced { } LoadBalancerClient则表示客户端负载均衡器。所以使用@LoadBalanced就代表使用ribbon实现客户端的负载均衡负载均衡的策略默认使用的是轮询算法 负载均衡是在同一个功能的微服务中根据不同的策略选择不同的微服务，因此这些微服务对外暴露的实例名称要相同（spring.application.name）ribbon是一个客户端的负载均衡，必须要连接eureka，才能在指定的微服务实例中按照策略选择 假设支付模块（pay）部署在两台或者多台服务器，只有端口号不一样，其他的都一样，那么我订单模块按照轮询算法去调用支付模块。 方式三：通过 Feign第一步：添加Feign依赖打开订单模块pom文件 &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-openfeign&lt;/artifactId> &lt;/dependency> 第二步：修改订单模块启动类OrderApplication添加注解@EnableFeignClients第三步：声明需要调用的接口创建PayClient接口 /** * @ Description : 支付接口声明 * @ Author : GMaya * @ CreateDate : 2020/3/16 14:28 * @ Version : 1.0 */ @FeignClient(name = \"pay\") // 服务名称 public interface PayClient { @RequestMapping(\"/getPay\") // 这里要和pay提供的接口一致 String getPay(); } 修改OrderController /** * @ Description : 订单接口接口 * @ Author : GMaya * @ CreateDate : 2020/3/16 11:03 * @ Version : 1.0 */ @RestController public class OrderController { @Autowired private PayClient payClient; // 注入支付模块服务 @RequestMapping(\"/getOrder\") public String getOrder(){ String pay = payClient.getPay(); return pay; } } 重启订单模块项目，一切正常！ 只有先入门，才能追查源码以及骚操作！","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"https://gmaya.top/tags/Eureka/"}]},{"title":"Spring Cloud Eureka高可用的服务注册中心（二）","slug":"20200316","date":"2020-03-16T04:52:12.000Z","updated":"2020-05-16T04:44:20.341Z","comments":true,"path":"2020/20200316/","link":"","permalink":"https://gmaya.top/2020/20200316/","excerpt":"","text":"集群原理图如果只有一个注册中心（服务端），一旦挂掉，就直接GG，在分布式系统中，任何地方存在单点故障，整个系统就不是高可用的。（自己手画的难看见谅） 创建服务端2在上一篇文章中写了最简单的单服务端，单客户端的例子。所以，在上一篇的基础上，继续新建一个服务端，起名字叫做服务端2登录spring官网 https://start.spring.io/保存到本地之后，导入项目服务器2（EurekaServerTwo）启动类添加注解 @EnableEurekaServer 修改EurekaServerTwo配置文件application.yml server: port: 8762 eureka: client: service-url: # #注册中心路径，表示我们向这个注册中心注册服务，如果向多个注册中心注册，用“，”进行分隔 # 向端口8761注册(两个服务端互相注册) defaultZone: http://server01:8761/eureka/ # 指示此实例是否应将其信息注册到eureka服务器以供其他服务发现，默认为false register-with-eureka: false instance: # 该服务实例所在主机名 hostname: server02 server: # 启用自我保护机制，默认为true enable-self-preservation: false spring: profiles: server02 修改服务器1EurekaServer修改EurekaServer项目的配置文件application.yml server: port: 8761 eureka: client: service-url: # 注册中心路径，表示我们向这个注册中心注册服务，如果向多个注册中心注册，用“，”进行分隔 # 向端口8762注册(两个服务端互相注册) defaultZone: http://server02:8762/eureka/ # 指示此实例是否应将其信息注册到eureka服务器以供其他服务发现，默认为false register-with-eureka: false instance: # 该服务实例所在主机名 hostname: server01 server: # 启用自我保护机制，默认为true enable-self-preservation: false spring: profiles: server01 修改电脑hosts文件修改host文件添加对application.yml配置文件中hostname的映射打开电脑C:\\Windows\\System32\\drivers\\etc找到hosts，在文件最后输入 127.0.0.1 server01 127.0.0.1 server02 注意使用管理员权限，不然没办法保存 启动查看效果修改启动配置项在服务器1配置 -Dspring.profiles.active=server01 在服务器2配置 -Dspring.profiles.active=server02 启动服务器1，启动服务器2，启动客户端如果在启动中报错，先则是因为服务器1向服务器2注册没有连接上，等你服务器2启动完毕就不会报错了。最终在浏览器查看效果http://localhost:8761/http://localhost:8762/ 停止服务器1此时检验效果，停止服务器1，即端口号为：8761的服务器稍微等待一会，因为有默认的失效时间。查看http://localhost:8762此时客户端已经消失，这是因为客户端没有配置服务器2的注册。 修改客户端修改EurekaClient项目的配置文件application.yml将两个注册中心全部配置上去 spring: application: name: eureka-client server: port: 8711 eureka: client: service-url: #注册中心路径，表示我们向这个注册中心注册服务，如果向多个注册中心注册，用“，”进行分隔 defaultZone: http://server01:8761/eureka/,http://server02:8762/eureka/ 将server02注册进来，重新启动服务端1，服务端2，客户端，再次演示上面断掉服务端1的操作这个时候server02就能一直显示客户端的注册 这样就算一个服务器挂掉，对整体也不会有影响了。","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"https://gmaya.top/tags/Eureka/"}]},{"title":"Spring Cloud Eureka最简单的入门（一）","slug":"20200315","date":"2020-03-15T13:33:45.000Z","updated":"2020-05-16T04:43:03.884Z","comments":true,"path":"2020/20200315/","link":"","permalink":"https://gmaya.top/2020/20200315/","excerpt":"","text":"Eureka介绍 Eureka是一个基于REST的服务，主要用于AWS云中的定位服务，以实现中间层服务器的负载平衡和故障转移，在 Spring Cloud 微服务架构中通常用作注册中心，我们称这个服务为 Eureka Server，还有一个与之交互的客户端称之为 Eureka Client 创建eureka服务端登录spring官网 https://start.spring.io/写好组名称，和项目名称,搜索eureka server，点击+，选择导出到本地。将下载好的项目导入idea。 然后在eureka server项目启动类添加注解 @EnableEurekaServer 修改配置application.yml文件其中defaultZone前后注意空格，而且这个单词是关联不出来的。 server: port: 8761 eureka: client: service-url: defaultZone: http://localhost:8761/eureka/ # 指示此实例是否应将其信息注册到eureka服务器以供其他服务发现，默认为false register-with-eureka: false server: # 启用自我保护机制，默认为true enable-self-preservation: false spring: application: # 微服务名称 name: eureka 启动localhost:8761显示页面即为成功！ 创建eureka客户端登录spring官网 https://start.spring.io/修改项目名称，搜索eureka discovery client 。点击+ ，保存到本地保存到本地之后，在上一个项目中打开选择eureka客户端导入。 修改eureka客户端启动类添加注解 @EnableDiscoveryClient pom文件添加 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 修改配置文件application.yml spring: application: name: eureka-client server: port: 8711 eureka: client: service-url: #注册中心路径，表示我们向这个注册中心注册服务，如果向多个注册中心注册，用“,”进行分隔 defaultZone: http://localhost:8761/eureka/ 启动服务端，启动客户端最简单的一个spring cloud + eureka 服务注册到此结束。全部是采用默认形式，最简单的一个项目搭建。","categories":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/categories/SpringCloud/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://gmaya.top/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"https://gmaya.top/tags/Eureka/"}]},{"title":"浅谈 MySQL 中优化 SQL 语句查询常用的 30 种方法","slug":"20200314","date":"2020-03-14T12:08:11.000Z","updated":"2020-05-16T04:41:55.626Z","comments":true,"path":"2020/20200314/","link":"","permalink":"https://gmaya.top/2020/20200314/","excerpt":"","text":"转载于网络，作者不详。 1、对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2、应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 3、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 4、应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 可以这样查询： select id from t where num=10 union all select id from t where num=20 5、下面的查询也将导致全表扫描： select id from t where name like '%abc%' 若要提高效率，可以考虑全文检索。 6、in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 7、如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num 可以改为强制查询使用索引： select id from t with(index(索引名)) where num=@num 8、应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2=100 应改为: select id from t where num=100*2 9、应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：name以abc开头的id select id from t where substring(name,1,3)='abc' ‘2005-11-30’生成的id select id from t where datediff(day,createdate,'2005-11-30')=0 应改为: select id from t where name like 'abc%' select id from t where createdate>='2005-11-30' and createdate&lt;'2005-12-1' 10、不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 11、在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12、不要写一些没有意义的查询，如需要生成一个空表结构： select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： create table #t(...) 13、很多时候用 exists 代替 in 是一个好的选择： select num from a where num in(select num from b) 用下面的语句替换： select num from a where exists(select 1 from b where num=a.num) 14、并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 15、索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 16、应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17、尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18、尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19、任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 20、尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21、避免频繁创建和删除临时表，以减少系统表资源的消耗。 22、临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23、在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 24、如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26、使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 27、与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28、在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 29、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 30、尽量避免大事务操作，提高系统并发能力。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://gmaya.top/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://gmaya.top/tags/MySQL/"},{"name":"转载","slug":"转载","permalink":"https://gmaya.top/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"idea新建springboot项目pom文件报错","slug":"20200313","date":"2020-03-13T05:01:11.000Z","updated":"2020-05-16T04:38:25.031Z","comments":true,"path":"2020/20200313/","link":"","permalink":"https://gmaya.top/2020/20200313/","excerpt":"","text":"前言之前也有过类似的情况，只不过都是把spring-boot-starter-parent版本号改成本地仓库已经有的，然后继续开发。今天想写个demo，就新建了一个，然后版本号不一致，就一直报错，所以找了一天问题，才解决。太可怕了 新建springboot项目的问题使用idea，一路next，到最后生成项目然后pom文件报错我特意换了一个全新的maven，发现是因为jar拉不下来，就配置了国内镜像找到maven ，conf 打开 settings.xml。在mirrors里面写入 &lt;!-- 阿里云镜像 --> &lt;mirror> &lt;id>alimaven&lt;/id> &lt;name>aliyun maven&lt;/name> &lt;url>http://maven.aliyun.com/nexus/content/groups/public/&lt;/url> &lt;mirrorOf>central&lt;/mirrorOf> &lt;/mirror> &lt;mirror> &lt;id>central&lt;/id> &lt;name>Maven Repository Switchboard&lt;/name> &lt;url>http://repo1.maven.org/maven2/&lt;/url> &lt;mirrorOf>central&lt;/mirrorOf>&lt;/mirror> &lt;mirror> &lt;id>repo2&lt;/id> &lt;mirrorOf>central&lt;/mirrorOf> &lt;name>Human Readable Name for this Mirror.&lt;/name> &lt;url>http://repo2.maven.org/maven2/&lt;/url> &lt;/mirror> &lt;mirror> &lt;id>ibiblio&lt;/id> &lt;mirrorOf>central&lt;/mirrorOf> &lt;name>Human Readable Name for this Mirror.&lt;/name> &lt;url>http://mirrors.ibiblio.org/pub/mirrors/maven2/&lt;/url> &lt;/mirror> &lt;!-- 中央仓库在中国的镜像 --> &lt;mirror> &lt;id>maven.net.cn&lt;/id> &lt;name>oneof the central mirrors in china&lt;/name> &lt;url>http://maven.net.cn/content/groups/public/&lt;/url> &lt;mirrorOf>central&lt;/mirrorOf> &lt;/mirror> 然后发现还是有点问题，有些拉不下来。继续设置idea，打开设置，搜索maven将上面的勾打上然后在VM Options下面输入 -Dmaven.wagon.http.ssl.insecure=true -Dmaven.wagon.http.ssl.allowall=true -Dmaven.wagon.http.ssl.ignore.validity.dates=true 然后点击最右面maven，重新安装一下（拉完之后再把勾去掉，还原）此时已经解决了我所有报错的问题，然后启动项目是启动不起来的，因为没有web继续在pom加入spring-boot-starter-web &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 启动类中加入hello代码校验启动，浏览器输入http://localhost:8080/hello哎。真tm菜","categories":[{"name":"idea","slug":"idea","permalink":"https://gmaya.top/categories/idea/"}],"tags":[{"name":"java","slug":"java","permalink":"https://gmaya.top/tags/java/"},{"name":"idea","slug":"idea","permalink":"https://gmaya.top/tags/idea/"}]},{"title":"hexo配置最新next主题的几个问题","slug":"20200312","date":"2020-03-12T05:01:11.000Z","updated":"2020-05-16T04:37:12.038Z","comments":true,"path":"2020/20200312/","link":"","permalink":"https://gmaya.top/2020/20200312/","excerpt":"","text":"前言截止到写这博客的时间，花费了一天左右的时间，将next主题美化了一下，以此记录。 模板我的博客使用next模板为Mist，打开主题配置文件 添加背景线条效果我是参考canvas-nest官方网站 创建文件在根目录的source 文件夹下创建_data文件夹，然后创建footer.swig注意不要将位置创建到主题里面去，然后将代码拷贝进去即可 &lt;script color=\"0,0,0\" opacity=\"0.5\" zIndex=\"-1\" count=\"99\" src=\"https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js\">&lt;/script> 其中color是颜色值，opacity是透明度，zIndex是现在层级，count是线条数量。 设置路径打开主题配置文件搜索 custom_file_path 将footer注释打开然后 hexo g ， hexo s ，启动看看效果 首页文章之间周围有边框刚开始这个首页文章之间没有边框，每个文章之间没有明显的间隔。效果：打开：\\themes\\next\\source\\css_common\\components\\post\\post.styl大概35行，找到motion.transition.post_block修改前： if (hexo-config('motion.transition.post_block')) { .post-block, .pagination, .comments { opacity: 0; } } 修改后： if (hexo-config('motion.transition.post_block')) { .post-block{ opacity: 0; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); } .pagination, .comments { opacity: 0; } } 然后 hexo g ， hexo s ，启动看看效果 修改文章之间的间隔默认的文章间距120px，有点大。 打开：\\themes\\next\\source\\css_schemes\\Mist_posts-expand.styl大概24行，找到.post-block:not修改前： .post-block:not(:first-child) { margin-top: 120px; } 将120px，修改为你想要的间距，我改成了20px修改后： .post-block:not(:first-child) { margin-top: 20px; } 评论数显示为英文问题我使用的Valine评论，但是文章标题下面显示的评论数为英文。打开：\\themes\\next\\languages\\zh-CN.yml在post下面新增： comments.valine: 评论数 注意空格，和下面保持对齐","categories":[{"name":"hexo","slug":"hexo","permalink":"https://gmaya.top/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://gmaya.top/tags/hexo/"}]},{"title":"获取qq音乐分享链接","slug":"0308","date":"2020-03-08T06:12:15.000Z","updated":"2020-05-16T04:35:44.077Z","comments":true,"path":"2020/0308/","link":"","permalink":"https://gmaya.top/2020/0308/","excerpt":"","text":"前言想要写个插件。支持qq，微信，在群里面点歌，这就要获取音乐接口的api 了。注：只允许个人学习使用。 分析通过浏览器分析音乐查询接口，分享接口，等等qq音乐搜索接口： https://c.y.qq.com/soso/fcgi-bin/client_search_cp?p=1&amp;n=1&amp;w=水手&amp;format=json&amp;inCharset=utf8&amp;outCharset=utf-8 其中：w=歌名，p=页数，n=条数，format=格式。剩下两个就是编码格式了 结果：得到以下几个参数：albumname：专辑albumid ： 专辑idalbummid : 专辑图片songname ：歌名singer[0].name:歌手名字songid ： 分享链接使用 音乐分享链接： https://i.y.qq.com/v8/playsong.html?songid= 例子： https://i.y.qq.com/v8/playsong.html?songid=4829638 获取专辑图片： https://y.gtimg.cn/music/photo_new/T002R300x300M000专辑图片.jpg 例子： https://y.gtimg.cn/music/photo_new/T002R300x300M000002MwneO44kDAi.jpg","categories":[{"name":"音乐","slug":"音乐","permalink":"https://gmaya.top/categories/%E9%9F%B3%E4%B9%90/"}],"tags":[{"name":"音乐链接","slug":"音乐链接","permalink":"https://gmaya.top/tags/%E9%9F%B3%E4%B9%90%E9%93%BE%E6%8E%A5/"}]},{"title":"易语言对接腾讯ai智能闲聊","slug":"0307","date":"2020-03-07T03:29:25.000Z","updated":"2020-05-16T04:34:25.638Z","comments":true,"path":"2020/0307/","link":"","permalink":"https://gmaya.top/2020/0307/","excerpt":"","text":"介绍 腾讯闲聊服务基于AI Lab领先的NLP引擎能力、数据运算能力和千亿级互联网语料数据的支持，同时集成了广泛的知识问答能力，可实现上百种自定义属性配置，以及男、女不同的语言风格及说话方式，从而让聊天变得更睿智、简单和有趣 基础闲聊接口提供基于文本的基础聊天能力，可以让您的应用快速拥有具备上下文语义理解的机器聊天功能。 注：同时我将此接口制作成插件，可以应用在QQ和微信上。再也不怕自己一个人没人聊天了。 准备首先在腾讯ai官网注册自己的账号，将智能闲聊接入。然后在控制台-应用管理，找到刚接入的智能闲聊，在应用信息里面得到APPID，APPKEY两个参数。 基础闲聊API地址： https://api.ai.qq.com/fcgi-bin/nlp/nlp_textchat入参： 编写代码getAI 方法 注意： #app_id 和 #app_key 就是上面申请得到的两个参数 .版本 2 .子程序 getAI, , 公开 .参数 msg, 文本型, , 消息内容 .局部变量 入参, 文本型 .局部变量 time_stamp, 文本型 .局部变量 nonce_str, 文本型 .局部变量 session, 文本型 .局部变量 question, 文本型 .局部变量 sign, 文本型 .局部变量 反参, 文本型 .局部变量 JSON, 类_json .局部变量 回复消息, 文本型 msg ＝ 删全部空 (msg) &#39; 限制问题字数不能超过250字节 ，官方要求：非空且长度上限300字节 .如果真 (取文本长度 (msg) ＞ 250) 返回 () .如果真结束 time_stamp ＝ 时间_取现行时间戳 (真) nonce_str ＝ 文本_取随机字符 (16) session ＝ nonce_str question ＝ 编码_URL编码 (编码_gb2312到utf8 (msg), 真, ) 入参 ＝ “app_id=” ＋ #app_id ＋ “&amp;nonce_str=” ＋ nonce_str ＋ “&amp;question=” ＋ question ＋ “&amp;session=” ＋ session ＋ “&amp;time_stamp=” ＋ time_stamp &#39; 入参一定要按照字典顺序排好，不然签名不对 sign ＝ getSign (入参) 入参 ＝ 入参 ＋ “&amp;sign=” ＋ sign 反参 ＝ 编码_utf8到gb2312 (网页_访问S (“https://api.ai.qq.com/fcgi-bin/nlp/nlp_textchat”, 1, 入参, , , , , , , , , , , , , , , , , , )) .如果真 (JSON.解析 (反参)) .如果真 (JSON.取通用属性 (“ret”, ) ＝ “0”) 回复消息 ＝ JSON.取通用属性 (“data.answer”, ) .如果真 (回复消息 ≠ “”) &#39; 得到智能ai的回答之后，做你想做的事情 .如果真结束 .如果真结束 getSign 方法 .版本 2 .子程序 getSign, 文本型 .参数 入参, 文本型 返回 (校验_取md5 (到字节集 (入参 ＋ “&amp;app_key=” ＋ #app_key), 真, )) 结束最后还可以在腾讯ai控制台中配置多重属性的闲聊画像，并且可配置的属性会持续增加，使闲聊的风格因您的喜好而不同。可以实时观看应用概况","categories":[{"name":"易语言","slug":"易语言","permalink":"https://gmaya.top/categories/%E6%98%93%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"易语言","slug":"易语言","permalink":"https://gmaya.top/tags/%E6%98%93%E8%AF%AD%E8%A8%80/"}]},{"title":"win10安装git","slug":"0306","date":"2020-03-06T09:57:08.000Z","updated":"2020-05-16T05:25:58.102Z","comments":true,"path":"2020/0306/","link":"","permalink":"https://gmaya.top/2020/0306/","excerpt":"","text":"下载git去git官网下载win版本64位安装全部下一步到结束 下载小乌龟去tortoriseGit官网下载64位小乌龟。然后下面还有一个中文语言包（不下载也行），Language Packs 下面的 Chinese, simplified先安装小乌龟，一路暴力下一步到结束。安装语言包，一路到结束。鼠标右键打开小乌龟设置，选择语言为中文，结束！ 生成SSH密钥鼠标右键 打开 Git Bash 输入下面的命令将邮箱换成你自己的邮箱、然后回车 ssh-keygen -o -t rsa -b 4096 -C \"email@example.com\" 然后出现提示 密钥保存的位置。记一下使用默认密钥地址，继续回车提示你对密钥设置密码，这里无需设置，直接按回车键就好，要按两次回车键。之后就会显示保存成功然后打开默认位置，看一下有没有两个文件 添加密钥到Git服务器登录进来之后点击右上角的图标，选择settings设置SSH密钥选择 SSH keys然后 new SSH keys将刚才生成的公钥放进去保存即可。 配置本地私钥在文件夹空白处右键选择TortoiseGit的设置(setting)选项一直点下一步到下面的这个页面，中间会让你输入你的名称和邮箱（名称随便写，邮箱还写你那个邮箱），然后点击 生成PuTTY密钥对选择你刚才的私钥然后点击Save private key 保存成PuTTY 格式的私钥保存成功后关闭窗口 TortoiseGit克隆项目加载私钥右键Git克隆弹出如下窗口结束。","categories":[{"name":"安裝","slug":"安裝","permalink":"https://gmaya.top/categories/%E5%AE%89%E8%A3%9D/"}],"tags":[{"name":"安装git","slug":"安装git","permalink":"https://gmaya.top/tags/%E5%AE%89%E8%A3%85git/"},{"name":"win10","slug":"win10","permalink":"https://gmaya.top/tags/win10/"}]},{"title":"酷q表情使用","slug":"0119","date":"2020-01-19T12:20:52.000Z","updated":"2020-05-16T04:31:40.936Z","comments":true,"path":"2020/0119/","link":"","permalink":"https://gmaya.top/2020/0119/","excerpt":"","text":"表情1在消息内添加 [CQ:face,id=XXX] 其中XXX为下面中的编号 例如你想添加微笑表情：[CQ:face,id=14][CQ:face,id=14][CQ:face,id=14] 结果就是三个微笑表情 表情2在消息内添加 [CQ:emoji,id=XXX] 其中XXX为下面中的编号 例如你想添加雪花表情：[CQ:emoji,id=10052]","categories":[{"name":"酷q","slug":"酷q","permalink":"https://gmaya.top/categories/%E9%85%B7q/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"},{"name":"酷q","slug":"酷q","permalink":"https://gmaya.top/tags/%E9%85%B7q/"}]},{"title":"Excel导出导入","slug":"0116","date":"2020-01-16T14:40:12.000Z","updated":"2020-05-16T04:31:16.808Z","comments":true,"path":"2020/0116/","link":"","permalink":"https://gmaya.top/2020/0116/","excerpt":"","text":"1. EasyExcel介绍1.1 为什么使用EasyExcel？ Java解析、生成Excel比较有名的框架有Apache poi、jxl。但他们都存在一个严重的问题就是非常的耗内存，poi有一套SAX模式的API可以一定程度的解决一些内存溢出的问题，但POI还是有一些缺陷，比如07版Excel解压缩以及解压后存储都是在内存中完成的，内存消耗依然很大。easyexcel重写了poi对07版Excel的解析，能够原本一个3M的excel用POI sax依然需要100M左右内存降低到几M，并且再大的excel不会出现内存溢出，03版依赖POI的sax模式。在上层做了模型转换的封装，让使用者更加简单方便 快速、简单避免OOM的java处理Excel工具，64M内存1分钟内读取75M(46W行25列)的Excel。 写的内容大致和阅读官方文档差不多 2. 开始使用2.1 引入jar&lt;!--easyexcel插件 --> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>easyexcel&lt;/artifactId> &lt;version>2.1.4&lt;/version> &lt;/dependency> &lt;!--lombok插件--> &lt;dependency> &lt;groupId>org.projectlombok&lt;/groupId> &lt;artifactId>lombok&lt;/artifactId> &lt;optional>true&lt;/optional> &lt;/dependency> 2.2 实体类根据自己的情况自定义表格导入导出实体类 package top.gmaya.demo.excelentity; import com.alibaba.excel.annotation.ExcelIgnore; import com.alibaba.excel.annotation.ExcelProperty; import com.alibaba.excel.annotation.format.DateTimeFormat; import com.alibaba.excel.annotation.write.style.ColumnWidth; import lombok.Data; import java.util.Date; /** * @ Description : 用户表格专用实体 * @ Author : GMaya * @ CreateDate : 2020/1/16 9:13 * @ Version : 1.0 */ @Data public class UserExcel{ /** 生成报表时忽略，不生成次字段 */ @ExcelIgnore private Long id; /** 定义表头名称和位置,0代表第一列 */ @ExcelProperty(value = \"用户名称\",index = 0) private String userName; @ExcelProperty(value = \"用户年龄\",index = 1) private int userAge; @ExcelProperty(value = \"用户昵称\",index = 2) private String userNick; @ExcelProperty(value = \"用户地址\",index = 3) private String userAddress; /** 指定列宽 */ @ColumnWidth(20) /** 转化时间 */ @DateTimeFormat(value = \"yyyy-MM-dd\") @ExcelProperty(value = \"用户生日\",index = 4) private Date userBrithday; } 2.3 ExcelUtil工具类根据实际情况封装成工具类，我就写了俩 package top.gmaya.demo.util; import com.alibaba.excel.EasyExcel; import com.alibaba.excel.read.listener.ReadListener; import com.alibaba.excel.support.ExcelTypeEnum; import org.springframework.web.multipart.MultipartFile; import javax.servlet.http.HttpServletResponse; import java.io.IOException; import java.net.URLEncoder; import java.util.List; /** * @ Description : excel工具类 * @ Author : GMaya * @ CreateDate : 2020/1/16 9:35 * @ Version : 1.0 */ public class ExcelUtil { /** * 导出文件 到浏览器 * @param response 响应请求 * @param excelName excel名称 * @param sheetName sheet页面名称 * @param clazz 要转换的实体类类型 * @param data 要导出的数据 * @throws Exception 异常 */ public static void export2Web(HttpServletResponse response, String excelName, String sheetName, Class clazz, List data) throws Exception { response.setContentType(\"application/vnd.ms-excel\"); response.setCharacterEncoding(\"utf-8\"); // 这里URLEncoder.encode可以防止中文乱码 excelName = URLEncoder.encode(excelName, \"UTF-8\"); response.setHeader(\"Content-disposition\", \"attachment;filename=\" + excelName + ExcelTypeEnum.XLSX.getValue()); EasyExcel.write(response.getOutputStream(), clazz).sheet(sheetName).doWrite(data); } /** * 读取Excel表格 * @param excel 文件 * @param head 实体类映射 * @param readListener 模板的读取类 * @throws Exception */ public static void readExcel(MultipartFile excel, Class head,ReadListener readListener) throws Exception{ EasyExcel.read(excel.getInputStream(),head,readListener).sheet().doRead(); } } 2.4 模板读取类创建模板读取类， 这个是表格导入的时候需要的，表格中的数据将在这个类里面单独处理，根据类里面设置的数值进行存库，也就是10w条数据，每当1000条就存一下表，方便内存回收。 不然一次性读10w，要占多少。。。实际情况将下面的构造方法注释打开，相当于你在controlle中调用，然后将Service传进来就ok了。 package top.gmaya.demo.excelentity; import com.alibaba.excel.context.AnalysisContext; import com.alibaba.excel.event.AnalysisEventListener; import lombok.extern.slf4j.Slf4j; import java.util.ArrayList; import java.util.List; /** * @ Description : 模板的读取类 * @ Author : GMaya * @ CreateDate : 2020/1/16 10:43 * @ Version : 1.0 */ // 有个很重要的点 UserListener 不能被spring管理，要每次读取excel都要new,然后里面用到spring可以构造方法传进去 @Slf4j public class UserListener extends AnalysisEventListener&lt;UserExcel> { /** * 每隔5条存储数据库，实际使用中可以3000条，然后清理list ，方便内存回收 */ private static final int BATCH_COUNT = 5; List&lt;UserExcel> list = new ArrayList&lt;>(); /** * 假设这个是一个DAO，当然有业务逻辑这个也可以是一个service。当然如果不用存储这个对象没用。 */ // private UserService userService; /* public UserListener(){ // 这里是demo，所以随便new一个。实际使用如果到了spring,请使用下面的有参构造函数 userService = new UserService(); }*/ /** * 如果使用了spring,请使用这个构造方法。每次创建Listener的时候需要把spring管理的类传进来 * * @param userService */ /* public UserListener(UserService userService) { this.userService = userService; }*/ /** * 这个每一条数据解析都会来调用 * @param data * @param context * */ @Override public void invoke(UserExcel data, AnalysisContext context) { log.info(\"解析到一条数据:{}\", data.getUserName()); list.add(data); if (list.size() >= BATCH_COUNT) { // 达到BATCH_COUNT了，需要去存储一次数据库，防止数据几万条数据在内存，容易OOM saveData(); // 存储完成清理 list list.clear(); } } @Override public void doAfterAllAnalysed(AnalysisContext context) { saveData(); log.info(\"所有数据解析完成！\"); } /** * 加上存储数据库 */ private void saveData() { log.info(\"{}条数据，开始存储数据库！\", list.size()); // userService.saveBatch(list); log.info(\"存储数据库成功！\"); } } 2.5 controller测试package top.gmaya.demo.controller; import com.alibaba.excel.EasyExcel; import com.alibaba.excel.ExcelReader; import lombok.extern.slf4j.Slf4j; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PostMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.multipart.MultipartFile; import top.gmaya.demo.entity.User; import top.gmaya.demo.excelentity.UserExcel; import top.gmaya.demo.excelentity.UserListener; import top.gmaya.demo.util.ExcelUtil; import javax.servlet.http.HttpServletResponse; import java.util.ArrayList; import java.util.Date; import java.util.List; /** * @ Description : 测试 * @ Author : GMaya * @ CreateDate : 2020/1/15 16:55 * @ Version : 1.0 */ @Slf4j @RestController public class DemoController { /** * 导出excel * @param response */ @GetMapping(\"/export2Web\") public void export2Web(HttpServletResponse response, String id) { // 模拟数据 List&lt;User> list = new ArrayList&lt;>(); for (int i = 0; i &lt; 10; i++) { User user = new User((long) i, \"张三\" + i, i, \"小三\" + i, \"杭州\" + i, new Date()); list.add(user); } try { ExcelUtil.export2Web(response, \"用户表\", \"用户信息\", UserExcel.class, list); } catch (Exception e) { log.error(\"报表导出异常:\", e); } } /** * 导入excel * @param file * @return */ @PostMapping(\"/exportImport\") public String exportImport(MultipartFile file) { try { ExcelUtil.readExcel(file, UserExcel.class, new UserListener()); } catch (Exception e) { e.printStackTrace(); } return \"succeed\"; } } 3. 测试3.1 导出测试启动项目，浏览器访问接口直接弹框提示保存位置。保存查看里面内容 3.2 导入测试使用postman测试文件导入key：Content-Typevalue：multipart/form-data","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"工具","slug":"工具","permalink":"https://gmaya.top/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"建站心得","slug":"0110","date":"2020-01-10T13:20:33.000Z","updated":"2020-05-16T04:29:44.264Z","comments":true,"path":"2020/0110/","link":"","permalink":"https://gmaya.top/2020/0110/","excerpt":"","text":"从偶然发现个人博客开始，就开始着手搭建一个自己的博客网站，也许写不出什么有技术含量的东西，但是也图个新鲜，练练手，记录一下自己的成长经历！但是就是这个随手开始弄得网站，前前后后花了将近一个月，SO,我要将自己的过程记录下来！ 1. 买服务器&lt;此步骤可以省略&gt;我是购买的腾讯云，首次大幅度优惠，99/年。当然，博客如果都是静态页面，也可以不需要服务器，直接部署到github，或者码云上，都是可以的（免费）。如果你只需要搭建博客不需要其他的，可以不买服务器，忽略即可。我是因为还要用服务器干点其他的。 2. 买域名&lt;此步骤可以省略&gt;我是在腾讯云打折5块钱买了一年的域名，虽然不买也行，用github自动生成的也中，但我不想要！ 2.1 备案使用国内服务器，域名不管是哪里的，都需要备案！反正我资料啥的一顿填，大概半个月备案成功！我刚开始买了之后大概一星期左右网站使用域名已经进不去了，一直让我去备案。 2.2 域名解析将域名解析到我的服务器 2.3 nginx代理我是使用的nginx代理的静态页面，然后将我生成好的博客页面直接放到服务器，配置好nginx的配置就ok，然后直接访问域名。当然也可以搞个tomcat放到里面。 2.4 SSL证书弄完之后我的访问域名是不安全的链接，所以我又免费申请了一个SSL证书，配置到nginx中，然后博客就可以https访问了。 3. hexo建站3.1 配置环境首先根据hexo官方网站将本地环境搭建起来，对于经常使用git提交拉取公司项目的人，很快都搞定了。 3.2 挑选模板直接在hexo官方网站挑选自己中意的博客模板，我默默挑了好多，中间改过好几次，总之挑一个自己喜欢的。 3.3 修改模板一般都是根据模板的博主介绍，一步一步一步一步一步一步修改成为自己的博客。总之我感觉这一步是最长的。 4. 写博客不积跬步，无以至千里；不积小流，无以成江海。在接下来的岁月写你想写的东西吧。","categories":[{"name":"随笔","slug":"随笔","permalink":"https://gmaya.top/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://gmaya.top/tags/%E9%9A%8F%E7%AC%94/"}]},{"title":"认识酷q","slug":"kuq-know","date":"2019-12-30T13:41:04.000Z","updated":"2020-05-16T05:22:11.421Z","comments":true,"path":"2019/kuq-know/","link":"","permalink":"https://gmaya.top/2019/kuq-know/","excerpt":"","text":"##酷q是干嘛的 酷Q Air 是一个轻巧、免费、高效的机器人核心，功能需要安装“应用”实现。登录酷Q，完成教程后，建议 访问 酷Q应用发布板块，下载安装您所需要的应用，创造让大家喜爱并且属于你自己的专属机器人。 官网：酷q社区 通俗的讲就是，打造一个属于自己的qq机器人，有点类似于现在的q群管家，qq小冰。但是你可以把他的功能再次扩大！（活跃群气氛必不可少）。主要有：入群欢迎，问答，便民（查天气，查快递等），各种小游戏娱乐，关键词禁言等等，需要看代码功底，当然你也可以去社区下载别人分享的插件，直接使用。 我也是在偶然的机会认识到了酷q，然后就爱不释手，从0开始学习了易语言，然后写出了第一个酷q插件猜拳签到游戏，当然我并不满足，然后又写出了查电影，问答，查运势，查天气，查新闻，随机笑话等，当然这些我并没有发布出来， 因为这些第三方接口我是免费用的，每天有限制几百条， 所以不适合分享，只是在自己的群里面使用而已。 如果你和我一样刚刚认识酷q，那么你不妨看一下这个快速入门使用sdk进行酷q开发。由于教程很多，我就没得写了，如果有不懂的可以留言评论或者到关于我页面查看联系方式。 但是我是java开发者，使用易语言难免有所不习惯，所以参考了南荒喵大佬的教程，使用支持java的sdk进行开发。 到了最后我在此demo上又加上mybatis-plus，springboot，连接数据库，等一系列操作。 有的时候思路是个好东西，当我弄完之后，我不知道写什么东西了，整天都是配环境，搭建环境，但是到最后写插件内容的时候，迷茫了，写什么呢，写什么好呢，写什么会有人用呢，看社区里面，那都是人家写出来的，你再写一遍干嘛呢。 好多东西都是 从简单，到复杂，到迷茫，到不知所措。（服务器自己买了一个，酷q是用的免费的） 然后我就认识了jsoup，百度百科是这样说的 jsoup 是一款Java 的HTML解析器，可直接解析某个URL地址、HTML文本内容。它提供了一套非常省力的API，可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。 然后我就又陷入了进去，忘记了游戏，忘记了时间。 希望前进的路上不再迷茫！","categories":[{"name":"酷q","slug":"酷q","permalink":"https://gmaya.top/categories/%E9%85%B7q/"}],"tags":[{"name":"酷q","slug":"酷q","permalink":"https://gmaya.top/tags/%E9%85%B7q/"}]},{"title":"idea在jdk1.8情况下使用反编译插件","slug":"idea-decompilation","date":"2019-12-30T05:41:01.000Z","updated":"2020-05-16T05:21:12.613Z","comments":true,"path":"2019/idea-decompilation/","link":"","permalink":"https://gmaya.top/2019/idea-decompilation/","excerpt":"","text":"前言使用JD-GUI在jdk1.8情况下失效，我也查找了好多办法，最后看到idea有默认的，只是没有打开，瞬间搞定，记录一下！ 没有反编译是显示compiled code public static void debug(java.lang.String... messages) { /* compiled code */ } public static void debug(java.lang.Object... messages) { /* compiled code */ } idea不需要安装任何插件， 比如JD-GUI（这个在jdk1.8还是打不开） 因为，安装idea默认有，只是没有打开罢了 打开setting –&gt;plugings 搜索 Java Bytecode Decompiler 勾选，然后重启idea （注意：你的其他反编译插件就不要开了，关掉） 重启之后会让你选择 是否同意使用此插件， 点击最左面accept，同意即可！","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"idea插件","slug":"idea插件","permalink":"https://gmaya.top/tags/idea%E6%8F%92%E4%BB%B6/"}]},{"title":"Java性能测试大比拼","slug":"property-compare","date":"2019-12-28T15:23:56.000Z","updated":"2020-05-16T05:24:07.422Z","comments":true,"path":"2019/property-compare/","link":"","permalink":"https://gmaya.top/2019/property-compare/","excerpt":"","text":"实体类转换哪个快？Map循环遍历哪个快？转换为字符串哪个快？测试，分析，看源码，看底层知识。 dao和vo实体转换比拼参与比拼的两个选手: org.springframework.beans.BeanUtils org.springframework.cglib.beans.BeanCopier 测试方式: public static void main(String[] args) { int leng = 10000000; Stu stu = new Stu(); stu.setStuId(\"id\"); stu.setStuName(\"name\"); stu.setStuAge(12); stu.setStuClassId(\"ClassId\"); StuVO stuVO = new StuVO(); long l = System.currentTimeMillis(); for (int i = 0; i &lt; leng; i++) { BeanCopier beanCopier = BeanCopier.create(stu.getClass(), stuVO.getClass(), false); beanCopier.copy(stu,stuVO,null); } System.out.println(\"beanCopier.copy:\"+(System.currentTimeMillis()-l)); l = System.currentTimeMillis(); for (int i = 0; i &lt; leng; i++) { BeanUtils.copyProperties(stu,stuVO); } System.out.println(\"BeanUtils.copyProperties:\"+(System.currentTimeMillis()-l)); } 比拼结果: 备注: 如果把BeanCopier.create放到循环外,效率还可以加快!!! 可能由于各方面原因,次数较少时测试不准 Map遍历几种方式比拼参与比拼的四个选手: keySet的for循环方式 keySet的iterator迭代器方式 entrySet的for循环方式 entrySet的iterator迭代器方式 测试方式: public static void main(String[] args) { Map&lt;String, String> map = new HashMap&lt;>(); for (int i = 0; i &lt; 1; i++) { map.put(i + \"\", i + \"AA\"); } keySet(map); keySetIterator(map); entrySet(map); entrySetIterator(map); } /** * keySet的for循环方式 * @param map */ public static void keySet(Map&lt;String, String> map){ long startTime = System.currentTimeMillis(); for (String key : map.keySet()) { String value = map.get(key); } long endTime = System.currentTimeMillis(); System.out.println(\"keySet运行时间\" + (endTime - startTime)); } /** * keySet的iterator迭代器方式 * @param map */ public static void keySetIterator(Map&lt;String, String> map){ long startTime = System.currentTimeMillis(); Iterator&lt;String> iterator = map.keySet().iterator(); while (iterator.hasNext()) { String key = iterator.next(); String value = map.get(key); } long endTime = System.currentTimeMillis(); System.out.println(\"keySetIterator运行时间\" + (endTime - startTime)); } /** * entrySet的for循环方式 * @param map */ public static void entrySet(Map&lt;String, String> map){ long startTime = System.currentTimeMillis(); for (Map.Entry&lt;String, String> entry : map.entrySet()) { String key = entry.getKey(); String value = entry.getValue(); } long endTime = System.currentTimeMillis(); System.out.println(\"entrySet运行时间\" + (endTime - startTime)); } /** * entrySet的iterator迭代器方式 * @param map */ public static void entrySetIterator(Map&lt;String, String> map){ long startTime = System.currentTimeMillis(); Iterator&lt;Map.Entry&lt;String, String>> iterator = map.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry&lt;String, String> entry = iterator.next(); String key = entry.getKey(); String value = entry.getValue(); } long endTime = System.currentTimeMillis(); System.out.println(\"entrySetIterator运行时间\" + (endTime - startTime)); } 比拼结果: 备注:可能由于各方面原因,次数较少时测试不准 基本数据类型转为字符串方法比拼参与比拼的三个选手: 基本数据类型.toString() String.valueOf(数据) 数据+”” 测试方法: public static void main(String[] args) { int end = 1000000000; Integer t = 1; long startTime = System.currentTimeMillis(); for (int i = 0; i &lt; end; i++){ String str = t.toString(); } System.out.println(\"Integer.toString()：\" + (System.currentTimeMillis() - startTime) + \"ms\"); startTime = System.currentTimeMillis(); for (int i = 0; i &lt; end; i++){ String str = String.valueOf(t); } System.out.println(\"String.valueOf()：\" + (System.currentTimeMillis() - startTime) + \"ms\"); startTime = System.currentTimeMillis(); for (int i = 0; i &lt; end; i++){ String str = t + \"\"; } System.out.println(\"t + '' :\" + (System.currentTimeMillis() - startTime) + \"ms\"); } 比拼结果: 备注:可能由于各方面原因,次数较少时测试不准 1.String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 2.Integer.toString()方法，直接调用 3.t + \"\"底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"性能比较","slug":"性能比较","permalink":"https://gmaya.top/tags/%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/"}]},{"title":"Mybatis-plus之自动生成","slug":"Mybatis-plus","date":"2019-12-27T14:16:25.000Z","updated":"2020-05-16T05:23:22.115Z","comments":true,"path":"2019/Mybatis-plus/","link":"","permalink":"https://gmaya.top/2019/Mybatis-plus/","excerpt":"","text":"1. 前言本文章首次是在csdn上写的，本次发布到自己的博客中，增添一点色彩。 点击查看Mybatis-plus官方文档 MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 2. 创建spring boot 项目使用idea默认生成即可 … 3. 修改pom文件 org.springframework.boot spring-boot-starter org.springframework.boot spring-boot-starter-web org.springframework.boot spring-boot-starter-test test org.projectlombok lombok true com.baomidou mybatis-plus-boot-starter 3.1.0 com.baomidou mybatis-plus-generator 3.1.0 org.apache.velocity velocity-engine-core 2.0 mysql mysql-connector-java runtime org.projectlombok lombok 1.16.16 org.springframework.boot spring-boot-configuration-processor true io.springfox springfox-swagger2 2.9.2 io.springfox springfox-swagger-ui 2.9.2 4. 修改application.yml文件server: port: 8080 spring: datasource: url: jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: admin driver-class-name: com.mysql.jdbc.Driver mybatis-plus: mapperLocations: classpath*:mapper/*.xml typeAliasesPackage: com.mybatis.plus.demo.model # mybatis 别名包扫描路径 global-config: db-config: logic-delete-value: 1 #逻辑已删除值(默认为 1) logic-not-delete-value: 0 # 逻辑未删除值(默认为 0) configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl #线上关闭sql日志打印 5. 修改启动类 6. 创建Mybatis-plus配置新建文件MyBatisPlusConfiguration 注意 : 如果使用mp自带的控制逻辑删除和控制版本号还有分页功能,则需要添加这个配置 package com.plus.demo.util; import com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import com.baomidou.mybatisplus.core.injector.ISqlInjector; import com.baomidou.mybatisplus.extension.injector.LogicSqlInjector; import com.baomidou.mybatisplus.extension.plugins.OptimisticLockerInterceptor; @Configuration public class MyBatisPlusConfiguration { /** * 控制逻辑删除 * @return */ @Bean public ISqlInjector sqlInjector() { return new LogicSqlInjector(); } /** * 控制版本号 * @return */ @Bean public OptimisticLockerInterceptor optimisticLockerInterceptor() { return new OptimisticLockerInterceptor(); } /** * 分页插件 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } } 7. 编写自动生成工具类注 : 直接把Controller/service/serviceImpl/dao/mapper/form/vo全部生成 此处使用默认velocity 模版引擎 则没有常用的crud方法,使用自己的模板则可以生成常用的crud方法 其中自定义模板中有工具类是公司自己的jar包,所以只推荐参考 拷贝mybatis-plus-generator jar包中的模板到自己项目resources下 package com.mybatis.plus.demo.util; import com.baomidou.mybatisplus.annotation.DbType; import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.core.exceptions.MybatisPlusException; import com.baomidou.mybatisplus.core.toolkit.StringPool; import com.baomidou.mybatisplus.core.toolkit.StringUtils; import com.baomidou.mybatisplus.generator.AutoGenerator; import com.baomidou.mybatisplus.generator.InjectionConfig; import com.baomidou.mybatisplus.generator.config.*; import com.baomidou.mybatisplus.generator.config.converts.MySqlTypeConvert; import com.baomidou.mybatisplus.generator.config.po.TableInfo; import com.baomidou.mybatisplus.generator.config.rules.DbColumnType; import com.baomidou.mybatisplus.generator.config.rules.IColumnType; import com.baomidou.mybatisplus.generator.config.rules.NamingStrategy; import java.util.ArrayList; import java.util.List; import java.util.Scanner; /** * 代码生成器 * */ public class MabatisPlusGenerator { private static String path = \"E:\\\\idea\\\\springboot-mybatis-plus\"; //文件路径 private static String table = \"tm_user\"; //table名字 public static void main(String[] args) { //1. 全局配置 GlobalConfig config = new GlobalConfig(); config.setActiveRecord(false) // 是否支持AR模式 .setAuthor(\"GMaya\") // 作者 .setSwagger2(true) // 使用Swagger .setOutputDir(path + \"/src/main/java\") // 生成路径 .setFileOverride(true) // 文件覆盖 .setIdType(IdType.AUTO) // 主键策略 // 自定义文件命名，注意 %s 会自动填充表实体属性！ .setServiceName(\"%sService\").setServiceImplName(\"%sServiceImpl\") .setControllerName(\"%sController\").setMapperName(\"%sDao\").setXmlName(\"%sMapper\") .setOpen(false) // 生成文件后 不打开文件夹 .setBaseResultMap(true) // XML ResultMap .setBaseColumnList(true); // XML columList //2. 数据源配置 DataSourceConfig dsConfig = new DataSourceConfig(); dsConfig.setDbType(DbType.MYSQL) // 设置数据库类型 .setDriverName(\"com.mysql.jdbc.Driver\") .setUrl( \"jdbc:mysql://localhost:3306/test?characterEncoding=UTF-8\") .setUsername(\"root\").setPassword(\"admin\").setTypeConvert(new MySqlTypeConvert() { @Override public IColumnType processTypeConvert(GlobalConfig globalConfig, String fieldType) { if (fieldType.toLowerCase().contains(\"tinyint\")) { return DbColumnType.BOOLEAN; } //将数据库中datetime转换成date if (fieldType.toLowerCase().contains(\"datetime\")) { return DbColumnType.DATE; } return super.processTypeConvert(globalConfig, fieldType); } }); //3. 策略配置 StrategyConfig stConfig = new StrategyConfig(); stConfig.setColumnNaming(NamingStrategy.underline_to_camel) .setNaming(NamingStrategy.underline_to_camel) // 数据库表映射到实体的命名策略 .setEntityLombokModel(true) // 使用Lombok .setVersionFieldName(\"version\") // 数据库版本控制字段 .setLogicDeleteFieldName(\"status\") // 数据库逻辑删除字段 .setRestControllerStyle(true) .setSuperServiceClass(\"com.baomidou.mybatisplus.extension.service.IService\") .setInclude(new String[]{table}); // 生成的表 //4. 包名策略配置 PackageConfig pkConfig = new PackageConfig(); pkConfig.setParent(\"com.mybatis.plus.demo\").setMapper(\"dao\").setService(\"service\") .setServiceImpl(\"service.impl\").setController(\"controller\").setEntity(\"model\"); //5.自定义配置 InjectionConfig cfg = new InjectionConfig() { @Override public void initMap() { // to do nothing } }; String templatePath = \"/templates/mapper.xml.vm\"; // 如果模板引擎是 velocity List&lt;FileOutConfig> focList = new ArrayList&lt;>(); // 自定义输出配置 focList.add(new FileOutConfig(templatePath) { // 自定义配置会被优先输出 @Override public String outputFile(TableInfo tableInfo) { // mapper自定义输出文件名 return path + \"/src/main/resources/mapper/\" + tableInfo.getEntityName() + \"Mapper\" + StringPool.DOT_XML; } }); // ------------form vo 使用默认模板请注释-begin----------- templatePath = \"/templates/entityForm.java.vm\"; focList.add(new FileOutConfig(templatePath) { // 自定义配置会被优先输出 @Override public String outputFile(TableInfo tableInfo) { // form自定义输出文件名 return path + \"/src/main/java/com/mybatis/plus/demo/pojo/\" + tableInfo.getEntityName() + \"Form\" + StringPool.DOT_JAVA; } }); templatePath = \"/templates/entityVO.java.vm\"; focList.add(new FileOutConfig(templatePath) { // 自定义配置会被优先输出 @Override public String outputFile(TableInfo tableInfo) { // VO自定义输出文件名 return path + \"/src/main/java/com/mybatis/plus/demo/pojo/\" + tableInfo.getEntityName() + \"VO\" + StringPool.DOT_JAVA; } }); // ------------form vo 使用默认模板请注释-end----------- cfg.setFileOutConfigList(focList); // 6 配置模板 自定义模板/在resources/templates 可以编辑 TemplateConfig templateConfig = new TemplateConfig(); // 关闭默认 xml 生成，调整生成 至 根目录 templateConfig.setEntity(\"/templates/entity.java\").setService(\"/templates/service.java\") .setController(\"/templates/controller.java\").setMapper(\"/templates/mapper.java\") .setServiceImpl(\"/templates/serviceImpl.java\").setXml(null); //7. 整合配置 AutoGenerator ag = new AutoGenerator(); ag.setGlobalConfig(config).setDataSource(dsConfig).setStrategy(stConfig).setCfg(cfg) .setPackageInfo(pkConfig).setTemplate(templateConfig); //8. 执行 ag.execute(); } } 注 : 如果是使用自定义模板,请注意修改模板中的form,vo引入包路径等 使用默认模板请注释代码中生成form/vo代码 第一次运行可能有以下错误,如果没有请忽略 问题1 : The server time zone value ‘ÖÐ¹ú±ê×¼Ê±¼ä’ is unrecognized or represents 解决 : set global time_zone=’+8:00’ 问题2 : 使用Lombok插件,Idea,Eclipse必须安装改插件,否则找不到get/set方法 解决 : IDEA安装Lombok步骤 Eclipse的自行百度 /滑稽 修改文件路径表名之后运行 生成结构图如下 perfect ! ! !","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"Mybatis-plus","slug":"Mybatis-plus","permalink":"https://gmaya.top/tags/Mybatis-plus/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-12-16T14:16:00.000Z","updated":"2020-05-16T05:25:20.018Z","comments":true,"path":"2019/hello-world/","link":"","permalink":"https://gmaya.top/2019/hello-world/","excerpt":"","text":"各种语言的Hello WorldJava$ System.out.println(\"Hello,World!\"); 我说Java天下第一,你信么! C$ printf(\"Hello,World!\"); C++$ std::cout &lt;&lt; \"Hello,World!\" &lt;&lt; std::endl; Python$ >>> print(\"Hello,World!\") C#$ Console.WriteLine(\"Hello,World!\"); PHP$ echo \"Hello,World!\"; JavaScript$ var sys = require(\"sys\"); sys.puts(\"Hello,World!\"); Ruby$ puts \"Hello,World!\" R$ print(\"Hello,World!\") SQL$ select 'Hello,World!' from dual; HTML$ &lt;p>Hello,World!&lt;/p> VB$ MsgBox(\"Hello,World!\") Shell$ echo \"Hello,World!\" Delphi$ label1.Caption := 'Hello,World!'; TCL$ % puts \"Hello,World!\" Pascal$ writeln('Hello, world!') 我认输了, 我是菜鸟! 一个Hello, world! 打败我了","categories":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gmaya.top/tags/Java/"},{"name":"HelloWorld","slug":"HelloWorld","permalink":"https://gmaya.top/tags/HelloWorld/"}]}]}